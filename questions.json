[{"id": 1, "question": "Which AWS service is primarily used for storing static files?", "options": ["EC2", "S3", "DynamoDB", "RDS"], "correct_answer": "S3", "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance for storing static files."}, {"id": 2, "question": "Which AWS service would you use to run containers?", "options": ["EC2", "S3", "ECS/EKS", "Lambda"], "correct_answer": "ECS/EKS", "explanation": "Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service) are services designed specifically for running containers in AWS."}, {"id": 3, "question": "A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.\r\n\r\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?", "options": ["4 shards : 2 instances", "1 shard : 6 instances", "6 shards : 1 instance", "4 shards : 8 instances"], "correct_answer": "4 shards : 2 instances", "explanation": "A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\n\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nSince the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer.\r\n\r\nThe 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.\r\n\r\nThe 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn\u2019t have a backup instance to process the shards in the event of an outage.\r\n\r\nThe 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well.\r\n\r\n"}, {"id": 4, "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\r\n\r\nHow should the developer retrieve the variables with the FEWEST application changes? ", "options": ["Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", " Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment. ", " Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment. ", "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process. "], "correct_answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", "explanation": "A is correct:\r\n\r\n- It uses the appropriate services for the right types of data (Parameter Store for configuration, Secrets Manager for sensitive credentials)\r\n\r\n- It provides a centralized approach that requires minimal application changes\r\n- It supports hierarchical organization for different environments\r\n- It provides robust security controls through IAM\r\n- It enables changes to parameters without application redeployment\r\n\r\nThe application would only need to be updated once to retrieve variables from these services, and then all future changes to the variables would be managed through the services without additional application changes."}, {"id": 5, "question": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.\r\n\r\nWhich section of a CloudFormation template cannot be associated with Condition?", "options": ["Conditions", "Resources", "Outputs", "Parameters"], "correct_answer": "Parameters", "explanation": "Parameters\r\n\r\nParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\r\n\r\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\r\n\r\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.\r\n\r\nConditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.\r\n\r\nPlease review this note for more details:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\r\n\r\nPlease visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.\r\n\r\nIncorrect options:\r\n\r\nResources - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.\r\n\r\nConditions - You actually define conditions in this section of the CloudFormation template\r\n\r\nOutputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create."}, {"id": 6, "question": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\r\n\r\nAs a Developer Associate, which of the following options would you recommend for the given use-case?\r\n\r\n\r\n\r\n\r\n\r\n", "options": ["Cognito User Pools", "Lambda Authorizer", "API Gateway User Pools", "IAM permissions with sigv4"], "correct_answer": "Lambda Authorizer", "explanation": "Correct option:\r\n\r\n\"Lambda Authorizer\"\r\n\r\nAn Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.\r\n\r\n via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nIncorrect options:\r\n\r\n\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.\r\n\r\n\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.\r\n\r\n\"API Gateway User Pools\" - This is a made-up option, added as a distractor.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"}, {"id": 7, "question": "You're designing an application that processes data from an Amazon Kinesis Data Stream. Your current architecture includes a Kinesis stream with 8 shards. Each EC2 instance in your application runs a single Kinesis Client Library (KCL) consumer application.\r\n\r\nWhat is the optimal number of EC2 instances you should deploy to efficiently process this Kinesis stream?", "options": ["4 instances", " 8 instances", "16 instances", "32 instances"], "correct_answer": " 8 instances", "explanation": "When designing applications that process Amazon Kinesis Data Streams, it's important to understand the relationship between shards and KCL workers for optimal performance.\r\n\r\nThe best practice is to match the number of instances (each running one KCL worker) to the number of shards in your Kinesis stream. This is because:\r\n\r\n- Each shard in a Kinesis Data Stream can be processed by exactly one KCL worker at any given time\r\n\r\n- A single KCL worker can process multiple shards, but this may not be optimal for performance\r\n\r\n- Having more KCL workers than shards is inefficient as some workers would be idle\r\n\r\n- Having fewer KCL workers than shards means some workers would be processing multiple shards, which might create a bottleneck\r\n\r\nIn this scenario, with 8 shards in your Kinesis stream, the optimal configuration would be 8 EC2 instances, each running one KCL worker application. This provides a 1:1 mapping between shards and KCL workers, ensuring maximum throughput and parallel processing capability.\r\n\r\nIf your data processing needs change, you would typically adjust both the number of shards and the number of instances accordingly to maintain this optimal ratio."}, {"id": 8, "question": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\r\n\r\n\r\nWhich of the following does **NOT** help with debugging the issue?", "options": ["X-Ray sampling", "EC2 Instance Role", "EC2 X-Ray Daemon", "CloudTrail"], "correct_answer": "X-Ray sampling", "explanation": "Correct option:\r\n\r\n**X-Ray sampling**\r\n\r\nWhy X-Ray sampling WON'T help:\r\nX-Ray sampling only controls which requests get traced, not whether those traces successfully reach X-Ray. Adjusting sampling rules is like deciding how many photos to take, but won't help if the camera can't transmit pictures to your cloud storage.\r\n\r\n\r\n**EC2 X-Ray Daemon**\r\n\r\n- The X-Ray daemon is the component that actually sends trace data to AWS\r\n\r\n- Checking daemon logs would show connection errors, timeouts, or permission issues\r\n\r\n- You could verify if the daemon is running correctly with ps aux | grep xray\r\n\r\n- The daemon log file at /var/log/xray/xray.log might contain error messages\r\n\r\n\r\n#### 2. EC2 Instance Role\r\n* The X-Ray daemon needs proper IAM permissions to send data to X-Ray\r\n* The EC2 instance role provides these permissions automatically\r\n* Checking the attached role and its policies would reveal missing permissions\r\n* You could verify the policy includes `xray:PutTraceSegments` and `xray:PutTelemetryRecords`\r\n\r\n\r\n#### 3. CloudTrail\r\n* CloudTrail logs all API calls made to AWS services\r\n* It would show denied API calls due to permission issues\r\n* You could search for X-Ray related API calls from your EC2 instance\r\n* Failed API calls would include detailed error messages explaining why they failed\r\n\r\n## Key Troubleshooting Steps\r\n\r\nIn this scenario, you should:\r\n1. Check if the X-Ray daemon is running on the EC2 instance\r\n2. Verify the EC2 instance role has appropriate X-Ray permissions\r\n3. Look at CloudTrail logs for denied X-Ray API calls\r\n4. Check security groups and network ACLs to ensure outbound traffic to X-Ray endpoints is allowed\r\n\r\nAdjusting sampling rules would not provide any useful diagnostic information since no data is being transmitted at all."}, {"id": 9, "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket, the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).\r\n\r\nWhich solution will guarantee that any upload request without the mandated encryption is not processed?", "options": ["Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `aws:kms`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `sse:s3`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header."], "correct_answer": "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "explanation": "### Why Option C is Correct:\r\nWhen using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), you need to:\r\n\r\n1. Set the `x-amz-server-side-encryption` header to `AES256` in your PutObject requests\r\n2. Implement a bucket policy that enforces this requirement\r\n\r\nThis approach works because:\r\n* `AES256` is the correct value that specifies SSE-S3 encryption (Amazon S3-managed keys)\r\n* The bucket policy can deny any requests that either:\r\n  * Don't include the encryption header at all\r\n  * Include the header but with an incorrect value\r\n\r\nA typical bucket policy would look like this:\r\n```json\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis policy uses two statements:\r\n* The first statement denies requests where the header exists but has an incorrect value\r\n* The second statement denies requests where the header is missing entirely\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option A is incorrect** because:\r\n* `aws:kms` is used for SSE-KMS (Server-Side Encryption with AWS KMS keys)\r\n* The question specifically requires SSE-S3 (Server-Side Encryption with Amazon S3-managed keys)\r\n* While this would enforce encryption, it would use the wrong type of encryption\r\n\r\n**Option B is incorrect** because:\r\n* `sse:s3` is not a valid value for the `x-amz-server-side-encryption` header\r\n* The valid values are `AES256` (for SSE-S3) or `aws:kms` (for SSE-KMS)\r\n* Using an invalid header value would cause all requests to fail\r\n\r\n**Option D is incorrect** because:\r\n* With SSE-S3, you don't specify or manage the encryption keys\r\n* Amazon S3 automatically handles key management, generating a unique key for each object\r\n* There is no way to \"set the encryption key for SSE-S3\" in an HTTP header\r\n\r\n### Key Concept:\r\nWhen using SSE-S3, remember that:\r\n1. Amazon S3 handles all key management automatically\r\n2. Each object is encrypted with a unique key\r\n3. These keys are themselves encrypted with a master key that AWS rotates regularly\r\n4. The encryption standard used is AES-256\r\n5. You only need to specify the encryption method (`AES256`), not any keys\r\n\r\n## AWS Documentation Reference\r\nFor more information, refer to the [Amazon S3 Developer Guide on Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)."}, {"id": 10, "question": "A development team has created a serverless application that uses Amazon API Gateway and AWS Lambda. They want to use a single Lambda function across multiple API Gateway stages (development, testing, and production), but they need the function to read from a different DynamoDB table depending on which stage is being called. \r\n\r\nWhat is the MOST appropriate way for the developer to pass these configuration parameters to the Lambda function?", "options": ["Use Stage Variables in API Gateway and reference them in mapping templates", "Set up an API Gateway Private Integration to the Lambda function", "Create environment variables in the Lambda function for each table name", "Configure traffic shifting with Lambda Aliases for each stage"], "correct_answer": "Use Stage Variables in API Gateway and reference them in mapping templates", "explanation": "### Why Option A is Correct:\r\nStage variables are name-value pairs that function as configuration attributes for different deployment stages of your API Gateway REST API. They effectively work like environment variables that can be accessed from various parts of your API configuration, including mapping templates.\r\n\r\nThis solution works perfectly for the scenario because:\r\n\r\n1. **Dynamic Configuration Per Stage**: Stage variables allow you to set different values for each deployment stage (development, testing, production)\r\n\r\n2. **Accessible in Mapping Templates**: You can reference stage variables in the mapping templates that generate the request for your Lambda function\r\n\r\n3. **No Code Changes Needed**: The Lambda function code remains the same across all environments, making maintenance easier\r\n\r\n4. **Implementation Example**:\r\n   ```json\r\n   // API Gateway mapping template example\r\n   {\r\n     \"tableName\": \"$stageVariables.dynamoDBTableName\",\r\n     \"operation\": \"read\",\r\n     \"key\": {\r\n       \"id\": \"$input.params('id')\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In this example, `$stageVariables.dynamoDBTableName` would contain different values in different stages:\r\n   - In development: \"dev-customer-table\"\r\n   - In testing: \"test-customer-table\"\r\n   - In production: \"prod-customer-table\"\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option B is incorrect** because:\r\n* API Gateway Private Integration is used to connect API Gateway to private resources in your VPC\r\n* It doesn't provide a way to dynamically configure which DynamoDB table the Lambda function should use\r\n* It's designed for network connectivity, not for configuration parameter passing\r\n\r\n**Option C is incorrect** because:\r\n* Lambda environment variables are static for a given function and don't change based on which API stage called the function\r\n* While you could check the stage name in your Lambda code and use different tables based on that, this would require code changes and additional logic\r\n* This approach would be less maintainable as it mixes configuration with application code\r\n\r\n**Option D is incorrect** because:\r\n* Lambda Aliases are used to point to specific versions of Lambda functions\r\n* Traffic shifting with aliases is for gradually moving traffic between different versions of a function\r\n* This doesn't solve the problem of using different DynamoDB tables without modifying the function code\r\n* This approach would require maintaining multiple versions of essentially the same code with different table names hardcoded\r\n\r\n### Key Concept:\r\nAPI Gateway Stage Variables provide a clean separation of configuration from code, allowing you to deploy the same Lambda function code to multiple environments while dynamically changing its behavior based on which stage is calling it.\r\n\r\n## AWS Documentation Reference\r\nFor more information on using stage variables with Lambda functions, refer to the [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html)."}, {"id": 11, "question": "A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department.\r\n\r\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.\r\n\r\nWhich of the following options is the MOST suitable solution that the developer implements?\r\n\r\n", "options": ["Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.", "Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.", "Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.", "Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls."], "correct_answer": "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.", "explanation": "The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.\r\n\r\nIt is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following:\r\n\r\n-The ability to send traces to multiple different tracing backends without having to re-instrument your code\r\n\r\n-Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community\r\n\r\n-Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js\r\n\r\n\r\nConversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following:\r\n\r\n-A tightly integrated single-vendor solution\r\n\r\n-Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET\r\n\r\nAn account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default:\r\n\r\nhttps://Your_AWS_Account_ID.signin.aws.amazon.com/console/\r\n\r\nIf you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example.\r\n\r\nhttps://Your_Alias.signin.aws.amazon.com/console/\r\n\r\nThe original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account:\r\n\r\naws iam create-account-alias --account-alias demosite.com\r\n\r\n**Hence, for this scenario, the correct answer is:** Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\r\n\r\n**The option that says:** Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature.\r\n\r\n**The option that says:** Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA).\r\n\r\n**The option that says:**Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn\u2019t have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry."}, {"id": 12, "question": "An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\r\n\r\n\r\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?\r\n\r\n", "options": ["Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.", "Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.", "Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.", "Update the stacks on multiple AWS accounts using CloudFormation StackSets."], "correct_answer": "Update the stacks on multiple AWS accounts using CloudFormation StackSets.", "explanation": "**AWS CloudFormation StackSets** extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\r\n\r\n\r\n\r\nA ***stack set*** lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set\u2019s AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.\r\n\r\nHence, the correct solution in this scenario is to **update the stacks on multiple AWS accounts using CloudFormation StackSets.**\r\n\r\n&nbsp;\r\n\r\nAfter you\u2019ve defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions.\r\n\r\nThe option that says: **Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets** is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets.\r\n\r\nThe option that says: **Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances** is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets.\r\n\r\nThe option that says: **Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts** is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"}, {"id": 13, "question": "A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition.\r\n\r\nWhich of the following is the EASIEST way to accomplish this task?\r\n\r\n", "options": ["Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.", "Use AWS SAM to migrate and deploy the company's web services to API Gateway.", "Create models and templates for request and response mappings based on the company's API definitions.", "Use CodeDeploy to migrate and deploy the company's web services to API Gateway."], "correct_answer": "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.", "explanation": "You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.\r\n\r\nYou can paste a [Swagger](http://swagger.io/) API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below:\r\n\r\n\r\nYou can also import your Swagger definition through the AWS CLI and SDKs.\r\n\r\nHence, the correct answer in this scenario is to **import their Swagger or OpenAPI definitions to API Gateway using the AWS Console**.\r\n\r\n**Using CodeDeploy to migrate and deploy the company\u2019s web services to API Gateway** is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment.\r\n\r\n**Using AWS SAM to migrate and deploy the company\u2019s web services to API Gateway** is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS.\r\n\r\n**Creating models and templates for request and response mappings based on the company\u2019s API definitions** is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file."}, {"id": 14, "question": "A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don\u2019t need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring.\r\n\r\nWhich service will you use to easily deploy and manage the application?\r\n\r\n", "options": ["AWS Elastic Beanstalk", "AWS CloudFormation", "AWS CodeDeploy", "AWS SAM"], "correct_answer": "AWS Elastic Beanstalk", "explanation": "With **Elastic Beanstalk**, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n&nbsp;\r\n\r\nYou can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).\r\n\r\nTo use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**AWS CloudFormation** is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can\u2019t host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n**AWS CodeDeploy** is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."}, {"id": 15, "question": "A mobile game has a DynamoDB table named `AWSDevScores` which keeps track of the users and their respective scores. Each item in the table is identified by the `FighterId` attribute as its partition key and the `FightTitle` attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named `AWSDevTopScores` and `AWSDevDateTime` attributes.\r\n\r\n\r\nWhich type of index should the developer add in the table to speed up queries on non-key attributes?\r\n\r\n", "options": ["Primary Index", "Sparse Index", "Global Secondary Index", "Local Secondary Index"], "correct_answer": "Global Secondary Index", "explanation": "Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue `Query` or `Scan` requests against these indexes.\r\n\r\n&nbsp;\r\n\r\nA ***secondary index*** is a data structure that contains a subset of attributes from a table, along with an alternate key to support `Query` operations. You can retrieve data from the index using a `Query`, in much the same way as you use `Query` with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nTo speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn\u2019t even need to have the same key schema as a table.\r\n\r\nHence, the correct answer in this scenario is to add a **Global Secondary Index**.\r\n\r\n**Sparse index** is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn\u2019t appear in every table item, the index is said to be *\u201csparse\u201d*.\r\n\r\n**Local Secondary Index** is incorrect because this is used for queries which use the same partition key value, and in addition, you can\u2019t add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.\r\n\r\n**Primary Index** is incorrect because this one actually refers to the partition key, which is the `FighterId` attribute in this scenario."}, {"id": 16, "question": "A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture.\r\n\r\nWhich of the following services is the MOST suitable one to use?", "options": ["AWS SAM", "EC2", "Elastic Beanstalk", "ECS"], "correct_answer": "ECS", "explanation": "**Amazon Elastic Container Service (Amazon ECS)** is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.\r\n\r\nYou can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\r\n\r\nElastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more **fine-grained** control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **ECS.**\r\n\r\n**Elastic Beanstalk** is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications.\r\n\r\n**EC2** is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB.\r\n\r\n"}, {"id": 17, "question": "An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available.\r\n\r\nWhich of the following is the most suitable feature that you have to use to meet this requirement?", "options": ["AWS Device Farm", "Amazon Cognito Identity Pools", "Amazon Cognito Sync", "Amazon Cognito User Pools"], "correct_answer": "Amazon Cognito Sync", "explanation": "Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.\r\n\r\nAmazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user\u2019s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.\r\n\r\nThe Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize.\r\n\r\nAmazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.\r\n\r\nHence, the correct answer is to **Amazon Cognito Sync***.*\r\n\r\n**Amazon Cognito User Pools** is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito.\r\n\r\n**Amazon Cognito Identity Pools** is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\n**AWS Device Farm** is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.\r\n\r\n&nbsp;"}, {"id": 18, "question": "A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing.\r\n\r\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?", "options": ["Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.", "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.", "Postpone the delivery of new messages by using a delay queue.", "Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0."], "correct_answer": "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.", "explanation": "The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.\r\n\r\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\u2019t automatically delete the message. Because Amazon SQS is a distributed system, there\u2019s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.\r\n\r\nImmediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a ***visibility timeout***, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.\r\n\r\nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn\u2019t call the `[DeleteMessage](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html)` action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout.\r\n\r\nEvery Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout.\r\n\r\nHence, the best solution in this scenario is to **set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue**.\r\n\r\n**Configuring the queue to use short polling by setting the `WaitTimeSeconds` parameter of the `ReceiveMessage` request to 0** is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario.\r\n\r\n**Configuring the queue to use long polling by setting the `Receive Message Wait Time` parameter to a value greater than 0** is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (*when there are no messages available for a ReceiveMessage request*) and false empty responses (*when messages are available but aren\u2019t included in a response*). A more appropriate solution in this scenario is to configure the visibility timeout of the messages.\r\n\r\n**Postponing the delivery of new messages by using a delay queue** is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts."}, {"id": 19, "question": "A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\r\n\r\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?", "options": ["Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.", "Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.", "Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.", "Enable auto-scaling on the Fargate tasks."], "correct_answer": "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.", "explanation": "CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).\r\n\r\n&nbsp;\r\n\r\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response)."}, {"id": 20, "question": "A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application\u2019s performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading.\r\n\r\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?", "options": ["Refactor the application to use DynamoDB transactional read and write APIs .", "Enable DynamoDB Streams.", "Upgrade the EC2 instances to a higher instance type.", "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations."], "correct_answer": "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.", "explanation": "For applications that need to read or write multiple items, DynamoDB provides the `BatchGetItem` and `BatchWriteItem` operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\r\n\r\nThe batch operations are essentially wrappers around multiple read or write requests. For example, if a `BatchGetItem` request contains five items, DynamoDB performs five `GetItem` operations on your behalf. Similarly, if a `BatchWriteItem` request contains two put requests and four delete requests, DynamoDB performs two `PutItem` and four `DeleteItem` requests.\r\n\r\nIn general, a batch operation does not fail unless *all* of the requests in the batch fail. For example, suppose you perform a `BatchGetItem`operation but one of the individual `GetItem` requests in the batch fails. In this case, `BatchGetItem` returns the keys and data from the `GetItem`request that failed. The other `GetItem` requests in the batch are not affected.\r\n\r\nHence, the correct answer is to **use DynamoDB Batch Operations API for GET, PUT, and DELETE operations** in this scenario.\r\n\r\n**Upgrading the EC2 instances to a higher instance type** is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation.\r\n\r\n**Enabling DynamoDB Streams** is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch.\r\n\r\n**Refactoring the application to use DynamoDB transactional read and write APIs** is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario."}, {"id": 21, "question": "You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\r\n\r\nWhich of the following statements are TRUE regarding this scenario?", "options": ["There will be at most 100 Lambda function invocations running concurrently.", "The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.", "The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.", "The Lambda function has 500 concurrent executions."], "correct_answer": "There will be at most 100 Lambda function invocations running concurrently.", "explanation": "You can use an **AWS Lambda function** to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.\r\n\r\n***Concurrent executions*** refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source.\r\n\r\nFor Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard\u2019s events in sequence.\r\n\r\nHence, the correct answer in this scenario is that: **there will be at most 100 Lambda function invocations running concurrently.**\r\n\r\nThe option that says: **the Lambda function has 500 concurrent executions** is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function.\r\n\r\nThe option that says: **the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards** is incorrect because, by default, AWS Lambda will automatically scale the function\u2019s concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream.\r\n\r\nThe option that says: **the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function** is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function."}, {"id": 22, "question": "A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity.\r\n\r\nWhich of the following is the MOST appropriate service that the developer should use in this scenario?", "options": ["Serverless Application Framework", "AWS CloudFormation", "AWS SAM", "AWS Systems Manager"], "correct_answer": "AWS SAM", "explanation": "The AWS Serverless Application Model (AWS SAM) is an open source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML.\r\n\r\nAWS SAM is natively supported by AWS CloudFormation and provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.\r\n\r\nHence, the correct answer is **AWS SAM**.\r\n\r\n**AWS CloudFormation** is incorrect. Although this service can deploy the serverless application to AWS, it is still more appropriate to use AWS SAM instead. AWS SAM can simplify the deployment of the serverless application by deploying all related resources together as a single, versioned entity.\r\n\r\n**AWS Systems Manager** is incorrect because it is more focused on management and operations of AWS resources, such as automation, patching, and configuration, but it is not a deployment or application modeling tool.\r\n\r\n**Serverless Application Framework** is incorrect. Although it is a well-known framework for building and deploying serverless applications into the AWS cloud, this is not an AWS native solution. It also does not allow configuration of DynamoDB databases or API Gateway APIs, unlike AWS SAM."}]