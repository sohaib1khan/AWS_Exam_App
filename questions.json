[{"id": 1, "question": "Which AWS service is primarily used for storing static files?", "options": ["EC2", "S3", "DynamoDB", "RDS"], "correct_answer": "S3", "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance for storing static files."}, {"id": 2, "question": "Which AWS service would you use to run containers?", "options": ["EC2", "S3", "ECS/EKS", "Lambda"], "correct_answer": "ECS/EKS", "explanation": "Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service) are services designed specifically for running containers in AWS."}, {"id": 3, "question": "A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.\r\n\r\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?", "options": ["4 shards : 2 instances", "1 shard : 6 instances", "6 shards : 1 instance", "4 shards : 8 instances"], "correct_answer": "4 shards : 2 instances", "explanation": "A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\n\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nSince the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer.\r\n\r\nThe 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.\r\n\r\nThe 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn\u2019t have a backup instance to process the shards in the event of an outage.\r\n\r\nThe 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well.\r\n\r\n"}, {"id": 4, "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\r\n\r\nHow should the developer retrieve the variables with the FEWEST application changes? ", "options": ["Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", " Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment. ", " Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment. ", "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process. "], "correct_answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", "explanation": "A is correct:\r\n\r\n- It uses the appropriate services for the right types of data (Parameter Store for configuration, Secrets Manager for sensitive credentials)\r\n\r\n- It provides a centralized approach that requires minimal application changes\r\n- It supports hierarchical organization for different environments\r\n- It provides robust security controls through IAM\r\n- It enables changes to parameters without application redeployment\r\n\r\nThe application would only need to be updated once to retrieve variables from these services, and then all future changes to the variables would be managed through the services without additional application changes."}, {"id": 5, "question": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.\r\n\r\nWhich section of a CloudFormation template cannot be associated with Condition?", "options": ["Conditions", "Resources", "Outputs", "Parameters"], "correct_answer": "Parameters", "explanation": "Parameters\r\n\r\nParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\r\n\r\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\r\n\r\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.\r\n\r\nConditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.\r\n\r\nPlease review this note for more details:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\r\n\r\nPlease visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.\r\n\r\nIncorrect options:\r\n\r\nResources - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.\r\n\r\nConditions - You actually define conditions in this section of the CloudFormation template\r\n\r\nOutputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create."}, {"id": 6, "question": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\r\n\r\nAs a Developer Associate, which of the following options would you recommend for the given use-case?\r\n\r\n\r\n\r\n\r\n\r\n", "options": ["Cognito User Pools", "Lambda Authorizer", "API Gateway User Pools", "IAM permissions with sigv4"], "correct_answer": "Lambda Authorizer", "explanation": "Correct option:\r\n\r\n\"Lambda Authorizer\"\r\n\r\nAn Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.\r\n\r\n via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nIncorrect options:\r\n\r\n\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.\r\n\r\n\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.\r\n\r\n\"API Gateway User Pools\" - This is a made-up option, added as a distractor.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"}, {"id": 7, "question": "You're designing an application that processes data from an Amazon Kinesis Data Stream. Your current architecture includes a Kinesis stream with 8 shards. Each EC2 instance in your application runs a single Kinesis Client Library (KCL) consumer application.\r\n\r\nWhat is the optimal number of EC2 instances you should deploy to efficiently process this Kinesis stream?", "options": ["4 instances", " 8 instances", "16 instances", "32 instances"], "correct_answer": " 8 instances", "explanation": "When designing applications that process Amazon Kinesis Data Streams, it's important to understand the relationship between shards and KCL workers for optimal performance.\r\n\r\nThe best practice is to match the number of instances (each running one KCL worker) to the number of shards in your Kinesis stream. This is because:\r\n\r\n- Each shard in a Kinesis Data Stream can be processed by exactly one KCL worker at any given time\r\n\r\n- A single KCL worker can process multiple shards, but this may not be optimal for performance\r\n\r\n- Having more KCL workers than shards is inefficient as some workers would be idle\r\n\r\n- Having fewer KCL workers than shards means some workers would be processing multiple shards, which might create a bottleneck\r\n\r\nIn this scenario, with 8 shards in your Kinesis stream, the optimal configuration would be 8 EC2 instances, each running one KCL worker application. This provides a 1:1 mapping between shards and KCL workers, ensuring maximum throughput and parallel processing capability.\r\n\r\nIf your data processing needs change, you would typically adjust both the number of shards and the number of instances accordingly to maintain this optimal ratio."}, {"id": 8, "question": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\r\n\r\n\r\nWhich of the following does **NOT** help with debugging the issue?", "options": ["X-Ray sampling", "EC2 Instance Role", "EC2 X-Ray Daemon", "CloudTrail"], "correct_answer": "X-Ray sampling", "explanation": "Correct option:\r\n\r\n**X-Ray sampling**\r\n\r\nWhy X-Ray sampling WON'T help:\r\nX-Ray sampling only controls which requests get traced, not whether those traces successfully reach X-Ray. Adjusting sampling rules is like deciding how many photos to take, but won't help if the camera can't transmit pictures to your cloud storage.\r\n\r\n\r\n**EC2 X-Ray Daemon**\r\n\r\n- The X-Ray daemon is the component that actually sends trace data to AWS\r\n\r\n- Checking daemon logs would show connection errors, timeouts, or permission issues\r\n\r\n- You could verify if the daemon is running correctly with ps aux | grep xray\r\n\r\n- The daemon log file at /var/log/xray/xray.log might contain error messages\r\n\r\n\r\n#### 2. EC2 Instance Role\r\n* The X-Ray daemon needs proper IAM permissions to send data to X-Ray\r\n* The EC2 instance role provides these permissions automatically\r\n* Checking the attached role and its policies would reveal missing permissions\r\n* You could verify the policy includes `xray:PutTraceSegments` and `xray:PutTelemetryRecords`\r\n\r\n\r\n#### 3. CloudTrail\r\n* CloudTrail logs all API calls made to AWS services\r\n* It would show denied API calls due to permission issues\r\n* You could search for X-Ray related API calls from your EC2 instance\r\n* Failed API calls would include detailed error messages explaining why they failed\r\n\r\n## Key Troubleshooting Steps\r\n\r\nIn this scenario, you should:\r\n1. Check if the X-Ray daemon is running on the EC2 instance\r\n2. Verify the EC2 instance role has appropriate X-Ray permissions\r\n3. Look at CloudTrail logs for denied X-Ray API calls\r\n4. Check security groups and network ACLs to ensure outbound traffic to X-Ray endpoints is allowed\r\n\r\nAdjusting sampling rules would not provide any useful diagnostic information since no data is being transmitted at all."}, {"id": 9, "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket, the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).\r\n\r\nWhich solution will guarantee that any upload request without the mandated encryption is not processed?", "options": ["Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `aws:kms`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `sse:s3`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header."], "correct_answer": "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "explanation": "### Why Option C is Correct:\r\nWhen using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), you need to:\r\n\r\n1. Set the `x-amz-server-side-encryption` header to `AES256` in your PutObject requests\r\n2. Implement a bucket policy that enforces this requirement\r\n\r\nThis approach works because:\r\n* `AES256` is the correct value that specifies SSE-S3 encryption (Amazon S3-managed keys)\r\n* The bucket policy can deny any requests that either:\r\n  * Don't include the encryption header at all\r\n  * Include the header but with an incorrect value\r\n\r\nA typical bucket policy would look like this:\r\n```json\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis policy uses two statements:\r\n* The first statement denies requests where the header exists but has an incorrect value\r\n* The second statement denies requests where the header is missing entirely\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option A is incorrect** because:\r\n* `aws:kms` is used for SSE-KMS (Server-Side Encryption with AWS KMS keys)\r\n* The question specifically requires SSE-S3 (Server-Side Encryption with Amazon S3-managed keys)\r\n* While this would enforce encryption, it would use the wrong type of encryption\r\n\r\n**Option B is incorrect** because:\r\n* `sse:s3` is not a valid value for the `x-amz-server-side-encryption` header\r\n* The valid values are `AES256` (for SSE-S3) or `aws:kms` (for SSE-KMS)\r\n* Using an invalid header value would cause all requests to fail\r\n\r\n**Option D is incorrect** because:\r\n* With SSE-S3, you don't specify or manage the encryption keys\r\n* Amazon S3 automatically handles key management, generating a unique key for each object\r\n* There is no way to \"set the encryption key for SSE-S3\" in an HTTP header\r\n\r\n### Key Concept:\r\nWhen using SSE-S3, remember that:\r\n1. Amazon S3 handles all key management automatically\r\n2. Each object is encrypted with a unique key\r\n3. These keys are themselves encrypted with a master key that AWS rotates regularly\r\n4. The encryption standard used is AES-256\r\n5. You only need to specify the encryption method (`AES256`), not any keys\r\n\r\n## AWS Documentation Reference\r\nFor more information, refer to the [Amazon S3 Developer Guide on Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)."}, {"id": 10, "question": "A development team has created a serverless application that uses Amazon API Gateway and AWS Lambda. They want to use a single Lambda function across multiple API Gateway stages (development, testing, and production), but they need the function to read from a different DynamoDB table depending on which stage is being called. \r\n\r\nWhat is the MOST appropriate way for the developer to pass these configuration parameters to the Lambda function?", "options": ["Use Stage Variables in API Gateway and reference them in mapping templates", "Set up an API Gateway Private Integration to the Lambda function", "Create environment variables in the Lambda function for each table name", "Configure traffic shifting with Lambda Aliases for each stage"], "correct_answer": "Use Stage Variables in API Gateway and reference them in mapping templates", "explanation": "### Why Option A is Correct:\r\nStage variables are name-value pairs that function as configuration attributes for different deployment stages of your API Gateway REST API. They effectively work like environment variables that can be accessed from various parts of your API configuration, including mapping templates.\r\n\r\nThis solution works perfectly for the scenario because:\r\n\r\n1. **Dynamic Configuration Per Stage**: Stage variables allow you to set different values for each deployment stage (development, testing, production)\r\n\r\n2. **Accessible in Mapping Templates**: You can reference stage variables in the mapping templates that generate the request for your Lambda function\r\n\r\n3. **No Code Changes Needed**: The Lambda function code remains the same across all environments, making maintenance easier\r\n\r\n4. **Implementation Example**:\r\n   ```json\r\n   // API Gateway mapping template example\r\n   {\r\n     \"tableName\": \"$stageVariables.dynamoDBTableName\",\r\n     \"operation\": \"read\",\r\n     \"key\": {\r\n       \"id\": \"$input.params('id')\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In this example, `$stageVariables.dynamoDBTableName` would contain different values in different stages:\r\n   - In development: \"dev-customer-table\"\r\n   - In testing: \"test-customer-table\"\r\n   - In production: \"prod-customer-table\"\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option B is incorrect** because:\r\n* API Gateway Private Integration is used to connect API Gateway to private resources in your VPC\r\n* It doesn't provide a way to dynamically configure which DynamoDB table the Lambda function should use\r\n* It's designed for network connectivity, not for configuration parameter passing\r\n\r\n**Option C is incorrect** because:\r\n* Lambda environment variables are static for a given function and don't change based on which API stage called the function\r\n* While you could check the stage name in your Lambda code and use different tables based on that, this would require code changes and additional logic\r\n* This approach would be less maintainable as it mixes configuration with application code\r\n\r\n**Option D is incorrect** because:\r\n* Lambda Aliases are used to point to specific versions of Lambda functions\r\n* Traffic shifting with aliases is for gradually moving traffic between different versions of a function\r\n* This doesn't solve the problem of using different DynamoDB tables without modifying the function code\r\n* This approach would require maintaining multiple versions of essentially the same code with different table names hardcoded\r\n\r\n### Key Concept:\r\nAPI Gateway Stage Variables provide a clean separation of configuration from code, allowing you to deploy the same Lambda function code to multiple environments while dynamically changing its behavior based on which stage is calling it.\r\n\r\n## AWS Documentation Reference\r\nFor more information on using stage variables with Lambda functions, refer to the [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html)."}, {"id": 11, "question": "A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department.\r\n\r\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.\r\n\r\nWhich of the following options is the MOST suitable solution that the developer implements?\r\n\r\n", "options": ["Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.", "Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.", "Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.", "Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls."], "correct_answer": "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.", "explanation": "The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.\r\n\r\nIt is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following:\r\n\r\n-The ability to send traces to multiple different tracing backends without having to re-instrument your code\r\n\r\n-Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community\r\n\r\n-Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js\r\n\r\n\r\nConversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following:\r\n\r\n-A tightly integrated single-vendor solution\r\n\r\n-Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET\r\n\r\nAn account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default:\r\n\r\nhttps://Your_AWS_Account_ID.signin.aws.amazon.com/console/\r\n\r\nIf you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example.\r\n\r\nhttps://Your_Alias.signin.aws.amazon.com/console/\r\n\r\nThe original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account:\r\n\r\naws iam create-account-alias --account-alias demosite.com\r\n\r\n**Hence, for this scenario, the correct answer is:** Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\r\n\r\n**The option that says:** Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature.\r\n\r\n**The option that says:** Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA).\r\n\r\n**The option that says:**Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn\u2019t have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry."}, {"id": 12, "question": "An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\r\n\r\n\r\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?\r\n\r\n", "options": ["Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.", "Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.", "Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.", "Update the stacks on multiple AWS accounts using CloudFormation StackSets."], "correct_answer": "Update the stacks on multiple AWS accounts using CloudFormation StackSets.", "explanation": "**AWS CloudFormation StackSets** extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\r\n\r\n\r\n\r\nA ***stack set*** lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set\u2019s AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.\r\n\r\nHence, the correct solution in this scenario is to **update the stacks on multiple AWS accounts using CloudFormation StackSets.**\r\n\r\n&nbsp;\r\n\r\nAfter you\u2019ve defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions.\r\n\r\nThe option that says: **Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets** is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets.\r\n\r\nThe option that says: **Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances** is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets.\r\n\r\nThe option that says: **Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts** is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"}, {"id": 13, "question": "A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition.\r\n\r\nWhich of the following is the EASIEST way to accomplish this task?\r\n\r\n", "options": ["Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.", "Use AWS SAM to migrate and deploy the company's web services to API Gateway.", "Create models and templates for request and response mappings based on the company's API definitions.", "Use CodeDeploy to migrate and deploy the company's web services to API Gateway."], "correct_answer": "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.", "explanation": "You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.\r\n\r\nYou can paste a [Swagger](http://swagger.io/) API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below:\r\n\r\n\r\nYou can also import your Swagger definition through the AWS CLI and SDKs.\r\n\r\nHence, the correct answer in this scenario is to **import their Swagger or OpenAPI definitions to API Gateway using the AWS Console**.\r\n\r\n**Using CodeDeploy to migrate and deploy the company\u2019s web services to API Gateway** is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment.\r\n\r\n**Using AWS SAM to migrate and deploy the company\u2019s web services to API Gateway** is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS.\r\n\r\n**Creating models and templates for request and response mappings based on the company\u2019s API definitions** is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file."}, {"id": 14, "question": "A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don\u2019t need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring.\r\n\r\nWhich service will you use to easily deploy and manage the application?\r\n\r\n", "options": ["AWS Elastic Beanstalk", "AWS CloudFormation", "AWS CodeDeploy", "AWS SAM"], "correct_answer": "AWS Elastic Beanstalk", "explanation": "With **Elastic Beanstalk**, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n&nbsp;\r\n\r\nYou can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).\r\n\r\nTo use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**AWS CloudFormation** is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can\u2019t host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n**AWS CodeDeploy** is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."}, {"id": 15, "question": "A mobile game has a DynamoDB table named `AWSDevScores` which keeps track of the users and their respective scores. Each item in the table is identified by the `FighterId` attribute as its partition key and the `FightTitle` attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named `AWSDevTopScores` and `AWSDevDateTime` attributes.\r\n\r\n\r\nWhich type of index should the developer add in the table to speed up queries on non-key attributes?\r\n\r\n", "options": ["Primary Index", "Sparse Index", "Global Secondary Index", "Local Secondary Index"], "correct_answer": "Global Secondary Index", "explanation": "Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue `Query` or `Scan` requests against these indexes.\r\n\r\n&nbsp;\r\n\r\nA ***secondary index*** is a data structure that contains a subset of attributes from a table, along with an alternate key to support `Query` operations. You can retrieve data from the index using a `Query`, in much the same way as you use `Query` with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nTo speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn\u2019t even need to have the same key schema as a table.\r\n\r\nHence, the correct answer in this scenario is to add a **Global Secondary Index**.\r\n\r\n**Sparse index** is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn\u2019t appear in every table item, the index is said to be *\u201csparse\u201d*.\r\n\r\n**Local Secondary Index** is incorrect because this is used for queries which use the same partition key value, and in addition, you can\u2019t add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.\r\n\r\n**Primary Index** is incorrect because this one actually refers to the partition key, which is the `FighterId` attribute in this scenario."}, {"id": 16, "question": "A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture.\r\n\r\nWhich of the following services is the MOST suitable one to use?", "options": ["AWS SAM", "EC2", "Elastic Beanstalk", "ECS"], "correct_answer": "ECS", "explanation": "**Amazon Elastic Container Service (Amazon ECS)** is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.\r\n\r\nYou can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\r\n\r\nElastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more **fine-grained** control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **ECS.**\r\n\r\n**Elastic Beanstalk** is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications.\r\n\r\n**EC2** is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB.\r\n\r\n"}, {"id": 17, "question": "An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available.\r\n\r\nWhich of the following is the most suitable feature that you have to use to meet this requirement?", "options": ["AWS Device Farm", "Amazon Cognito Identity Pools", "Amazon Cognito Sync", "Amazon Cognito User Pools"], "correct_answer": "Amazon Cognito Sync", "explanation": "Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.\r\n\r\nAmazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user\u2019s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.\r\n\r\nThe Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize.\r\n\r\nAmazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.\r\n\r\nHence, the correct answer is to **Amazon Cognito Sync***.*\r\n\r\n**Amazon Cognito User Pools** is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito.\r\n\r\n**Amazon Cognito Identity Pools** is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\n**AWS Device Farm** is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.\r\n\r\n&nbsp;"}, {"id": 18, "question": "A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing.\r\n\r\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?", "options": ["Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.", "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.", "Postpone the delivery of new messages by using a delay queue.", "Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0."], "correct_answer": "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.", "explanation": "The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.\r\n\r\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\u2019t automatically delete the message. Because Amazon SQS is a distributed system, there\u2019s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.\r\n\r\nImmediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a ***visibility timeout***, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.\r\n\r\nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn\u2019t call the `[DeleteMessage](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html)` action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout.\r\n\r\nEvery Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout.\r\n\r\nHence, the best solution in this scenario is to **set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue**.\r\n\r\n**Configuring the queue to use short polling by setting the `WaitTimeSeconds` parameter of the `ReceiveMessage` request to 0** is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario.\r\n\r\n**Configuring the queue to use long polling by setting the `Receive Message Wait Time` parameter to a value greater than 0** is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (*when there are no messages available for a ReceiveMessage request*) and false empty responses (*when messages are available but aren\u2019t included in a response*). A more appropriate solution in this scenario is to configure the visibility timeout of the messages.\r\n\r\n**Postponing the delivery of new messages by using a delay queue** is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts."}, {"id": 19, "question": "A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\r\n\r\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?", "options": ["Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.", "Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.", "Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.", "Enable auto-scaling on the Fargate tasks."], "correct_answer": "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.", "explanation": "CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).\r\n\r\n&nbsp;\r\n\r\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response)."}, {"id": 20, "question": "A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application\u2019s performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading.\r\n\r\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?", "options": ["Refactor the application to use DynamoDB transactional read and write APIs .", "Enable DynamoDB Streams.", "Upgrade the EC2 instances to a higher instance type.", "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations."], "correct_answer": "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.", "explanation": "For applications that need to read or write multiple items, DynamoDB provides the `BatchGetItem` and `BatchWriteItem` operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\r\n\r\nThe batch operations are essentially wrappers around multiple read or write requests. For example, if a `BatchGetItem` request contains five items, DynamoDB performs five `GetItem` operations on your behalf. Similarly, if a `BatchWriteItem` request contains two put requests and four delete requests, DynamoDB performs two `PutItem` and four `DeleteItem` requests.\r\n\r\nIn general, a batch operation does not fail unless *all* of the requests in the batch fail. For example, suppose you perform a `BatchGetItem`operation but one of the individual `GetItem` requests in the batch fails. In this case, `BatchGetItem` returns the keys and data from the `GetItem`request that failed. The other `GetItem` requests in the batch are not affected.\r\n\r\nHence, the correct answer is to **use DynamoDB Batch Operations API for GET, PUT, and DELETE operations** in this scenario.\r\n\r\n**Upgrading the EC2 instances to a higher instance type** is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation.\r\n\r\n**Enabling DynamoDB Streams** is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch.\r\n\r\n**Refactoring the application to use DynamoDB transactional read and write APIs** is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario."}, {"id": 21, "question": "You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\r\n\r\nWhich of the following statements are TRUE regarding this scenario?", "options": ["There will be at most 100 Lambda function invocations running concurrently.", "The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.", "The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.", "The Lambda function has 500 concurrent executions."], "correct_answer": "There will be at most 100 Lambda function invocations running concurrently.", "explanation": "You can use an **AWS Lambda function** to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.\r\n\r\n***Concurrent executions*** refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source.\r\n\r\nFor Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard\u2019s events in sequence.\r\n\r\nHence, the correct answer in this scenario is that: **there will be at most 100 Lambda function invocations running concurrently.**\r\n\r\nThe option that says: **the Lambda function has 500 concurrent executions** is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function.\r\n\r\nThe option that says: **the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards** is incorrect because, by default, AWS Lambda will automatically scale the function\u2019s concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream.\r\n\r\nThe option that says: **the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function** is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function."}, {"id": 22, "question": "A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity.\r\n\r\nWhich of the following is the MOST appropriate service that the developer should use in this scenario?", "options": ["Serverless Application Framework", "AWS CloudFormation", "AWS SAM", "AWS Systems Manager"], "correct_answer": "AWS SAM", "explanation": "The AWS Serverless Application Model (AWS SAM) is an open source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML.\r\n\r\nAWS SAM is natively supported by AWS CloudFormation and provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.\r\n\r\nHence, the correct answer is **AWS SAM**.\r\n\r\n**AWS CloudFormation** is incorrect. Although this service can deploy the serverless application to AWS, it is still more appropriate to use AWS SAM instead. AWS SAM can simplify the deployment of the serverless application by deploying all related resources together as a single, versioned entity.\r\n\r\n**AWS Systems Manager** is incorrect because it is more focused on management and operations of AWS resources, such as automation, patching, and configuration, but it is not a deployment or application modeling tool.\r\n\r\n**Serverless Application Framework** is incorrect. Although it is a well-known framework for building and deploying serverless applications into the AWS cloud, this is not an AWS native solution. It also does not allow configuration of DynamoDB databases or API Gateway APIs, unlike AWS SAM."}, {"id": 23, "question": "A developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with millisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.\r\n\r\nWhich of the following is the MOST suitable solution to reduce the cost and capacity of the stream?", "options": ["Split cold shards", "Split hot shards", "Merge cold shards", "Merge hot shards"], "correct_answer": "Merge cold shards", "explanation": "The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\r\n\r\nOne approach to resharding could be to split every shard in the stream\u2014which would double the stream\u2019s capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/93bb0d37c31e4f369a3ec81e0279248f.png)\r\n\r\nYou can also use metrics to determine which are your \u201chot\u201d or \u201ccold\u201d shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could ***merge cold shards*** to make better use of their unused capacity.\r\n\r\nYou can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream.\r\n\r\nHence, the correct answer is to **merge cold shards** to reduce the capacity and the cost of running your Kinesis Data Stream.\r\n\r\n**Splitting cold shards** is incorrect because a cold shard is the one that receives fewer data which means that you have to merge them to reduce the capacity rather than split them.\r\n\r\n**Merging hot shards** is incorrect. Although merging shards is correct, the type of shard to be merged is wrong. A hot shard is the one that receives more data in the stream. Merging hot shards could potentially overload the newly merged shard with a high volume of data, causing a bottleneck in processing and degrading the overall performance of the stream.\r\n\r\n**Splitting hot shards** is incorrect because this will actually further increase both the cost and capacity of the stream rather than reduce it. Moreover, there are no hot shards in the stream since the scenario specifically mentioned that the shards are way underutilized.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html"}, {"id": 24, "question": "You have two users concurrently accessing a DynamoDB table and submitting updates. If a user will modify a specific item in the table, she needs to make sure that the operation will not affect another user\u2019s attempt to modify the same item. You have to ensure that your update operations will only succeed if the item attributes meet one or more expected conditions.\r\n\r\nWhich of the following DynamoDB features should you use in this scenario?", "options": ["Conditional writes", "Batch Operations", "Update Expressions", "Projection Expressions"], "correct_answer": "Conditional writes", "explanation": "By default, the **DynamoDB** write operations (`PutItem`, `UpdateItem`, `DeleteItem`) are *unconditional*: each of these operations will overwrite an existing item that has the specified primary key.\r\n\r\nDynamoDB optionally supports **conditional writes** for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in cases where multiple users attempt to modify the same item.\r\n\r\nFor example, by adding a conditional expression that checks if the current value of the item is still the same, you can be sure that your update will not affect the operations of other users:\r\n\r\n```\r\naws dynamodb update-item \\\r\n --table-name ProductCatalog \\\r\n --key '{\"Id\":{\"N\":\"1\"}}' \\\r\n --update-expression \"SET Price = :newval\" \\\r\n --condition-expression \"Price = :currval\" \\\r\n --expression-attribute-values [file://expression-attribute-values.json](file://expression-attribute-values.json/)\r\n\r\n\r\n\r\n<img loading=\"lazy\" decoding=\"async\" src=\"file:///home/skworkstation/.config/joplin-desktop/resources/45f64df20f1b4df2a475bffa6dce5583.png?t=1748305400704\" width=\"674\" height=\"663\" style=\"box-sizing: border-box; border: 0px; font-style: italic; height: auto; max-width: 100%; vertical-align: sub !important; display: block; margin-left: auto; margin-right: auto;\">\r\n```\r\n\r\nHence, the correct answer is **conditional writes**.\r\n\r\n**Using projection expressions** is incorrect because this is just a string that identifies the attributes you want to retrieve during a `GetItem`, `Query`, or `Scan` operation. Take note that the scenario calls for a feature that can be used during a write operation hence, this option is irrelevant.\r\n\r\n**Using update expressions** is incorrect because this simply specifies how `UpdateItem` will modify the attributes of an item such as for setting a scalar value or removing elements from a list or a map. This feature doesn\u2019t use any conditions which is what the scenario is looking for. Therefore, this option is incorrect.\r\n\r\n**Using batch operations** is incorrect because these are essentially wrappers for multiple read or write requests. Batch operations are primarily used when you want to retrieve or submit multiple items in DynamoDB through a single API call, which reduces the number of network round trips from your application to DynamoDB.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ReadingData\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.html\r\n\r\n**Check out this Amazon DynamoDB Cheat Sheet:**\r\n\r\nhttps://tutorialsdojo.com/amazon-dynamodb/"}, {"id": 25, "question": "A company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the different processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each process to ensure application performance.\r\n\r\nWhich of the following is the MOST suitable solution that the developer should implement?", "options": ["Use Enhanced Monitoring in RDS.", "Develop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance.", "Use CloudWatch to track the CPU Utilization of your database.", "Track the CPU% and MEM% metrics which are readily available in the Amazon RDS console."], "correct_answer": "Use Enhanced Monitoring in RDS.", "explanation": "**Amazon RDS** provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the `RDSOSMetrics` log group in the CloudWatch console.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/dd2718487c7c4d85ad6a05d9da823421.png)\r\n\r\nTake note that there are certain differences between CloudWatch and Enhanced Monitoring Metrics. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work.\r\n\r\nThe differences can be greater if your DB instances use smaller instance classes because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.\r\n\r\nHence, the correct answer is to **use Enhanced Monitoring in RDS**.\r\n\r\n**Developing a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance** is incorrect. Although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database process. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.\r\n\r\n**Using CloudWatch to track the CPU Utilization of your database** is incorrect. Although you can use Amazon CloudWatch to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.\r\n\r\n**Tracking the `CPU%` and `MEM%` metrics which are readily available in the Amazon RDS console** is incorrect because these metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch\r\n\r\n**Check out these Amazon CloudWatch and RDS Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/amazon-cloudwatch/\r\n\r\n**https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/**"}, {"id": 26, "question": "A company has a suite of web applications that is heavily using RDS database in Multi-AZ Deployments configuration with several Read Replicas. For improved security, you were instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis. You should also configure your applications to use the latest version of the encrypted credentials when connecting to the RDS database.\r\n\r\nWhich of the following is the MOST appropriate solution to secure the credentials?\r\n\r\n", "options": ["Store the credentials in AWS KMS.", "Store the credentials to AWS ACM.", "Store the credentials to Systems Manager Parameter Store with a SecureString data type.", "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation."], "correct_answer": "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation.", "explanation": "**AWS Secrets Manager** is an AWS service that makes it easier for you to manage secrets. *Secrets* can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs.\r\n\r\nIn the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/fe3ac9000d2641d98d3d532b640e1ff4.png)\r\n\r\n**Secrets Manager** enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can\u2019t be compromised by someone examining your code, because the secret simply isn\u2019t there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.\r\n\r\nHence, **using AWS Secrets Manager to store and encrypt the credentials and enabling automatic rotation** is the most appropriate solution for this scenario.\r\n\r\n**Storing the credentials to Systems Manager Parameter Store with a `SecureString` data type** is incorrect because, by default, Systems Manager Parameter Store doesn\u2019t rotate its parameters which is one of the requirements in the above scenario.\r\n\r\n**Storing the credentials to AWS ACM** is incorrect because it is just a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates to allow SSL communication to your application. This is not a suitable service to store database or any other confidential credentials.\r\n\r\n**Storing the credentials in AWS KMS** is incorrect because this only makes it easy for you to create and manage encryption keys and control the use of encryption across a wide range of AWS services. This is primarily used for encryption and not for hosting your credentials.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\r\n\r\nhttps://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/\r\n\r\n**Check out these AWS Systems Manager and Secrets Manager Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/aws-systems-manager/\r\n\r\nhttps://tutorialsdojo.com/aws-secrets-manager/\r\n\r\n&nbsp;"}, {"id": 27, "question": "A developer is utilizing AWS X-Ray to generate a visual representation of the requests flowing through their enterprise web application. Since the application interacts with multiple services, all requests must be traced in X-Ray, including any downstream calls made to AWS resources.\r\n\r\nWhich of the following actions should the developer implement for this scenario?", "options": ["Install AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls.", "Pass multiple trace segments as a parameter of PutTraceSegments API.", "Use AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API.", "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches."], "correct_answer": "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches.", "explanation": "You can send trace data to X-Ray in the form of segment documents. A **segment document** is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments or work that uses downstream services and resources in subsegments.\r\n\r\nA segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the `PutTraceSegments` API. An alternative is, instead of sending segment documents to the X-Ray API, you can send segments and subsegments to an X-Ray daemon, which will buffer them and upload to the X-Ray API in batches. The X-Ray SDK sends segment documents to the daemon to avoid making calls to AWS directly. This is the correct option among the choices.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/322d4919e11e4f1cb6282f3d963f180e.png)\r\n\r\nHence, **using X-Ray SDK to generate segment documents with subsegments and sending them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches** is the correct answer in this scenario.\r\n\r\n**Using AWS X-Ray SDK to upload a trace segment by executing `PutTraceSegments` API** is incorrect because you should upload the segment documents with subsegments instead. A trace segment is just a JSON representation of a request that your application serves.\r\n\r\n**Installing AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls** is incorrect because you cannot run a trace on the application and the services at the same time as this will produce two different results. You simply have to send the segment documents with subsegments to get the information about downstream calls that your application makes to AWS resources.\r\n\r\n**Passing multiple trace segments as a parameter of `PutTraceSegments` API** is incorrect because, contrary to the API\u2019s name, you have to upload **segment** documents and not trace segments. The API has a single parameter: `TraceSegmentDocuments`, that takes a list of JSON segment documents.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\r\n\r\n"}, {"id": 28, "question": "A developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities of data, the write capacity units must be specified for the expected workload on both the base table and its secondary index.\r\n\r\nWhich of the following should the developer do to avoid any potential request throttling?", "options": ["Ensure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.", "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.", "Ensure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.", "Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table."], "correct_answer": "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.", "explanation": "A **global secondary index (GSI)** is an index with a partition key and a sort key that can be different from those on the base table. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nEvery global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.\r\n\r\nWhen you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A `Query` operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/b6e6a4f0ec9a4a738194046eca1d9cc5.gif)\r\n\r\nFor example, if you `Query` a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled.\r\n\r\nTo avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.\r\n\r\nHence, the correct answer in this scenario is to **ensure that the global secondary index\u2019s provisioned WCU is equal to or greater than the WCU of the base table**.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned WCU is equal or less than the WCU of the base table** is incorrect because it should be the other way around, just as what is mentioned above. The provisioned write capacity for a global secondary index should be equal to or greater than the write capacity of the base table.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal to or greater than the RCU of the base table** is incorrect because you have to set the WCU and not the RCU.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal or less than the RCU of the base table** is incorrect because this should be WCU and in addition, the global secondary index\u2019s provisioned WCU should be set to a value that is equal or greater than the WCU of the base table to prevent request throttling.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes\r\n"}, {"id": 29, "question": "You currently have an IAM user for working in the development environment using shell scripts that call the AWS CLI. The EC2 instance that you are using already contains the access key credential set and an IAM role, which are used to run the CLI and access the development environment. You were given a new set of access key credentials with another IAM role that allows you to access and manage the production environment.\r\n\r\nWhich of the following is the EASIEST way to switch from one role to another?", "options": ["Store the production access key credentials set in the instance metadata and call this whenever you need to access the production environment.", "Store the production access key credentials set in the user data of the instance and call this whenever you need to access the production environment.", "Create a new instance profile in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.", "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command."], "correct_answer": "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.", "explanation": "Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications.\r\n\r\nThis extra step is the creation of an *[instance profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)* that is attached to the instance. The instance profile contains the role and can provide the role\u2019s temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application\u2019s API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/52366299d8854f979e988e23c01c2a3a.png)\r\n\r\nUsing roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don\u2019t have to manage credentials, and you don\u2019t have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances.\r\n\r\nImagine that you have an IAM user for working in the development environment and you occasionally need to work with the production environment at the command line with the [AWS CLI](http://aws.amazon.com/cli/). You already have an access key credential set available to you. This can be the access key pair that is assigned to your standard IAM user. Or, if you signed in as a federated user, it can be the access key pair for the role that was initially assigned to you. If your current permissions grant you the ability to assume a specific IAM role, then you can identify that role in a \u201cprofile\u201d in the AWS CLI configuration files. That command is then run with the permissions of the specified IAM role, not the original identity.\r\n\r\nNote that when you specify that profile in an AWS CLI command, you are using the new role. In this situation, you cannot make use of your original permissions in the development account at the same time. The reason is that only one set of permissions can be in effect at a time.\r\n\r\nHence, the correct answer is to **create a new profile for the role in the AWS CLI configuration file then append the `--profile` parameter, along with the new profile name, whenever you run the CLI command**.\r\n\r\n**Storing the production access key credentials set in the instance metadata and calling this whenever you need to access the production environment** is incorrect because instance metadata is primarily used to fetch the data about your instance that you can use to configure or manage the running instance. This is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**Creating a new instance profile in the AWS CLI configuration file then appending the `--profile` parameter along with the new profile name whenever you run the CLI command** is incorrect because an instance profile is just a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is different from an AWS CLI **profile**, which you can use for switching to various profiles. In addition, an instance profile is associated with the instance and not configured in the AWS CLI.\r\n\r\n**Storing the production access key credentials set in the user data of the instance and calling this whenever you need to access the production environment** is incorrect because user data is primarily used to configure an instance during launch, or to run a configuration script. Just like instance metadata, this is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\r\n\r\n**Check out this AWS Identity & Access Management (IAM) Cheat Sheet:**\r\n\r\nhttps://tutorialsdojo.com/aws-identity-and-access-management-iam/"}, {"id": 30, "question": "A company has an application hosted in an On-Demand EC2 instance in your VPC. The developer has been instructed to create a shell script that fetches the instance\u2019s associated public and private IP addresses.\r\n\r\nWhat should the developer do to complete this task?", "options": ["Get the public and private IP addresses from AWS CloudTrail.", "Get the public and private IP addresses from Amazon CloudWatch.", "Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint.", "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint."], "correct_answer": "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint.", "explanation": "**Instance metadata** is data about your EC2 instance that you can use to configure or manage the running instance. Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you\u2019re writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/91b0c543781c40faaa38c368e3616c23.png)\r\n\r\nTo view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL: `http://169.254.169.254/latest/meta-data/`.\r\n\r\nHence, the correct answer is: **Get the public and private IP addresses from the instance metadata service using the `http://169.254.169.254/latest/meta-data/` endpoint.**\r\n\r\nThe option that says: **Get the public and private IP addresses from Amazon CloudWatch** is incorrect because there is no direct way to fetch the public and private IP addresses of the EC2 instance using CloudWatch.\r\n\r\nThe option that says: **Get the public and private IP addresses from AWS CloudTrail** is incorrect because CloudTrail is primarily used to track the API activity of each AWS service. Just like CloudWatch, there is no easy way to get the associated IP addresses of the EC2 instance using CloudTrail.\r\n\r\nThe option that says: **Get the public and private IP addresses from the instance user data service using the `http://169.254.169.254/latest/userdata/` endpoint** is incorrect because a user data is mainly used to perform common automated configuration tasks and run scripts after the instance starts. You will not find the associated IP addresses of the EC2 instance from its user data. You have to use the metadata service instead.\r\n\r\n**References:**\r\n\r\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html"}, {"id": 31, "question": "A leading financial company has recently deployed its application to AWS using Lambda and API Gateway. However, they noticed that all metrics are being populated in their CloudWatch dashboard except for `CacheHitCount` and `CacheMissCount`.\r\n\r\nWhat could be the MOST likely cause of this issue?", "options": ["API Caching is not enabled in API Gateway.", "The provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch.", "They have not provided an IAM role to their API Gateway yet.", "API Gateway Private Integrations has not been configured yet."], "correct_answer": "API Caching is not enabled in API Gateway.", "explanation": "You can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics. These statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your web application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/e7a5dad826b342b68617a7de98552fbc.png)\r\n\r\nThe metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list.\r\n\r\n\u2013 Monitor the **IntegrationLatency** metrics to measure the responsiveness of the backend.\r\n\r\n\u2013 Monitor the **Latency** metrics to measure the overall responsiveness of your API calls.\r\n\r\n\u2013 Monitor the **CacheHitCount** and **CacheMissCount** metrics to optimize cache capacities to achieve a desired performance. CacheMissCount tracks the number of requests served from the backend in a given period, <ins>when API caching is enabled</ins>. On the other hand, CacheHitCount track the number of requests served from the API cache in a given period.\r\n\r\nHence, the root cause of this issue is that the **API Caching is not enabled in API Gateway** which is why the ***CacheHitCount*** and ***CacheMissCount*** metrics are not populated.\r\n\r\nThe option that says: **they have not provided an IAM role to their API Gateway yet** is incorrect because, in the first place, the scenario already mentioned that all metrics are being populated in their CloudWatch dashboard except for two metrics. This implies that some of the metrics are populated which means that the API Gateway already has an IAM Role associated with it.\r\n\r\nThe option that says: **the provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch** is incorrect because just as what is mentioned above, there is no issue with the IAM Role since all metrics are being populated except only for `CacheHitCount` and `CacheMissCount`. This means that the associated IAM Role already has *write* privileges to write logs to CloudWatch to begin with. The only reason why those two metrics are not being populated is that the API Caching is not enabled.\r\n\r\nThe option that says: **API Gateway Private Integrations has not been configured yet** is incorrect because this feature only makes it easier to expose your HTTP/HTTPS resources behind an Amazon VPC for access by clients outside of the VPC.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring-cloudwatch.html"}, {"id": 32, "question": "Your application is hosted on an Auto Scaling group of EC2 instances with a DynamoDB database. There were a lot of data discrepancy issues where the changes made by one user were always overwritten by another user. You noticed that this usually happens whenever there are a lot of people updating the same data.\r\n\r\nWhat should you do to solve this problem?", "options": ["Implement a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.", "Use DynamoDB global tables and implement a pessimistic locking strategy.", "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.", "Use DynamoDB global tables and implement an optimistic locking strategy."], "correct_answer": "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.", "explanation": "**Optimistic locking** is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others \u2014 and vice-versa. Take note that:\r\n\r\n\u2013 DynamoDB global tables use a \u201clast writer wins\u201d reconciliation between concurrent updates. If you use Global Tables, last writer policy wins. So in this case, the locking strategy does not work as expected.\r\n\r\n\u2013 DynamoDBMapper transactional operations do not support optimistic locking.\r\n\r\nWith optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes.\r\n\r\nHence, **implementing an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is the correct answer in this scenario.\r\n\r\n**Using DynamoDB global tables and implementing a pessimistic locking strategy** is incorrect because you have to use optimistic locking here just as what was explained above.\r\n\r\n**Implementing a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is incorrect because an optimistic locking strategy is a more suitable solution for this scenario. Although the provided steps here are correct, the name of the strategy is wrong.\r\n\r\n**Using DynamoDB global tables and implementing an optimistic locking strategy** is incorrect. Although it is correct to use the optimistic locking strategy, the use of DynamoDB global tables is wrong. This uses a *\u201clast writer wins\u201d* reconciliation between concurrent updates. If you use Global Tables, the last writer policy is in effect so in this case, the locking strategy will not work as expected.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html"}, {"id": 33, "question": "A company has recently developed a containerized application that uses a multicontainer Docker platform which supports multiple containers per instance. They need a service that automatically handles tasks such as provisioning of the resources, load balancing, auto-scaling, monitoring, and placing the containers across the cluster.\r\n\r\nWhich of the following services provides the EASIEST way to accomplish the above requirement?", "options": ["Elastic Beanstalk", "EKS", "Lambda", "ECS"], "correct_answer": "Elastic Beanstalk", "explanation": "You can create docker environments that support multiple containers per Amazon EC2 instance with multicontainer Docker platform for Elastic Beanstalk.\r\n\r\n**Elastic Beanstalk** uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multicontainer Docker environments. Amazon ECS provides tools to manage a cluster of instances running Docker containers. Elastic Beanstalk takes care of Amazon ECS tasks including cluster creation, task definition and execution.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/ba1c0749eacd4161a6e951f787301a23.png)\r\n\r\n**AWS Elastic Beanstalk** is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links.\r\n\r\nElastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**ECS** is incorrect. Although it can host Docker applications, it doesn\u2019t automatically handle all the details such as resource provisioning, balancing load, auto-scaling, monitoring, and placing your containers across your cluster, unlike Elastic Beanstalk. Take note that even though you can use Service Auto Scaling in ECS, you still have to enable and configure it. Elastic Beanstalk still provides the easiest way to accomplish the requirements.\r\n\r\n**Lambda** is incorrect because this is primarily used for serverless applications and not for Docker or any other containerized applications.\r\n\r\n**EKS** is incorrect because Amazon EKS just provides you an easy way to run Kubernetes on AWS without needing to install and operate your own Kubernetes clusters.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\r\n\r\nhttps://aws.amazon.com/ecs/faqs/"}, {"id": 34, "question": "A leading insurance firm is hosting its customer portal in Elastic Beanstalk, which has an RDS database in AWS. The support team in your company discovered a lot of SQL injection attempts and cross-site scripting attacks on the portal, which is starting to affect the production environment.\r\n\r\nWhich of the following services should you implement to mitigate this attack?", "options": ["AWS WAF", "Amazon Guard\u200bDuty", "Network Access Control List", "AWS Firewall Manager"], "correct_answer": "AWS WAF", "explanation": "**AWS WAF** is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an Application Load Balancer responds to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/39fd073d4cfb4dd89e38274ccb4e79c6.png)\r\n\r\nAt the simplest level, AWS WAF lets you choose one of the following behaviors:\r\n\r\n**Allow all requests except the ones that you specify** \u2013 This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers.\r\n\r\n**Block all requests except the ones that you specify** \u2013 This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website.\r\n\r\n**Count the requests that match the properties that you specify** \u2013 When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn\u2019t accidentally configure AWS WAF to block all the traffic to your website. When you\u2019re confident that you specified the correct properties, you can change the behavior to allow or block requests.\r\n\r\nHence, the correct answer in this scenario is **AWS WAF.**\r\n\r\n**Amazon Guard\u200bDuty** is incorrect because this is just a threat detection service that continuously monitors malicious activity and unauthorized behavior to protect your AWS accounts and workloads.\r\n\r\n**AWS Firewall Manager** is incorrect because this just simplifies your AWS WAF and AWS Shield Advanced administration and maintenance tasks across multiple accounts and resources.\r\n\r\n**Network Access Control List** is incorrect because this is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/waf/\r\n\r\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\r\n\r\nhttps://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/"}]
