[
  {
    "id": 1,
    "question": "Which AWS service is primarily used for storing static files?",
    "options": [
      "EC2",
      "S3",
      "DynamoDB",
      "RDS"
    ],
    "correct_answer": "S3",
    "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance for storing static files."
  },
  {
    "id": 2,
    "question": "Which AWS service would you use to run containers?",
    "options": [
      "EC2",
      "S3",
      "ECS/EKS",
      "Lambda"
    ],
    "correct_answer": "ECS/EKS",
    "explanation": "Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service) are services designed specifically for running containers in AWS."
  },
  {
    "id": 3,
    "question": "A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.\r\n\r\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?",
    "options": [
      "4 shards : 2 instances",
      "1 shard : 6 instances",
      "6 shards : 1 instance",
      "4 shards : 8 instances"
    ],
    "correct_answer": "4 shards : 2 instances",
    "explanation": "A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\n\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nSince the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer.\r\n\r\nThe 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.\r\n\r\nThe 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn\u2019t have a backup instance to process the shards in the event of an outage.\r\n\r\nThe 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well.\r\n\r\n"
  },
  {
    "id": 4,
    "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\r\n\r\nHow should the developer retrieve the variables with the FEWEST application changes? ",
    "options": [
      "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
      " Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment. ",
      " Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment. ",
      "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process. "
    ],
    "correct_answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
    "explanation": "A is correct:\r\n\r\n- It uses the appropriate services for the right types of data (Parameter Store for configuration, Secrets Manager for sensitive credentials)\r\n\r\n- It provides a centralized approach that requires minimal application changes\r\n- It supports hierarchical organization for different environments\r\n- It provides robust security controls through IAM\r\n- It enables changes to parameters without application redeployment\r\n\r\nThe application would only need to be updated once to retrieve variables from these services, and then all future changes to the variables would be managed through the services without additional application changes."
  },
  {
    "id": 5,
    "question": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.\r\n\r\nWhich section of a CloudFormation template cannot be associated with Condition?",
    "options": [
      "Conditions",
      "Resources",
      "Outputs",
      "Parameters"
    ],
    "correct_answer": "Parameters",
    "explanation": "Parameters\r\n\r\nParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\r\n\r\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\r\n\r\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.\r\n\r\nConditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.\r\n\r\nPlease review this note for more details:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\r\n\r\nPlease visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.\r\n\r\nIncorrect options:\r\n\r\nResources - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.\r\n\r\nConditions - You actually define conditions in this section of the CloudFormation template\r\n\r\nOutputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create."
  },
  {
    "id": 6,
    "question": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\r\n\r\nAs a Developer Associate, which of the following options would you recommend for the given use-case?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Cognito User Pools",
      "Lambda Authorizer",
      "API Gateway User Pools",
      "IAM permissions with sigv4"
    ],
    "correct_answer": "Lambda Authorizer",
    "explanation": "Correct option:\r\n\r\n\"Lambda Authorizer\"\r\n\r\nAn Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.\r\n\r\n via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nIncorrect options:\r\n\r\n\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.\r\n\r\n\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.\r\n\r\n\"API Gateway User Pools\" - This is a made-up option, added as a distractor.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"
  },
  {
    "id": 7,
    "question": "You're designing an application that processes data from an Amazon Kinesis Data Stream. Your current architecture includes a Kinesis stream with 8 shards. Each EC2 instance in your application runs a single Kinesis Client Library (KCL) consumer application.\r\n\r\nWhat is the optimal number of EC2 instances you should deploy to efficiently process this Kinesis stream?",
    "options": [
      "4 instances",
      " 8 instances",
      "16 instances",
      "32 instances"
    ],
    "correct_answer": " 8 instances",
    "explanation": "When designing applications that process Amazon Kinesis Data Streams, it's important to understand the relationship between shards and KCL workers for optimal performance.\r\n\r\nThe best practice is to match the number of instances (each running one KCL worker) to the number of shards in your Kinesis stream. This is because:\r\n\r\n- Each shard in a Kinesis Data Stream can be processed by exactly one KCL worker at any given time\r\n\r\n- A single KCL worker can process multiple shards, but this may not be optimal for performance\r\n\r\n- Having more KCL workers than shards is inefficient as some workers would be idle\r\n\r\n- Having fewer KCL workers than shards means some workers would be processing multiple shards, which might create a bottleneck\r\n\r\nIn this scenario, with 8 shards in your Kinesis stream, the optimal configuration would be 8 EC2 instances, each running one KCL worker application. This provides a 1:1 mapping between shards and KCL workers, ensuring maximum throughput and parallel processing capability.\r\n\r\nIf your data processing needs change, you would typically adjust both the number of shards and the number of instances accordingly to maintain this optimal ratio."
  },
  {
    "id": 8,
    "question": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\r\n\r\n\r\nWhich of the following does **NOT** help with debugging the issue?",
    "options": [
      "X-Ray sampling",
      "EC2 Instance Role",
      "EC2 X-Ray Daemon",
      "CloudTrail"
    ],
    "correct_answer": "X-Ray sampling",
    "explanation": "Correct option:\r\n\r\n**X-Ray sampling**\r\n\r\nWhy X-Ray sampling WON'T help:\r\nX-Ray sampling only controls which requests get traced, not whether those traces successfully reach X-Ray. Adjusting sampling rules is like deciding how many photos to take, but won't help if the camera can't transmit pictures to your cloud storage.\r\n\r\n\r\n**EC2 X-Ray Daemon**\r\n\r\n- The X-Ray daemon is the component that actually sends trace data to AWS\r\n\r\n- Checking daemon logs would show connection errors, timeouts, or permission issues\r\n\r\n- You could verify if the daemon is running correctly with ps aux | grep xray\r\n\r\n- The daemon log file at /var/log/xray/xray.log might contain error messages\r\n\r\n\r\n#### 2. EC2 Instance Role\r\n* The X-Ray daemon needs proper IAM permissions to send data to X-Ray\r\n* The EC2 instance role provides these permissions automatically\r\n* Checking the attached role and its policies would reveal missing permissions\r\n* You could verify the policy includes `xray:PutTraceSegments` and `xray:PutTelemetryRecords`\r\n\r\n\r\n#### 3. CloudTrail\r\n* CloudTrail logs all API calls made to AWS services\r\n* It would show denied API calls due to permission issues\r\n* You could search for X-Ray related API calls from your EC2 instance\r\n* Failed API calls would include detailed error messages explaining why they failed\r\n\r\n## Key Troubleshooting Steps\r\n\r\nIn this scenario, you should:\r\n1. Check if the X-Ray daemon is running on the EC2 instance\r\n2. Verify the EC2 instance role has appropriate X-Ray permissions\r\n3. Look at CloudTrail logs for denied X-Ray API calls\r\n4. Check security groups and network ACLs to ensure outbound traffic to X-Ray endpoints is allowed\r\n\r\nAdjusting sampling rules would not provide any useful diagnostic information since no data is being transmitted at all."
  },
  {
    "id": 9,
    "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket, the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).\r\n\r\nWhich solution will guarantee that any upload request without the mandated encryption is not processed?",
    "options": [
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `aws:kms`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `sse:s3`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header."
    ],
    "correct_answer": "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
    "explanation": "### Why Option C is Correct:\r\nWhen using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), you need to:\r\n\r\n1. Set the `x-amz-server-side-encryption` header to `AES256` in your PutObject requests\r\n2. Implement a bucket policy that enforces this requirement\r\n\r\nThis approach works because:\r\n* `AES256` is the correct value that specifies SSE-S3 encryption (Amazon S3-managed keys)\r\n* The bucket policy can deny any requests that either:\r\n  * Don't include the encryption header at all\r\n  * Include the header but with an incorrect value\r\n\r\nA typical bucket policy would look like this:\r\n```json\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis policy uses two statements:\r\n* The first statement denies requests where the header exists but has an incorrect value\r\n* The second statement denies requests where the header is missing entirely\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option A is incorrect** because:\r\n* `aws:kms` is used for SSE-KMS (Server-Side Encryption with AWS KMS keys)\r\n* The question specifically requires SSE-S3 (Server-Side Encryption with Amazon S3-managed keys)\r\n* While this would enforce encryption, it would use the wrong type of encryption\r\n\r\n**Option B is incorrect** because:\r\n* `sse:s3` is not a valid value for the `x-amz-server-side-encryption` header\r\n* The valid values are `AES256` (for SSE-S3) or `aws:kms` (for SSE-KMS)\r\n* Using an invalid header value would cause all requests to fail\r\n\r\n**Option D is incorrect** because:\r\n* With SSE-S3, you don't specify or manage the encryption keys\r\n* Amazon S3 automatically handles key management, generating a unique key for each object\r\n* There is no way to \"set the encryption key for SSE-S3\" in an HTTP header\r\n\r\n### Key Concept:\r\nWhen using SSE-S3, remember that:\r\n1. Amazon S3 handles all key management automatically\r\n2. Each object is encrypted with a unique key\r\n3. These keys are themselves encrypted with a master key that AWS rotates regularly\r\n4. The encryption standard used is AES-256\r\n5. You only need to specify the encryption method (`AES256`), not any keys\r\n\r\n## AWS Documentation Reference\r\nFor more information, refer to the [Amazon S3 Developer Guide on Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)."
  },
  {
    "id": 10,
    "question": "A development team has created a serverless application that uses Amazon API Gateway and AWS Lambda. They want to use a single Lambda function across multiple API Gateway stages (development, testing, and production), but they need the function to read from a different DynamoDB table depending on which stage is being called. \r\n\r\nWhat is the MOST appropriate way for the developer to pass these configuration parameters to the Lambda function?",
    "options": [
      "Use Stage Variables in API Gateway and reference them in mapping templates",
      "Set up an API Gateway Private Integration to the Lambda function",
      "Create environment variables in the Lambda function for each table name",
      "Configure traffic shifting with Lambda Aliases for each stage"
    ],
    "correct_answer": "Use Stage Variables in API Gateway and reference them in mapping templates",
    "explanation": "### Why Option A is Correct:\r\nStage variables are name-value pairs that function as configuration attributes for different deployment stages of your API Gateway REST API. They effectively work like environment variables that can be accessed from various parts of your API configuration, including mapping templates.\r\n\r\nThis solution works perfectly for the scenario because:\r\n\r\n1. **Dynamic Configuration Per Stage**: Stage variables allow you to set different values for each deployment stage (development, testing, production)\r\n\r\n2. **Accessible in Mapping Templates**: You can reference stage variables in the mapping templates that generate the request for your Lambda function\r\n\r\n3. **No Code Changes Needed**: The Lambda function code remains the same across all environments, making maintenance easier\r\n\r\n4. **Implementation Example**:\r\n   ```json\r\n   // API Gateway mapping template example\r\n   {\r\n     \"tableName\": \"$stageVariables.dynamoDBTableName\",\r\n     \"operation\": \"read\",\r\n     \"key\": {\r\n       \"id\": \"$input.params('id')\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In this example, `$stageVariables.dynamoDBTableName` would contain different values in different stages:\r\n   - In development: \"dev-customer-table\"\r\n   - In testing: \"test-customer-table\"\r\n   - In production: \"prod-customer-table\"\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option B is incorrect** because:\r\n* API Gateway Private Integration is used to connect API Gateway to private resources in your VPC\r\n* It doesn't provide a way to dynamically configure which DynamoDB table the Lambda function should use\r\n* It's designed for network connectivity, not for configuration parameter passing\r\n\r\n**Option C is incorrect** because:\r\n* Lambda environment variables are static for a given function and don't change based on which API stage called the function\r\n* While you could check the stage name in your Lambda code and use different tables based on that, this would require code changes and additional logic\r\n* This approach would be less maintainable as it mixes configuration with application code\r\n\r\n**Option D is incorrect** because:\r\n* Lambda Aliases are used to point to specific versions of Lambda functions\r\n* Traffic shifting with aliases is for gradually moving traffic between different versions of a function\r\n* This doesn't solve the problem of using different DynamoDB tables without modifying the function code\r\n* This approach would require maintaining multiple versions of essentially the same code with different table names hardcoded\r\n\r\n### Key Concept:\r\nAPI Gateway Stage Variables provide a clean separation of configuration from code, allowing you to deploy the same Lambda function code to multiple environments while dynamically changing its behavior based on which stage is calling it.\r\n\r\n## AWS Documentation Reference\r\nFor more information on using stage variables with Lambda functions, refer to the [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html)."
  },
  {
    "id": 11,
    "question": "A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department.\r\n\r\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.\r\n\r\nWhich of the following options is the MOST suitable solution that the developer implements?\r\n\r\n",
    "options": [
      "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
      "Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.",
      "Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.",
      "Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls."
    ],
    "correct_answer": "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
    "explanation": "The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.\r\n\r\nIt is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following:\r\n\r\n-The ability to send traces to multiple different tracing backends without having to re-instrument your code\r\n\r\n-Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community\r\n\r\n-Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js\r\n\r\n\r\nConversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following:\r\n\r\n-A tightly integrated single-vendor solution\r\n\r\n-Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET\r\n\r\nAn account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default:\r\n\r\nhttps://Your_AWS_Account_ID.signin.aws.amazon.com/console/\r\n\r\nIf you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example.\r\n\r\nhttps://Your_Alias.signin.aws.amazon.com/console/\r\n\r\nThe original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account:\r\n\r\naws iam create-account-alias --account-alias demosite.com\r\n\r\n**Hence, for this scenario, the correct answer is:** Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\r\n\r\n**The option that says:** Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature.\r\n\r\n**The option that says:** Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA).\r\n\r\n**The option that says:**Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn\u2019t have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry."
  },
  {
    "id": 12,
    "question": "An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\r\n\r\n\r\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?\r\n\r\n",
    "options": [
      "Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.",
      "Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.",
      "Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.",
      "Update the stacks on multiple AWS accounts using CloudFormation StackSets."
    ],
    "correct_answer": "Update the stacks on multiple AWS accounts using CloudFormation StackSets.",
    "explanation": "**AWS CloudFormation StackSets** extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\r\n\r\n\r\n\r\nA ***stack set*** lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set\u2019s AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.\r\n\r\nHence, the correct solution in this scenario is to **update the stacks on multiple AWS accounts using CloudFormation StackSets.**\r\n\r\n&nbsp;\r\n\r\nAfter you\u2019ve defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions.\r\n\r\nThe option that says: **Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets** is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets.\r\n\r\nThe option that says: **Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances** is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets.\r\n\r\nThe option that says: **Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts** is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"
  },
  {
    "id": 13,
    "question": "A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition.\r\n\r\nWhich of the following is the EASIEST way to accomplish this task?\r\n\r\n",
    "options": [
      "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",
      "Use AWS SAM to migrate and deploy the company's web services to API Gateway.",
      "Create models and templates for request and response mappings based on the company's API definitions.",
      "Use CodeDeploy to migrate and deploy the company's web services to API Gateway."
    ],
    "correct_answer": "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",
    "explanation": "You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.\r\n\r\nYou can paste a [Swagger](http://swagger.io/) API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below:\r\n\r\n\r\nYou can also import your Swagger definition through the AWS CLI and SDKs.\r\n\r\nHence, the correct answer in this scenario is to **import their Swagger or OpenAPI definitions to API Gateway using the AWS Console**.\r\n\r\n**Using CodeDeploy to migrate and deploy the company\u2019s web services to API Gateway** is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment.\r\n\r\n**Using AWS SAM to migrate and deploy the company\u2019s web services to API Gateway** is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS.\r\n\r\n**Creating models and templates for request and response mappings based on the company\u2019s API definitions** is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file."
  },
  {
    "id": 14,
    "question": "A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don\u2019t need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring.\r\n\r\nWhich service will you use to easily deploy and manage the application?\r\n\r\n",
    "options": [
      "AWS Elastic Beanstalk",
      "AWS CloudFormation",
      "AWS CodeDeploy",
      "AWS SAM"
    ],
    "correct_answer": "AWS Elastic Beanstalk",
    "explanation": "With **Elastic Beanstalk**, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n&nbsp;\r\n\r\nYou can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).\r\n\r\nTo use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**AWS CloudFormation** is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can\u2019t host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n**AWS CodeDeploy** is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."
  },
  {
    "id": 15,
    "question": "A mobile game has a DynamoDB table named `AWSDevScores` which keeps track of the users and their respective scores. Each item in the table is identified by the `FighterId` attribute as its partition key and the `FightTitle` attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named `AWSDevTopScores` and `AWSDevDateTime` attributes.\r\n\r\n\r\nWhich type of index should the developer add in the table to speed up queries on non-key attributes?\r\n\r\n",
    "options": [
      "Primary Index",
      "Sparse Index",
      "Global Secondary Index",
      "Local Secondary Index"
    ],
    "correct_answer": "Global Secondary Index",
    "explanation": "Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue `Query` or `Scan` requests against these indexes.\r\n\r\n&nbsp;\r\n\r\nA ***secondary index*** is a data structure that contains a subset of attributes from a table, along with an alternate key to support `Query` operations. You can retrieve data from the index using a `Query`, in much the same way as you use `Query` with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nTo speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn\u2019t even need to have the same key schema as a table.\r\n\r\nHence, the correct answer in this scenario is to add a **Global Secondary Index**.\r\n\r\n**Sparse index** is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn\u2019t appear in every table item, the index is said to be *\u201csparse\u201d*.\r\n\r\n**Local Secondary Index** is incorrect because this is used for queries which use the same partition key value, and in addition, you can\u2019t add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.\r\n\r\n**Primary Index** is incorrect because this one actually refers to the partition key, which is the `FighterId` attribute in this scenario."
  },
  {
    "id": 16,
    "question": "A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture.\r\n\r\nWhich of the following services is the MOST suitable one to use?",
    "options": [
      "AWS SAM",
      "EC2",
      "Elastic Beanstalk",
      "ECS"
    ],
    "correct_answer": "ECS",
    "explanation": "**Amazon Elastic Container Service (Amazon ECS)** is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.\r\n\r\nYou can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\r\n\r\nElastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more **fine-grained** control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **ECS.**\r\n\r\n**Elastic Beanstalk** is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications.\r\n\r\n**EC2** is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB.\r\n\r\n"
  },
  {
    "id": 17,
    "question": "An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available.\r\n\r\nWhich of the following is the most suitable feature that you have to use to meet this requirement?",
    "options": [
      "AWS Device Farm",
      "Amazon Cognito Identity Pools",
      "Amazon Cognito Sync",
      "Amazon Cognito User Pools"
    ],
    "correct_answer": "Amazon Cognito Sync",
    "explanation": "Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.\r\n\r\nAmazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user\u2019s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.\r\n\r\nThe Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize.\r\n\r\nAmazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.\r\n\r\nHence, the correct answer is to **Amazon Cognito Sync***.*\r\n\r\n**Amazon Cognito User Pools** is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito.\r\n\r\n**Amazon Cognito Identity Pools** is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\n**AWS Device Farm** is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.\r\n\r\n&nbsp;"
  },
  {
    "id": 18,
    "question": "A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing.\r\n\r\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?",
    "options": [
      "Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.",
      "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
      "Postpone the delivery of new messages by using a delay queue.",
      "Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0."
    ],
    "correct_answer": "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
    "explanation": "The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.\r\n\r\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\u2019t automatically delete the message. Because Amazon SQS is a distributed system, there\u2019s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.\r\n\r\nImmediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a ***visibility timeout***, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.\r\n\r\nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn\u2019t call the `[DeleteMessage](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html)` action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout.\r\n\r\nEvery Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout.\r\n\r\nHence, the best solution in this scenario is to **set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue**.\r\n\r\n**Configuring the queue to use short polling by setting the `WaitTimeSeconds` parameter of the `ReceiveMessage` request to 0** is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario.\r\n\r\n**Configuring the queue to use long polling by setting the `Receive Message Wait Time` parameter to a value greater than 0** is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (*when there are no messages available for a ReceiveMessage request*) and false empty responses (*when messages are available but aren\u2019t included in a response*). A more appropriate solution in this scenario is to configure the visibility timeout of the messages.\r\n\r\n**Postponing the delivery of new messages by using a delay queue** is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts."
  },
  {
    "id": 19,
    "question": "A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\r\n\r\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?",
    "options": [
      "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
      "Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.",
      "Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.",
      "Enable auto-scaling on the Fargate tasks."
    ],
    "correct_answer": "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
    "explanation": "CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).\r\n\r\n&nbsp;\r\n\r\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response)."
  },
  {
    "id": 20,
    "question": "A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application\u2019s performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading.\r\n\r\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?",
    "options": [
      "Refactor the application to use DynamoDB transactional read and write APIs .",
      "Enable DynamoDB Streams.",
      "Upgrade the EC2 instances to a higher instance type.",
      "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations."
    ],
    "correct_answer": "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.",
    "explanation": "For applications that need to read or write multiple items, DynamoDB provides the `BatchGetItem` and `BatchWriteItem` operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\r\n\r\nThe batch operations are essentially wrappers around multiple read or write requests. For example, if a `BatchGetItem` request contains five items, DynamoDB performs five `GetItem` operations on your behalf. Similarly, if a `BatchWriteItem` request contains two put requests and four delete requests, DynamoDB performs two `PutItem` and four `DeleteItem` requests.\r\n\r\nIn general, a batch operation does not fail unless *all* of the requests in the batch fail. For example, suppose you perform a `BatchGetItem`operation but one of the individual `GetItem` requests in the batch fails. In this case, `BatchGetItem` returns the keys and data from the `GetItem`request that failed. The other `GetItem` requests in the batch are not affected.\r\n\r\nHence, the correct answer is to **use DynamoDB Batch Operations API for GET, PUT, and DELETE operations** in this scenario.\r\n\r\n**Upgrading the EC2 instances to a higher instance type** is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation.\r\n\r\n**Enabling DynamoDB Streams** is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch.\r\n\r\n**Refactoring the application to use DynamoDB transactional read and write APIs** is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario."
  },
  {
    "id": 21,
    "question": "You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\r\n\r\nWhich of the following statements are TRUE regarding this scenario?",
    "options": [
      "There will be at most 100 Lambda function invocations running concurrently.",
      "The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.",
      "The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.",
      "The Lambda function has 500 concurrent executions."
    ],
    "correct_answer": "There will be at most 100 Lambda function invocations running concurrently.",
    "explanation": "You can use an **AWS Lambda function** to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.\r\n\r\n***Concurrent executions*** refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source.\r\n\r\nFor Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard\u2019s events in sequence.\r\n\r\nHence, the correct answer in this scenario is that: **there will be at most 100 Lambda function invocations running concurrently.**\r\n\r\nThe option that says: **the Lambda function has 500 concurrent executions** is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function.\r\n\r\nThe option that says: **the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards** is incorrect because, by default, AWS Lambda will automatically scale the function\u2019s concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream.\r\n\r\nThe option that says: **the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function** is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function."
  },
  {
    "id": 22,
    "question": "A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity.\r\n\r\nWhich of the following is the MOST appropriate service that the developer should use in this scenario?",
    "options": [
      "Serverless Application Framework",
      "AWS CloudFormation",
      "AWS SAM",
      "AWS Systems Manager"
    ],
    "correct_answer": "AWS SAM",
    "explanation": "The AWS Serverless Application Model (AWS SAM) is an open source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML.\r\n\r\nAWS SAM is natively supported by AWS CloudFormation and provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.\r\n\r\nHence, the correct answer is **AWS SAM**.\r\n\r\n**AWS CloudFormation** is incorrect. Although this service can deploy the serverless application to AWS, it is still more appropriate to use AWS SAM instead. AWS SAM can simplify the deployment of the serverless application by deploying all related resources together as a single, versioned entity.\r\n\r\n**AWS Systems Manager** is incorrect because it is more focused on management and operations of AWS resources, such as automation, patching, and configuration, but it is not a deployment or application modeling tool.\r\n\r\n**Serverless Application Framework** is incorrect. Although it is a well-known framework for building and deploying serverless applications into the AWS cloud, this is not an AWS native solution. It also does not allow configuration of DynamoDB databases or API Gateway APIs, unlike AWS SAM."
  },
  {
    "id": 23,
    "question": "A developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with millisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.\r\n\r\nWhich of the following is the MOST suitable solution to reduce the cost and capacity of the stream?",
    "options": [
      "Split cold shards",
      "Split hot shards",
      "Merge cold shards",
      "Merge hot shards"
    ],
    "correct_answer": "Merge cold shards",
    "explanation": "The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\r\n\r\nOne approach to resharding could be to split every shard in the stream\u2014which would double the stream\u2019s capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/93bb0d37c31e4f369a3ec81e0279248f.png)\r\n\r\nYou can also use metrics to determine which are your \u201chot\u201d or \u201ccold\u201d shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could ***merge cold shards*** to make better use of their unused capacity.\r\n\r\nYou can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream.\r\n\r\nHence, the correct answer is to **merge cold shards** to reduce the capacity and the cost of running your Kinesis Data Stream.\r\n\r\n**Splitting cold shards** is incorrect because a cold shard is the one that receives fewer data which means that you have to merge them to reduce the capacity rather than split them.\r\n\r\n**Merging hot shards** is incorrect. Although merging shards is correct, the type of shard to be merged is wrong. A hot shard is the one that receives more data in the stream. Merging hot shards could potentially overload the newly merged shard with a high volume of data, causing a bottleneck in processing and degrading the overall performance of the stream.\r\n\r\n**Splitting hot shards** is incorrect because this will actually further increase both the cost and capacity of the stream rather than reduce it. Moreover, there are no hot shards in the stream since the scenario specifically mentioned that the shards are way underutilized.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html"
  },
  {
    "id": 24,
    "question": "You have two users concurrently accessing a DynamoDB table and submitting updates. If a user will modify a specific item in the table, she needs to make sure that the operation will not affect another user\u2019s attempt to modify the same item. You have to ensure that your update operations will only succeed if the item attributes meet one or more expected conditions.\r\n\r\nWhich of the following DynamoDB features should you use in this scenario?",
    "options": [
      "Conditional writes",
      "Batch Operations",
      "Update Expressions",
      "Projection Expressions"
    ],
    "correct_answer": "Conditional writes",
    "explanation": "By default, the **DynamoDB** write operations (`PutItem`, `UpdateItem`, `DeleteItem`) are *unconditional*: each of these operations will overwrite an existing item that has the specified primary key.\r\n\r\nDynamoDB optionally supports **conditional writes** for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in cases where multiple users attempt to modify the same item.\r\n\r\nFor example, by adding a conditional expression that checks if the current value of the item is still the same, you can be sure that your update will not affect the operations of other users:\r\n\r\n```\r\naws dynamodb update-item \\\r\n --table-name ProductCatalog \\\r\n --key '{\"Id\":{\"N\":\"1\"}}' \\\r\n --update-expression \"SET Price = :newval\" \\\r\n --condition-expression \"Price = :currval\" \\\r\n --expression-attribute-values [file://expression-attribute-values.json](file://expression-attribute-values.json/)\r\n\r\n\r\n\r\n<img loading=\"lazy\" decoding=\"async\" src=\"file:///home/skworkstation/.config/joplin-desktop/resources/45f64df20f1b4df2a475bffa6dce5583.png?t=1748305400704\" width=\"674\" height=\"663\" style=\"box-sizing: border-box; border: 0px; font-style: italic; height: auto; max-width: 100%; vertical-align: sub !important; display: block; margin-left: auto; margin-right: auto;\">\r\n```\r\n\r\nHence, the correct answer is **conditional writes**.\r\n\r\n**Using projection expressions** is incorrect because this is just a string that identifies the attributes you want to retrieve during a `GetItem`, `Query`, or `Scan` operation. Take note that the scenario calls for a feature that can be used during a write operation hence, this option is irrelevant.\r\n\r\n**Using update expressions** is incorrect because this simply specifies how `UpdateItem` will modify the attributes of an item such as for setting a scalar value or removing elements from a list or a map. This feature doesn\u2019t use any conditions which is what the scenario is looking for. Therefore, this option is incorrect.\r\n\r\n**Using batch operations** is incorrect because these are essentially wrappers for multiple read or write requests. Batch operations are primarily used when you want to retrieve or submit multiple items in DynamoDB through a single API call, which reduces the number of network round trips from your application to DynamoDB.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ReadingData\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.html\r\n\r\n**Check out this Amazon DynamoDB Cheat Sheet:**\r\n\r\nhttps://tutorialsdojo.com/amazon-dynamodb/"
  },
  {
    "id": 25,
    "question": "A company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the different processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each process to ensure application performance.\r\n\r\nWhich of the following is the MOST suitable solution that the developer should implement?",
    "options": [
      "Use Enhanced Monitoring in RDS.",
      "Develop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance.",
      "Use CloudWatch to track the CPU Utilization of your database.",
      "Track the CPU% and MEM% metrics which are readily available in the Amazon RDS console."
    ],
    "correct_answer": "Use Enhanced Monitoring in RDS.",
    "explanation": "**Amazon RDS** provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the `RDSOSMetrics` log group in the CloudWatch console.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/dd2718487c7c4d85ad6a05d9da823421.png)\r\n\r\nTake note that there are certain differences between CloudWatch and Enhanced Monitoring Metrics. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work.\r\n\r\nThe differences can be greater if your DB instances use smaller instance classes because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.\r\n\r\nHence, the correct answer is to **use Enhanced Monitoring in RDS**.\r\n\r\n**Developing a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance** is incorrect. Although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database process. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.\r\n\r\n**Using CloudWatch to track the CPU Utilization of your database** is incorrect. Although you can use Amazon CloudWatch to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.\r\n\r\n**Tracking the `CPU%` and `MEM%` metrics which are readily available in the Amazon RDS console** is incorrect because these metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch\r\n\r\n**Check out these Amazon CloudWatch and RDS Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/amazon-cloudwatch/\r\n\r\n**https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/**"
  },
  {
    "id": 26,
    "question": "A company has a suite of web applications that is heavily using RDS database in Multi-AZ Deployments configuration with several Read Replicas. For improved security, you were instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis. You should also configure your applications to use the latest version of the encrypted credentials when connecting to the RDS database.\r\n\r\nWhich of the following is the MOST appropriate solution to secure the credentials?\r\n\r\n",
    "options": [
      "Store the credentials in AWS KMS.",
      "Store the credentials to AWS ACM.",
      "Store the credentials to Systems Manager Parameter Store with a SecureString data type.",
      "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation."
    ],
    "correct_answer": "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation.",
    "explanation": "**AWS Secrets Manager** is an AWS service that makes it easier for you to manage secrets. *Secrets* can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs.\r\n\r\nIn the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/fe3ac9000d2641d98d3d532b640e1ff4.png)\r\n\r\n**Secrets Manager** enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can\u2019t be compromised by someone examining your code, because the secret simply isn\u2019t there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.\r\n\r\nHence, **using AWS Secrets Manager to store and encrypt the credentials and enabling automatic rotation** is the most appropriate solution for this scenario.\r\n\r\n**Storing the credentials to Systems Manager Parameter Store with a `SecureString` data type** is incorrect because, by default, Systems Manager Parameter Store doesn\u2019t rotate its parameters which is one of the requirements in the above scenario.\r\n\r\n**Storing the credentials to AWS ACM** is incorrect because it is just a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates to allow SSL communication to your application. This is not a suitable service to store database or any other confidential credentials.\r\n\r\n**Storing the credentials in AWS KMS** is incorrect because this only makes it easy for you to create and manage encryption keys and control the use of encryption across a wide range of AWS services. This is primarily used for encryption and not for hosting your credentials.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\r\n\r\nhttps://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/\r\n\r\n**Check out these AWS Systems Manager and Secrets Manager Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/aws-systems-manager/\r\n\r\nhttps://tutorialsdojo.com/aws-secrets-manager/\r\n\r\n&nbsp;"
  },
  {
    "id": 27,
    "question": "A developer is utilizing AWS X-Ray to generate a visual representation of the requests flowing through their enterprise web application. Since the application interacts with multiple services, all requests must be traced in X-Ray, including any downstream calls made to AWS resources.\r\n\r\nWhich of the following actions should the developer implement for this scenario?",
    "options": [
      "Install AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls.",
      "Pass multiple trace segments as a parameter of PutTraceSegments API.",
      "Use AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API.",
      "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches."
    ],
    "correct_answer": "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches.",
    "explanation": "You can send trace data to X-Ray in the form of segment documents. A **segment document** is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments or work that uses downstream services and resources in subsegments.\r\n\r\nA segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the `PutTraceSegments` API. An alternative is, instead of sending segment documents to the X-Ray API, you can send segments and subsegments to an X-Ray daemon, which will buffer them and upload to the X-Ray API in batches. The X-Ray SDK sends segment documents to the daemon to avoid making calls to AWS directly. This is the correct option among the choices.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/322d4919e11e4f1cb6282f3d963f180e.png)\r\n\r\nHence, **using X-Ray SDK to generate segment documents with subsegments and sending them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches** is the correct answer in this scenario.\r\n\r\n**Using AWS X-Ray SDK to upload a trace segment by executing `PutTraceSegments` API** is incorrect because you should upload the segment documents with subsegments instead. A trace segment is just a JSON representation of a request that your application serves.\r\n\r\n**Installing AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls** is incorrect because you cannot run a trace on the application and the services at the same time as this will produce two different results. You simply have to send the segment documents with subsegments to get the information about downstream calls that your application makes to AWS resources.\r\n\r\n**Passing multiple trace segments as a parameter of `PutTraceSegments` API** is incorrect because, contrary to the API\u2019s name, you have to upload **segment** documents and not trace segments. The API has a single parameter: `TraceSegmentDocuments`, that takes a list of JSON segment documents.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\r\n\r\n"
  },
  {
    "id": 28,
    "question": "A developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities of data, the write capacity units must be specified for the expected workload on both the base table and its secondary index.\r\n\r\nWhich of the following should the developer do to avoid any potential request throttling?",
    "options": [
      "Ensure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.",
      "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",
      "Ensure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.",
      "Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table."
    ],
    "correct_answer": "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",
    "explanation": "A **global secondary index (GSI)** is an index with a partition key and a sort key that can be different from those on the base table. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nEvery global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.\r\n\r\nWhen you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A `Query` operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/b6e6a4f0ec9a4a738194046eca1d9cc5.gif)\r\n\r\nFor example, if you `Query` a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled.\r\n\r\nTo avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.\r\n\r\nHence, the correct answer in this scenario is to **ensure that the global secondary index\u2019s provisioned WCU is equal to or greater than the WCU of the base table**.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned WCU is equal or less than the WCU of the base table** is incorrect because it should be the other way around, just as what is mentioned above. The provisioned write capacity for a global secondary index should be equal to or greater than the write capacity of the base table.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal to or greater than the RCU of the base table** is incorrect because you have to set the WCU and not the RCU.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal or less than the RCU of the base table** is incorrect because this should be WCU and in addition, the global secondary index\u2019s provisioned WCU should be set to a value that is equal or greater than the WCU of the base table to prevent request throttling.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes\r\n"
  },
  {
    "id": 29,
    "question": "You currently have an IAM user for working in the development environment using shell scripts that call the AWS CLI. The EC2 instance that you are using already contains the access key credential set and an IAM role, which are used to run the CLI and access the development environment. You were given a new set of access key credentials with another IAM role that allows you to access and manage the production environment.\r\n\r\nWhich of the following is the EASIEST way to switch from one role to another?",
    "options": [
      "Store the production access key credentials set in the instance metadata and call this whenever you need to access the production environment.",
      "Store the production access key credentials set in the user data of the instance and call this whenever you need to access the production environment.",
      "Create a new instance profile in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",
      "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command."
    ],
    "correct_answer": "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",
    "explanation": "Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications.\r\n\r\nThis extra step is the creation of an *[instance profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)* that is attached to the instance. The instance profile contains the role and can provide the role\u2019s temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application\u2019s API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/52366299d8854f979e988e23c01c2a3a.png)\r\n\r\nUsing roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don\u2019t have to manage credentials, and you don\u2019t have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances.\r\n\r\nImagine that you have an IAM user for working in the development environment and you occasionally need to work with the production environment at the command line with the [AWS CLI](http://aws.amazon.com/cli/). You already have an access key credential set available to you. This can be the access key pair that is assigned to your standard IAM user. Or, if you signed in as a federated user, it can be the access key pair for the role that was initially assigned to you. If your current permissions grant you the ability to assume a specific IAM role, then you can identify that role in a \u201cprofile\u201d in the AWS CLI configuration files. That command is then run with the permissions of the specified IAM role, not the original identity.\r\n\r\nNote that when you specify that profile in an AWS CLI command, you are using the new role. In this situation, you cannot make use of your original permissions in the development account at the same time. The reason is that only one set of permissions can be in effect at a time.\r\n\r\nHence, the correct answer is to **create a new profile for the role in the AWS CLI configuration file then append the `--profile` parameter, along with the new profile name, whenever you run the CLI command**.\r\n\r\n**Storing the production access key credentials set in the instance metadata and calling this whenever you need to access the production environment** is incorrect because instance metadata is primarily used to fetch the data about your instance that you can use to configure or manage the running instance. This is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**Creating a new instance profile in the AWS CLI configuration file then appending the `--profile` parameter along with the new profile name whenever you run the CLI command** is incorrect because an instance profile is just a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is different from an AWS CLI **profile**, which you can use for switching to various profiles. In addition, an instance profile is associated with the instance and not configured in the AWS CLI.\r\n\r\n**Storing the production access key credentials set in the user data of the instance and calling this whenever you need to access the production environment** is incorrect because user data is primarily used to configure an instance during launch, or to run a configuration script. Just like instance metadata, this is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\r\n\r\n**Check out this AWS Identity & Access Management (IAM) Cheat Sheet:**\r\n\r\n",
    "question_type": "single",
    "correct_answers": [
      "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command."
    ]
  },
  {
    "id": 30,
    "question": "A company has an application hosted in an On-Demand EC2 instance in your VPC. The developer has been instructed to create a shell script that fetches the instance\u2019s associated public and private IP addresses.\r\n\r\nWhat should the developer do to complete this task?",
    "options": [
      "Get the public and private IP addresses from AWS CloudTrail.",
      "Get the public and private IP addresses from Amazon CloudWatch.",
      "Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint.",
      "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint."
    ],
    "correct_answer": "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint.",
    "explanation": "**Instance metadata** is data about your EC2 instance that you can use to configure or manage the running instance. Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you\u2019re writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/91b0c543781c40faaa38c368e3616c23.png)\r\n\r\nTo view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL: `http://169.254.169.254/latest/meta-data/`.\r\n\r\nHence, the correct answer is: **Get the public and private IP addresses from the instance metadata service using the `http://169.254.169.254/latest/meta-data/` endpoint.**\r\n\r\nThe option that says: **Get the public and private IP addresses from Amazon CloudWatch** is incorrect because there is no direct way to fetch the public and private IP addresses of the EC2 instance using CloudWatch.\r\n\r\nThe option that says: **Get the public and private IP addresses from AWS CloudTrail** is incorrect because CloudTrail is primarily used to track the API activity of each AWS service. Just like CloudWatch, there is no easy way to get the associated IP addresses of the EC2 instance using CloudTrail.\r\n\r\nThe option that says: **Get the public and private IP addresses from the instance user data service using the `http://169.254.169.254/latest/userdata/` endpoint** is incorrect because a user data is mainly used to perform common automated configuration tasks and run scripts after the instance starts. You will not find the associated IP addresses of the EC2 instance from its user data. You have to use the metadata service instead.\r\n\r\n**References:**\r\n\r\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html"
  },
  {
    "id": 31,
    "question": "A leading financial company has recently deployed its application to AWS using Lambda and API Gateway. However, they noticed that all metrics are being populated in their CloudWatch dashboard except for `CacheHitCount` and `CacheMissCount`.\r\n\r\nWhat could be the MOST likely cause of this issue?",
    "options": [
      "API Caching is not enabled in API Gateway.",
      "The provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch.",
      "They have not provided an IAM role to their API Gateway yet.",
      "API Gateway Private Integrations has not been configured yet."
    ],
    "correct_answer": "API Caching is not enabled in API Gateway.",
    "explanation": "You can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics. These statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your web application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/e7a5dad826b342b68617a7de98552fbc.png)\r\n\r\nThe metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list.\r\n\r\n\u2013 Monitor the **IntegrationLatency** metrics to measure the responsiveness of the backend.\r\n\r\n\u2013 Monitor the **Latency** metrics to measure the overall responsiveness of your API calls.\r\n\r\n\u2013 Monitor the **CacheHitCount** and **CacheMissCount** metrics to optimize cache capacities to achieve a desired performance. CacheMissCount tracks the number of requests served from the backend in a given period, <ins>when API caching is enabled</ins>. On the other hand, CacheHitCount track the number of requests served from the API cache in a given period.\r\n\r\nHence, the root cause of this issue is that the **API Caching is not enabled in API Gateway** which is why the ***CacheHitCount*** and ***CacheMissCount*** metrics are not populated.\r\n\r\nThe option that says: **they have not provided an IAM role to their API Gateway yet** is incorrect because, in the first place, the scenario already mentioned that all metrics are being populated in their CloudWatch dashboard except for two metrics. This implies that some of the metrics are populated which means that the API Gateway already has an IAM Role associated with it.\r\n\r\nThe option that says: **the provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch** is incorrect because just as what is mentioned above, there is no issue with the IAM Role since all metrics are being populated except only for `CacheHitCount` and `CacheMissCount`. This means that the associated IAM Role already has *write* privileges to write logs to CloudWatch to begin with. The only reason why those two metrics are not being populated is that the API Caching is not enabled.\r\n\r\nThe option that says: **API Gateway Private Integrations has not been configured yet** is incorrect because this feature only makes it easier to expose your HTTP/HTTPS resources behind an Amazon VPC for access by clients outside of the VPC.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring-cloudwatch.html"
  },
  {
    "id": 32,
    "question": "Your application is hosted on an Auto Scaling group of EC2 instances with a DynamoDB database. There were a lot of data discrepancy issues where the changes made by one user were always overwritten by another user. You noticed that this usually happens whenever there are a lot of people updating the same data.\r\n\r\nWhat should you do to solve this problem?",
    "options": [
      "Implement a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
      "Use DynamoDB global tables and implement a pessimistic locking strategy.",
      "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
      "Use DynamoDB global tables and implement an optimistic locking strategy."
    ],
    "correct_answer": "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
    "explanation": "**Optimistic locking** is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others \u2014 and vice-versa. Take note that:\r\n\r\n\u2013 DynamoDB global tables use a \u201clast writer wins\u201d reconciliation between concurrent updates. If you use Global Tables, last writer policy wins. So in this case, the locking strategy does not work as expected.\r\n\r\n\u2013 DynamoDBMapper transactional operations do not support optimistic locking.\r\n\r\nWith optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes.\r\n\r\nHence, **implementing an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is the correct answer in this scenario.\r\n\r\n**Using DynamoDB global tables and implementing a pessimistic locking strategy** is incorrect because you have to use optimistic locking here just as what was explained above.\r\n\r\n**Implementing a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is incorrect because an optimistic locking strategy is a more suitable solution for this scenario. Although the provided steps here are correct, the name of the strategy is wrong.\r\n\r\n**Using DynamoDB global tables and implementing an optimistic locking strategy** is incorrect. Although it is correct to use the optimistic locking strategy, the use of DynamoDB global tables is wrong. This uses a *\u201clast writer wins\u201d* reconciliation between concurrent updates. If you use Global Tables, the last writer policy is in effect so in this case, the locking strategy will not work as expected.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html"
  },
  {
    "id": 33,
    "question": "A company has recently developed a containerized application that uses a multicontainer Docker platform which supports multiple containers per instance. They need a service that automatically handles tasks such as provisioning of the resources, load balancing, auto-scaling, monitoring, and placing the containers across the cluster.\r\n\r\nWhich of the following services provides the EASIEST way to accomplish the above requirement?",
    "options": [
      "Elastic Beanstalk",
      "EKS",
      "Lambda",
      "ECS"
    ],
    "correct_answer": "Elastic Beanstalk",
    "explanation": "You can create docker environments that support multiple containers per Amazon EC2 instance with multicontainer Docker platform for Elastic Beanstalk.\r\n\r\n**Elastic Beanstalk** uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multicontainer Docker environments. Amazon ECS provides tools to manage a cluster of instances running Docker containers. Elastic Beanstalk takes care of Amazon ECS tasks including cluster creation, task definition and execution.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/ba1c0749eacd4161a6e951f787301a23.png)\r\n\r\n**AWS Elastic Beanstalk** is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links.\r\n\r\nElastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**ECS** is incorrect. Although it can host Docker applications, it doesn\u2019t automatically handle all the details such as resource provisioning, balancing load, auto-scaling, monitoring, and placing your containers across your cluster, unlike Elastic Beanstalk. Take note that even though you can use Service Auto Scaling in ECS, you still have to enable and configure it. Elastic Beanstalk still provides the easiest way to accomplish the requirements.\r\n\r\n**Lambda** is incorrect because this is primarily used for serverless applications and not for Docker or any other containerized applications.\r\n\r\n**EKS** is incorrect because Amazon EKS just provides you an easy way to run Kubernetes on AWS without needing to install and operate your own Kubernetes clusters.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\r\n\r\nhttps://aws.amazon.com/ecs/faqs/"
  },
  {
    "id": 34,
    "question": "A leading insurance firm is hosting its customer portal in Elastic Beanstalk, which has an RDS database in AWS. The support team in your company discovered a lot of SQL injection attempts and cross-site scripting attacks on the portal, which is starting to affect the production environment.\r\n\r\nWhich of the following services should you implement to mitigate this attack?",
    "options": [
      "AWS WAF",
      "Amazon Guard\u200bDuty",
      "Network Access Control List",
      "AWS Firewall Manager"
    ],
    "correct_answer": "AWS WAF",
    "explanation": "**AWS WAF** is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an Application Load Balancer responds to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/39fd073d4cfb4dd89e38274ccb4e79c6.png)\r\n\r\nAt the simplest level, AWS WAF lets you choose one of the following behaviors:\r\n\r\n**Allow all requests except the ones that you specify** \u2013 This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers.\r\n\r\n**Block all requests except the ones that you specify** \u2013 This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website.\r\n\r\n**Count the requests that match the properties that you specify** \u2013 When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn\u2019t accidentally configure AWS WAF to block all the traffic to your website. When you\u2019re confident that you specified the correct properties, you can change the behavior to allow or block requests.\r\n\r\nHence, the correct answer in this scenario is **AWS WAF.**\r\n\r\n**Amazon Guard\u200bDuty** is incorrect because this is just a threat detection service that continuously monitors malicious activity and unauthorized behavior to protect your AWS accounts and workloads.\r\n\r\n**AWS Firewall Manager** is incorrect because this just simplifies your AWS WAF and AWS Shield Advanced administration and maintenance tasks across multiple accounts and resources.\r\n\r\n**Network Access Control List** is incorrect because this is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/waf/\r\n\r\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\r\n\r\nhttps://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/"
  },
  {
    "id": 35,
    "question": "A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games.\r\n\r\nYou have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases:\r\n\r\nFor a given name and version number, get all details about the game that has that name and version number.\r\n\r\nFor a given name, get all details about all games that have that name.\r\n\r\nFor a given category, get all details about all games in that category.\r\n\r\nWhat will you recommend as the most efficient solution?",
    "options": [
      "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
      "Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key",
      "Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance",
      "Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key"
    ],
    "correct_answer": "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
    "explanation": "**Correct option**:\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key**\r\n\r\nWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.\r\n\r\n**Incorrect options:**\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key** - The DynamoDB table for this option has the primary key and GSI that do not solve for the condition - \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multi**Correct option**:\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key**\r\n\r\nWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.ple queries which would be less efficient than the single query allowed by the correct option.\r\n\r\n**Set up an Amazon RDS MySQL instance having a `games` table that contains columns for name, version number, and category. Configure the name column as the primary key** - This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.\r\n\r\n**Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance** - You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/primary-key-dynamodb-table/"
  },
  {
    "id": 36,
    "question": "You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.\r\n\r\nWhen creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?",
    "options": [
      ".config_<mysettings>.ebextensions",
      ".ebextensions_<mysettings>.config",
      ".ebextensions/<mysettings>.config",
      ".config/<mysettings>.ebextensions"
    ],
    "correct_answer": ".ebextensions/<mysettings>.config",
    "explanation": "**Correct option:**\r\n\r\n**`.ebextensions/<mysettings>.config`** : You can add AWS Elastic Beanstalk configuration files (`.ebextensions`) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.\r\n\r\n**Incorrect options:**\r\n\r\n**`.ebextensions_<mysettings>.config`**\r\n\r\n**`.config/<mysettings>.ebextensions`**\r\n\r\n**`.config_<mysettings>.ebextensions`**\r\n\r\nThese three options contradict the explanation provided earlier. So these are incorrect.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\r\n\r\n&nbsp;"
  },
  {
    "id": 37,
    "question": "You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).\r\n\r\nWhich AWS service will help you with token handling and management?",
    "options": [
      "Cognito Identity Pools",
      "Cognito Sync",
      "API Gateway",
      "Cognito User Pools"
    ],
    "correct_answer": "Cognito User Pools",
    "explanation": "**Correct option:**\r\n\r\n\"Cognito User Pools\"\r\n\r\nAfter successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.\r\n\r\nAmazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.\r\n\r\nThe ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.\r\n\r\n**Incorrect options:**\r\n\r\n\"API Gateway\" - If you are processing tokens server-side and using other programming languages not supported in AWS it may be a good choice. Other than that, go with a service already providing the functionality.\r\n\r\n\"Cognito Identity Pools\" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.\r\n\r\n\"Cognito Sync\" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend."
  },
  {
    "id": 38,
    "question": "A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.\r\n\r\nAs a Developer Associate, which AWS service would you recommend for the given use-case?",
    "options": [
      "CloudFormation",
      "CodeDeploy",
      "Serverless Application Model (SAM)",
      "Elastic Beanstalk"
    ],
    "correct_answer": "Elastic Beanstalk",
    "explanation": "**Correct option:**\r\n\r\n**Elastic Beanstalk**\r\n\r\nAWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.\r\n\r\nAWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.\r\n\r\n&nbsp;\r\n\r\n**Incorrect options:**\r\n\r\n**CloudFormation** - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.\r\n\r\n**CodeDeploy** - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.\r\n\r\n**Serverless Application Model** - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\r\n\r\nhttps://aws.amazon.com/cloudformation/"
  },
  {
    "id": 39,
    "question": "You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week.\r\n\r\nWhat should you use?",
    "options": [
      "Use DynamoDB Streams",
      "Use TTL",
      "Use DynamoDB Streams",
      "Use DAX"
    ],
    "correct_answer": "Use TTL",
    "explanation": "**Correct option:**\r\n\r\n**Use TTL**\r\n\r\nTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.\r\n\r\n**Incorrect options:**\r\n\r\n**Use DynamoDB Streams** - These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.\r\n\r\n**Use DAX** - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.\r\n\r\n**Use a Lambda function** - This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This band-aid solution would never be as good as using the TTL feature in DynamoDB.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
  },
  {
    "id": 40,
    "question": "A retail company is migrating its on-premises database to Amazon RDS for PostgreSQL. The company has read-heavy workloads. The development team at the company is looking at refactoring the code to achieve optimum read performance for SQL queries.\r\n\r\nWhich solution will address this requirement with the least current as well as future development effort?",
    "options": [
      "Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint",
      "Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint",
      "Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint",
      "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas"
    ],
    "correct_answer": "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas",
    "explanation": "**Correct option:**\r\n\r\n**Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas**\r\n\r\nAmazon RDS uses the PostgreSQL DB engine's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the given use case, you can achieve optimum read performance for SQL queries by using the read-replica endpoint for the read-heavy workload.\r\n\r\n**Incorrect options:**\r\n\r\n**Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint**\r\n\r\n**Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint**\r\n\r\nBoth Redis and Memcached are popular, open-source, in-memory data stores (also known as in-memory caches). These are not relational databases and cannot be used to run SQL queries. So, both these options are incorrect.\r\n\r\n**Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint** - In an Amazon RDS Multi-AZ deployment with a single standby instance, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention. You cannot route the read queries from an application to the standby instance of a multi-AZ RDS database as it's not accessible for the read traffic in the single standby instance configuration.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/\r\n\r\nhttps://aws.amazon.com/rds/features/multi-az/\r\n\r\nhttps://aws.amazon.com/blogs/database/readable-standby-instances-in-amazon-rds-multi-az-deployments-a-new-high-availability-option/"
  },
  {
    "id": 41,
    "question": "You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.\r\n\r\nWhich of the following options represents the best solution to address this use-case?",
    "options": [
      "Use VPC Flow Logs",
      "Use AWS CloudWatch Events",
      "Use AWS Lambda",
      "Enable CodeBuild timeouts"
    ],
    "correct_answer": "Enable CodeBuild timeouts",
    "explanation": "**Correct option:**\r\n\r\n**Enable CodeBuild timeouts**\r\n\r\nA build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).\r\n\r\nThe following rules apply when you run multiple builds:\r\n\r\nWhen possible, builds run concurrently. The maximum number of concurrently running builds can vary.\r\n\r\nBuilds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.\r\n\r\nA build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.\r\n\r\nBy setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.\r\n\r\nIncorrect options:\r\n\r\n**Use AWS Lambda** - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.\r\n\r\n**Use AWS CloudWatch Events** - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.\r\n\r\n**Use VPC Flow Logs** - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html"
  },
  {
    "id": 42,
    "question": "An application running on an EC2 instance regularly fetches large amounts of data from multiple S3 buckets. A data analysis team will perform ad-hoc queries on the data. To reduce costs and optimize the process, the application requires a solution that can perform serverless queries directly on the data stored in S3 without the need to load it into a database first.\r\n\r\nWhich is the MOST suitable service that will help accomplish this requirement?",
    "options": [
      "Amazon Athena",
      "AWS Step Functions",
      "Amazon EMR",
      "Amazon Redshift Spectrum"
    ],
    "correct_answer": "Amazon Athena",
    "explanation": "**Correct**\r\n\r\n**Amazon Athena** is a serverless query service which enables fast analysis of data stored in Amazon S3 using standard SQL. With minimal configuration through the AWS Management Console, Athena allows you to run ad-hoc SQL queries on S3 data and obtain results in seconds. It supports various file formats, including JSON, CSV, ORC, and Parquet. Athena is ideal for ad-hoc queries and operates on a pay-per-query pricing model, making it highly cost-effective for analyzing large datasets without the need to manage any infrastructure.\r\n\r\n**Athena** directly addresses the need for querying data in S3 without moving it to a database, which significantly lowers costs and optimizes data analysis processes.\r\n\r\nHence, the correct answer is: **Amazon Athena.**\r\n\r\nThe option that says: **Amazon EMR** is incorrect because this service is a managed Hadoop framework that helps process large datasets using tools like Apache Spark and Hive. While it can analyze data in S3, it requires setting up clusters, adding infrastructure management, and is not as serverless or cost-efficient as Athena for ad-hoc SQL queries.\r\n\r\nThe option that says: **Amazon Redshift Spectrum** is incorrect. Although it allows querying S3 data, it requires a Redshift cluster and involves additional setup and cost, making it less ideal for serverless ad-hoc querying.\r\n\r\nThe option that says: **AWS Step Functions** is incorrect because this service only lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. It doesn\u2019t provide a function to do an in-place query to an S3 bucket.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html\r\n\r\nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html"
  },
  {
    "id": 43,
    "question": "A developer is building an AI-based traffic monitoring application using Lambda in AWS. Due to the complexity of the application, the developer must do certain modifications such as the way Lambda runs the function\u2019s setup code and how the invocation events are read from the Lambda runtime API.\r\n\r\nIn this scenario, which feature of Lambda should you take advantage of to meet the above requirement?",
    "options": [
      "Layers",
      "Custom Runtime",
      "DLQ",
      "Lambda@Edge"
    ],
    "correct_answer": "Custom Runtime",
    "explanation": "You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function\u2019s handler method when the function is invoked. You can include a runtime in your function\u2019s deployment package in the form of an executable file named `bootstrap`.\r\n\r\nA runtime is responsible for running the function\u2019s setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler and posts the response from the handler back to Lambda.\r\n\r\nYour custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that\u2019s included in Amazon Linux, or a binary executable file that\u2019s compiled in Amazon Linux.\r\n\r\nTake note that Lambda has a deployment package size limit of 50 MB for direct upload (zipped file) and 250 MB for layers (unzipped).\r\n\r\nHence, the correct answer in this scenario is: **Custom Runtime.**\r\n\r\n**Layers** is incorrect because this just enables you to use libraries and other dependencies in your function without having to include them in your deployment package.\r\n\r\n**Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/)** is incorrect because this is actually a feature of Amazon CloudFront and not Lambda, which lets you run code closer to users of your application to improve performance and reduce latency.\r\n\r\n**DLQ** is incorrect because the Dead Letter Queue (DLQ) only directs unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure if the retries fail. This feature does not meet the required capabilities mentioned in the scenario.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html"
  },
  {
    "id": 44,
    "question": "You are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should also allow multiple users to synchronize and collaborate shared data in real time.\r\n\r\nWhich of the following is the MOST appropriate solution that you should implement in this scenario?",
    "options": [
      "Integrate Amazon Cognito Sync to your mobile app.",
      "Integrate Amazon Pinpoint to your mobile app.",
      "Integrate AWS AppSync to your mobile app.",
      "Integrate AWS Amplify to your mobile app."
    ],
    "correct_answer": "Integrate AWS AppSync to your mobile app.",
    "explanation": "**AWS AppSync** simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.\r\n\r\nWith AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.\r\n\r\nAWS AppSync is quite similar with Amazon Cognito Sync which is also a service for synchronizing application data across devices. It enables user data like app preferences or game state to be synchronized as well however, the key difference is that, it also extends these capabilities by allowing multiple users to synchronize and collaborate in real time on shared data.\r\n\r\nHence, the correct answer is to **integrate AWS AppSync to your mobile app**.\r\n\r\n**Integrating AWS Amplify to your mobile app** is incorrect because this service just makes it easy to create, configure, and implement scalable mobile and web apps powered by AWS. This is primarily used to automate the application release process of both your frontend and backend allowing you to deliver features faster, and not for synchronizing application data across devices.\r\n\r\n**Integrating Amazon Cognito Sync to your mobile app** is incorrect. Although this service can also be used in synchronizing application data across devices, it does not allow multiple users to synchronize and collaborate in real-time on shared data, unlike AWS AppSync.\r\n\r\n**Integrating Amazon Pinpoint to your mobile app** is incorrect because this service simply allows you to engage with your customers across multiple messaging channels. This is primarily used to send push notifications, emails, SMS text messages, and voice messages."
  },
  {
    "id": 45,
    "question": "A media company seeks to protect its copyrighted images from unauthorized distribution. They want images uploaded to their Amazon S3 bucket to be automatically watermarked. A developer has already prepared the Lambda function for this image-processing job.\r\n\r\nWhich option must the developer configure to automatically invoke the function at each upload?",
    "options": [
      "Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation.",
      "Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket.",
      "Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users.",
      "Configure an S3 Lifecycle policy to transition images to the INTELLIGENT_TIERING storage class. Use S3 Inventory to generate a report of images that weren\u2019t watermarked and set up the Lambda function to process the report."
    ],
    "correct_answer": "Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket.",
    "explanation": "You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, add a notification configuration that identifies the events that you want Amazon S3 to publish. Make sure that it also identifies the destinations where you want Amazon S3 to send the notifications.\r\n\r\nAmazon S3 can send event notification messages to the following destinations:\r\n\r\n\u2013 Amazon SQS queue\r\n\r\n\u2013 AWS Lambda function\r\n\r\n\u2013 Amazon SNS topic\r\n\r\n\u2013 Amazon EventBridge\r\n\r\nIn the given scenario, you can set up a notification for the `ObjectCreated:Put` event to immediately trigger a Lambda function when an object is uploaded to the S3 bucket.\r\n\r\nHence, the correct answer is: **Set up an Amazon S3 Event Notification to trigger the Lambda function when an `ObjectCreated:Put` event is detected in the bucket.**\r\n\r\nThe option that says: **Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation** is incorrect. S3 Storage Lens just provide visibility into storage usage and activity trends. It does not trigger actions or Lambda functions based on object operations.\r\n\r\nThe option that says: **Configure an S3 Lifecycle policy to transition images to the `INTELLIGENT_TIERING` storage class. Use S3 Inventory to generate a report of images that weren\u2019t watermarked and set up the Lambda function to process the report** is incorrect. S3 Lifecycle policies simply manage storage transitions and object expirations, not event-driven actions like invoking Lambda functions upon uploads. Moreover, S3 Inventory just provides object lists and their metadata, but it doesn\u2019t automatically invoke Lambda functions upon image uploads.\r\n\r\nThe option that says: **Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users** is incorrect because S3 Object Lambda is primarily designed to transform objects at retrieval, not at upload. While it can dynamically apply watermarks, it does so when the object is accessed, not as part of the upload process, which would lead to watermarking every time the image is retrieved rather than just once upon upload.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-event-notifications.html"
  },
  {
    "id": 46,
    "question": "A developer has recently deployed an application, which is hosted in an Auto Scaling group of EC2 instances and processes data from an Amazon Kinesis Data Stream. Each of the EC2 instances has exactly one KCL worker processing one Kinesis data stream which has 10 shards. Due to performance issues, the systems operations team has resharded the data stream to increase the number of open shards to 20.\r\n\r\nWhat is the maximum number of running EC2 instances that should ideally be kept to maintain application performance?",
    "options": [
      "10",
      "30",
      "40",
      "20"
    ],
    "correct_answer": "20",
    "explanation": "Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nTo scale up processing in your application, you should test a combination of these approaches:\r\n\r\n\u2013 Increasing the instance size (because all record processors run in parallel within a process)\r\n\r\n\u2013 Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently)\r\n\r\n\u2013 Increasing the number of shards (which increases the level of parallelism)\r\n\r\nThus, the maximum number of instances you can launch is **20**, to match the number of open shards with a ratio of 1:1.\r\n\r\nAlthough you can launch **10** instances in which each instance handles 2 shards, this is not the maximum number of instances you can deploy for your application. Hence, this option is incorrect. Take note that the maximum number of your instances is not half the number of open shards.\r\n\r\nLaunching **30** or **40** instances is incorrect because you should ensure that the number of instances does not exceed the number of open shards. The maximum number of instances that you should deploy is 20.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html"
  },
  {
    "id": 47,
    "question": "A developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is the MOST secure way to achieve this?",
    "options": [
      "Move your RDS instance to a public subnet.",
      "Ensure that the Lambda function has proper IAM permission to access RDS.",
      "Configure the Lambda function to connect to your VPC.",
      "Expose an endpoint of your RDS to the Internet using an Elastic IP."
    ],
    "correct_answer": "Configure the Lambda function to connect to your VPC.",
    "explanation": "You can configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources during execution.\r\n\r\nAWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces [(ENIs)](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_ElasticNetworkInterfaces.html) that enable your function to connect securely to other resources within your private VPC.\r\n\r\nDon\u2019t put your Lambda function in a VPC unless you have to. There is no benefit outside of using this to access resources you cannot expose publicly, like a private Amazon Relational Database instance. Services like Amazon OpenSearch Service can be secured over IAM with access policies, so exposing the endpoint publicly is safe and wouldn\u2019t require you to run your function in the VPC to secure it.\r\n\r\nHence, **configuring the Lambda function to connect to your VPC** is the correct answer for this scenario.\r\n\r\n**Ensuring that the Lambda function has proper IAM permission to access RDS** is incorrect. Even though you grant the necessary IAM permissions to the Lambda function to access RDS, the function would still not be able to connect to RDS since there is no established connection between Lambda and the private subnet of your VPC.\r\n\r\n**Exposing an endpoint of your RDS to the Internet using an Elastic IP** is incorrect because this is not the most secure way of granting access to your Lambda function. It will be able to connect to RDS but so will the billions of people on the public Internet.\r\n\r\n**Moving your RDS instance to a public subnet** is incorrect because this is an unnecessary change and not a best practice from a security perspective. You only need to configure your Lambda function to your VPC so it can connect to the RDS in the private subnet. If you move your RDS instance to a public subnet, it will introduce a critical security flaw to your entire architecture since your database will become accessible publicly."
  },
  {
    "id": 49,
    "question": "A company has assigned a developer to automate its department\u2019s patch management, data synchronization, and other recurring tasks. The developer needs a service to coordinate multiple AWS services into serverless workflows.\r\n\r\nWhich of the following is the MOST cost-effective service the developer should implement in this scenario?",
    "options": [
      "AWS Batch",
      "AWS Elastic Beanstalk",
      "AWS Step Functions",
      "AWS Lambda"
    ],
    "correct_answer": "AWS Step Functions",
    "explanation": "**AWS Step Functions** provides serverless orchestration for modern applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintain the application state, tracking exactly which workflow step your application is in, and store an event log of data that is passed between application components. That means that if networks fail or components hang, your application can pick up right where it left off.\r\n\r\nApplication development is faster and more intuitive with Step Functions because you can define and manage the workflow of your application independently from its business logic. Making changes to one does not affect the other. You can easily update and modify workflows in one place, without having to struggle with managing, monitoring, and maintaining multiple point-to-point integrations. Step Functions frees your functions and containers from excess code, so your applications are faster to write, more resilient, and easier to maintain.\r\n\r\nHence, the correct answer is: **AWS Step Functions.**\r\n\r\n**AWS Elastic Beanstalk** is incorrect because this service is for deploying and scaling web applications and services. However, it\u2019s not designed to coordinate multiple AWS services into serverless workflows.\r\n\r\n**Lambda** is incorrect. Although it is typically used for serverless computing, it does not provide a direct way to coordinate multiple AWS services into serverless workflows.\r\n\r\n**AWS Batch** is incorrect because it is primarily used to efficiently run hundreds of thousands of batch computing jobs in AWS.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/step-functions/features/\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
  },
  {
    "id": 50,
    "question": "A developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a SAM template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS.\r\n\r\nWhich combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)",
    "options": [
      "Deploy the SAM template from AWS CodePipeline.",
      "Build the SAM template in the local environment",
      "Build the SAM template using the AWS SDK for AWS CodeDeploy.",
      "Deploy the SAM template from an Amazon S3 bucket.",
      "Build the SAM template in an Amazon EC2 instance.",
      "Package the SAM application for deployment."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Build the SAM template in the local environment",
      "Deploy the SAM template from an Amazon S3 bucket.",
      "Package the SAM application for deployment."
    ],
    "correct_answer": "Build the SAM template in the local environment",
    "explanation": "**AWS SAM** uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.\r\n\r\nThe typical AWS SAM deployment workflow starts with the `sam build` command, which compiles source code and readies deployment artifacts. Once built for deployment, the SAM template and the associated artifacts need to be stored in an S3 bucket. The `sam deploy` command takes care of this by first uploading the CloudFormation template to the S3 bucket. Though historically, the `sam package` command was used for this purpose, it\u2019s become somewhat legacy, as `sam deploy` , now implicitly handles the packaging. Once the template is in the S3 bucket, AWS CloudFormation references it to create or update the defined resources.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Build the SAM template in the local environment**\r\n\r\n**\u2013 Package the SAM application for deployment.**\r\n\r\n**\u2013 Deploy the SAM template from an Amazon S3 bucket.**\r\n\r\nThe option that says: **Deploy the SAM template from AWS CodePipeline** is incorrect. AWS CodePipeline is primarily a continuous integration and continuous delivery (CI/CD) service that automates the build, test, and deploy phases of your release process. While CodePipeline can deploy SAM applications, it is not a required step for a local SAM deployment workflow.\r\n\r\nThe option that says: **Build the SAM template using the AWS SDK for AWS CodeDeploy** is incorrect. The AWS SDK for CodeDeploy is typically used for management operations of the CodeDeploy service, not for building SAM templates. Building the SAM application is a separate process, typically done using the SAM CLI.\r\n\r\nThe option that says: **Build the SAM template in an Amazon EC2 instance** is incorrect. This option is unnecessary. While you can technically build on an EC2 instance, it\u2019s not a requirement for SAM deployment. In the scenario, there\u2019s no condition that warrants the use of an EC2 instance.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\r\n\r\nhttps://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html"
  },
  {
    "id": 51,
    "question": "The company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda application. It is your responsibility to ensure that application works effectively in AWS.\r\n\r\nWhich of the following are the best practices in developing Lambda functions? (Select TWO.)",
    "options": [
      "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
      "Take advantage of Execution Context reuse to improve the performance of your function.",
      "Use Amazon Inspector for troubleshooting.",
      "Include the core logic in the Lambda handler.",
      "Use recursive code."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
      "Take advantage of Execution Context reuse to improve the performance of your function."
    ],
    "correct_answer": "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
    "explanation": "Below are some of the best practices in working with AWS Lambda Functions:\r\n\r\n\u2013 Separate the Lambda handler (entry point) from your core logic.\r\n\r\n\u2013 Take advantage of Execution Context reuse to improve the performance of your function\r\n\r\n\u2013 Use AWS Lambda Environment Variables to pass operational parameters to your function.\r\n\r\n\u2013 Control the dependencies in your function\u2019s deployment package.\r\n\r\n\u2013 Minimize your deployment package size to its runtime necessities.\r\n\r\n\u2013 Reduce the time it takes Lambda to unpack deployment packages\r\n\r\n\u2013 Minimize the complexity of your dependencies\r\n\r\n\u2013 Avoid using recursive code\r\n\r\nHence, the correct answers in this scenario are:\r\n\r\n**\u2013 Take advantage of Execution Context reuse to improve the performance of your function**\r\n\r\n**\u2013 Use AWS Lambda Environment Variables to pass operational parameters to your function**\r\n\r\n**Using recursive code** is incorrect because this is a situation wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs.\r\n\r\n**Including the core logic in the Lambda handler** is incorrect because you have to separate the Lambda handler (entry point) from your core logic instead.\r\n\r\n**Using Amazon Inspector for troubleshooting** is incorrect because this service is primarily used for EC2 and not for Lambda. You have to use X-Ray instead of troubleshooting your functions.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html"
  },
  {
    "id": 52,
    "question": "A software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller\u2019s identity.\r\n\r\nWhich of the features of API Gateway is the MOST suitable one that she should use to build this feature?",
    "options": [
      "Lambda Authorizers",
      "Cross-Account Lambda Authorizer",
      "Cross-Origin Resource Sharing (CORS)",
      "Resource Policy"
    ],
    "question_type": "single",
    "correct_answers": [
      "Lambda Authorizers"
    ],
    "correct_answer": "Lambda Authorizers",
    "explanation": "A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API\u2019s methods, API Gateway calls your Lambda authorizer, which takes the caller\u2019s identity as input and returns an IAM policy as output.\r\n\r\nThere are two types of Lambda authorizers:\r\n\r\n\u2013 A **token-based** Lambda authorizer (also called a TOKEN authorizer) receives the caller\u2019s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.\r\n\r\n\u2013 A **request parameter-based** Lambda authorizer (also called a REQUEST authorizer) receives the caller\u2019s identity in a combination of headers, query string parameters, stageVariables, and $context variables.\r\n\r\nIt is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function by using a Cross-Account Lambda Authorizer.\r\n\r\nTherefore, the correct answer in this scenario is to use **Lambda Authorizers** since this feature is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML.\r\n\r\n**Resource Policy** is incorrect because this is simply a JSON policy document that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. This can\u2019t be used to implement a custom authorization scheme.\r\n\r\n**Cross-Origin Resource Sharing (CORS)** is incorrect because this just defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.\r\n\r\n**Cross-Account Lambda Authorizer** is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html"
  },
  {
    "id": 53,
    "question": "You are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20 eventually consistent reads per second where all the items have a size of 2 KB for both operations.\r\n\r\nWhich of the following are the most optimal WCU and RCU that you should provision to the table?",
    "options": [
      "40 RCU and 20 WCU",
      "40 RCU and 40 WCU",
      "20 RCU and 20 WCU",
      "10 RCU and 20 WCU"
    ],
    "question_type": "single",
    "correct_answers": [
      "10 RCU and 20 WCU"
    ],
    "correct_answer": "10 RCU and 20 WCU",
    "explanation": "When you create a new provisioned table in DynamoDB, you must specify its *provisioned throughput capacity*\u2014the amount of read and write activity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput requirements.\r\n\r\nYou can optionally allow DynamoDB auto-scaling to manage your table\u2019s throughput capacity. However, you still must provide initial settings for read and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them dynamically in response to your application\u2019s requirements. You specify throughput requirements in terms of capacity units\u2014the amount of data your application needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify them automatically.\r\n\r\n**1 WCU** can do **1 write per second** for an item up to **1KB.** To get the required WCU, simply multiply the given average item size by the required writes per second. In the scenario, the DynamoDB table is expected to perform 10 writes per second of a 2KB item. Multiplying 10 by 2 gives **20 WCU**.\r\n\r\n**1 RCU** can do **1 strongly consistent read** or **2 eventually consistent reads** for an item up to **4KB**.\r\n\r\n**To get the RCU with eventually consistent reads, do the following steps:**\r\n\r\n**Step #1 Divide the average item size by 4 KB. Round up the result**\r\n\r\nAverage Item Size = 2 KB\r\n\r\n= **2KB/4KB**\r\n\r\n**= 0.5 \u2248 1**\r\n\r\n**Step #2 Multiply the number of reads per second by the resulting value from Step 1. Divide the product by 2 for eventually consistent reads.**\r\n\r\n= **20** reads per second **x** **1**\r\n\r\n= **20** RCU\r\n\r\nSince the type of read being asked is eventually consistent, we get half of 20, which is 10.\r\n\r\n= 20/2 = **10 RCU**\r\n\r\nHence, the correct answer is to provision **10 RCU and 20 WCU** to your DynamoDB table.\r\n\r\nThe **20 RCU and 20 WCU** setting is incorrect because this would be the result if you use strong consistency reads. Remember that the scenario explicitly said that eventual consistency reads would be used.\r\n\r\nThe **40 RCU and 20 WCU** is incorrect because 40 RCU is overkill for the required eventual consistency reads. If the scenario was asking for transactional read requests, then this option could have been correct.\r\n\r\nThe **40 RCU and 40 WCU** setting is incorrect because this would be the result if you chose transactional requests both on your reads and writes. Take note that the scenario didn\u2019t say that the database is using DynamoDB Transactions.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"
  },
  {
    "id": 54,
    "question": "An organization is selling memorabilia that is illegal in specific countries. How can a developer restrict access to the website to countries where the memorabilia are illegal?",
    "options": [
      "Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification.",
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification.",
      "Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access.",
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access."
    ],
    "correct_answer": "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access.",
    "explanation": "**Overall explanation:**\r\nAWS WAF can be used to set up a WEB ACL that can be used to block statements that originate from a specific country.\r\n\r\n**CORRECT:** \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access\" is incorrect. AWS Shield is used to protect from DDoS attacks.\r\n\r\n**INCORRECT:** \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.\r\n\r\n**INCORRECT:** \" Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.\r\n\r\nReferences:\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/"
  },
  {
    "id": 55,
    "question": "A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found.\r\n\r\nWhat is the BEST explanation for the duplicate entries?",
    "options": [
      "The Lambda function failed, and the Lambda service retried the invocation with a delay",
      "The S3 bucket name was specified incorrectly",
      "The application stopped intermittently and then resumed",
      "There was an S3 outage, which caused duplicate entries of the same log file"
    ],
    "question_type": "single",
    "correct_answers": [
      "The Lambda function failed, and the Lambda service retried the invocation with a delay"
    ],
    "correct_answer": "The Lambda function failed, and the Lambda service retried the invocation with a delay",
    "explanation": "**Overall explanation\r\nFrom the AWS documentation:**\r\n\r\n\u201cWhen an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.\r\n\r\nFor asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a dead-letter queue.\u201d\r\n\r\nTherefore, the most likely explanation is that the function failed, and Lambda retried the invocation.\r\n\r\n**CORRECT:** \"The Lambda function failed, and the Lambda service retried the invocation with a delay\" is the correct answer.\r\n\r\n**INCORRECT:** \"The S3 bucket name was specified incorrectly\" is incorrect. If this was the case all attempts would fail but this is not the case.\r\n\r\n**INCORRECT:** \"There was an S3 outage, which caused duplicate entries of the same log file\" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have a different request ID.\r\n\r\n**INCORRECT:** \"The application stopped intermittently and then resumed\" is incorrect. The issue is duplicate entries of the same request ID.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html"
  },
  {
    "id": 56,
    "question": "A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Linear",
      "In-place",
      "Blue/green",
      "Canary"
    ],
    "question_type": "single",
    "correct_answers": [
      "Blue/green"
    ],
    "correct_answer": "Blue/green",
    "explanation": "**Overall explanation**\r\n**CodeDeploy** provides two deployment type options \u2013 in-place and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.\r\n\r\nThe Blue/green deployment type on an Amazon ECS compute platform works like this:\r\n\r\nTraffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.\r\n\r\nYou can set the traffic shifting to linear or canary through the deployment configuration.\r\n\r\nThe protocol and port of a specified load balancer listener is used to reroute production traffic.\r\n\r\nDuring a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.\r\n\r\n**CORRECT:** \"Blue/green\" is the correct answer.\r\n\r\n**INCORRECT:** \"Canary\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.\r\n\r\n**INCORRECT:** \"Linear\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.\r\n\r\n**INCORRECT:** \"In-place\" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"
  },
  {
    "id": 57,
    "question": "A developer has deployed an application on AWS Lambda. The application uses Python and must generate and then upload a file to an Amazon S3 bucket. The developer must implement the upload functionality with the least possible change to the application code.\r\n\r\nWhich solution BEST meets these requirements?",
    "options": [
      "Make an HTTP request directly to the S3 API to upload the file.",
      "Include the AWS SDK for Python in the Lambda function code.",
      "Use the AWS CLI that is installed in the Lambda execution environment.",
      "Use the AWS SDK for Python that is installed in the Lambda execution environment."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use the AWS SDK for Python that is installed in the Lambda execution environment."
    ],
    "correct_answer": "Use the AWS SDK for Python that is installed in the Lambda execution environment.",
    "explanation": "**Overall explanation**\r\nThe best practice for Lambda development is to bundle all dependencies used by your Lambda function, including the AWS SDK. However, since this question specifically requests that the least possible changes are made to the application code, the developer can instead use the SDK for Python that is installed in the Lambda environment to upload the file to Amazon S3.\r\n\r\n**CORRECT:** \"Use the AWS SDK for Python that is installed in the Lambda execution environment\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Include the AWS SDK for Python in the Lambda function code\" is incorrect.\r\n\r\nThis is the best practice for deployment. However, in this case the developer must minimize changes to code and including the SDK as a dependency in the code would require potential updates to existing Python code.\r\n\r\n**INCORRECT:** \"Make an HTTP request directly to the S3 API to upload the file\" is incorrect.\r\n\r\nAWS supports uploads to S3 using the console, AWS SDKs, REST API, and the AWS CLI.\r\n\r\n**INCORRECT:** \"Use the AWS CLI that is installed in the Lambda execution environment\" is incorrect.\r\n\r\nThe AWS CLI is not installed in the Lambda execution environment.\r\n\r\nReferences:\r\n\r\nhttps://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/"
  },
  {
    "id": 58,
    "question": "A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit.\r\nWhat needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?\r\n\r\n",
    "options": [
      "A public and private SSH key file",
      "An Amazon EC2 IAM role with CodeCommit permissions",
      "A set of Git credentials generated with IAM",
      "A GitHub secure authentication token"
    ],
    "question_type": "single",
    "correct_answers": [
      "A set of Git credentials generated with IAM"
    ],
    "correct_answer": "A set of Git credentials generated with IAM",
    "explanation": "**Overall explanation**\r\nAWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:\r\n\r\nGit credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.\r\n\r\nSSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.\r\n\r\nAWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.\r\n\r\nIn this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAM\r\n\r\n**CORRECT:** \"A set of Git credentials generated with IAM\" is the correct answer.\r\n\r\n**INCORRECT:** \"A GitHub secure authentication token\" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub (they have already accessed and cloned the repository).\r\n\r\n**INCORRECT:** \"A public and private SSH key file\" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.\r\n\r\n**INCORRECT:** \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html"
  },
  {
    "id": 59,
    "question": "A Developer needs to scan a full DynamoDB 50GB table within non-peak hours. About half of the strongly consistent RCUs are typically used during non-peak hours and the scan duration must be minimized.\r\n\r\nHow can the Developer optimize the scan execution time without impacting production workloads?",
    "options": [
      "Use sequential scans",
      "Use parallel scans while limiting the rate",
      "Change to eventually consistent RCUs during the scan operation",
      "Increase the RCUs during the scan operation"
    ],
    "question_type": "single",
    "correct_answers": [
      "Use parallel scans while limiting the rate"
    ],
    "correct_answer": "Use parallel scans while limiting the rate",
    "explanation": "**Overall explanation**\r\nPerforming a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn\u2019t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.\r\n\r\nFirstly, the Limit parameter can be used to reduce the page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.\r\n\r\nSecondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table\u2019s partitions.\r\n\r\nA parallel scan can be the right choice if the following conditions are met:\r\n\r\nThe table size is 20 GB or larger.\r\n\r\nThe table's provisioned read throughput is not being fully used.\r\n\r\nSequential Scan operations are too slow.\r\n\r\nTherefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.\r\n\r\n**CORRECT:** \"Use parallel scans while limiting the rate\" is the correct answer.\r\n\r\n**INCORRECT:** \"Use sequential scans\" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.\r\n\r\n**INCORRECT:** \"Increase the RCUs during the scan operation\" is incorrect as the table is only using half of the RCUs during non-peak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.\r\n\r\n**INCORRECT:** \"Change to eventually consistent RCUs during the scan operation\" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan"
  },
  {
    "id": 60,
    "question": "A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80.\r\n\r\nWhat is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Specify port 80 for the container port and port 0 for the host port",
      "Leave both the container port and host port configuration blank",
      "Specify port 80 for the container port and a unique port number for the host port",
      "Specify a unique port number for the container port and port 80 for the host port"
    ],
    "question_type": "single",
    "correct_answers": [
      "Specify port 80 for the container port and port 0 for the host port"
    ],
    "correct_answer": "Specify port 80 for the container port and port 0 for the host port",
    "explanation": "**Overall explanation**\r\nPort mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the user-specified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.\r\n\r\n\r\n\r\nAs we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.\r\n\r\nCORRECT: \"Specify port 80 for the container port and port 0 for the host port\" is the correct answer.\r\n\r\nINCORRECT: \"Specify port 80 for the container port and a unique port number for the host port\" is incorrect as this is more difficult to manage as you have to manually assign the port number.\r\n\r\nINCORRECT: \"Specify a unique port number for the container port and port 80 for the host port\" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.\r\n\r\nINCORRECT: \"Leave both the container port and host port configuration blank\" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html"
  },
  {
    "id": 61,
    "question": "A developer is updating an Amazon Aurora MySQL database to allow more clients to connect. What database parameter needs to be updated to support a higher number of client connections?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "max_connections",
      "max_join_size",
      "max_user_connections",
      "max_allowed_packet"
    ],
    "question_type": "single",
    "correct_answers": [
      "max_connections"
    ],
    "correct_answer": "max_connections",
    "explanation": "The maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.\r\n\r\nYou can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\r\n\r\n**CORRECT:** \"max_connections\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"max_allowed_packet\" is incorrect. This parameter sets the maximum size of one packet or any generated or intermediate string.\r\n\r\n**INCORRECT:** \"max_join_size\" is incorrect. This option is used to set a limit on the maximum number of row accesses.\r\n\r\n**INCORRECT:** \"max_user_connections\" is incorrect. This option limits the number of simultaneous connections that the user can make.\r\n\r\n**References**:\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/"
  },
  {
    "id": 62,
    "question": "An application asynchronously invokes an AWS Lambda function. The application has recently been experiencing occasional errors that result in failed invocations. A developer wants to store the messages that resulted in failed invocations such that the application can automatically retry processing them.\r\n\r\nWhat should the developer do to accomplish this goal with the LEAST operational overhead?",
    "options": [
      "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.",
      "Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again.",
      "Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group.",
      "Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function."
    ],
    "correct_answer": "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.",
    "explanation": "Overall explanation\r\n\r\nAmazon SQS supports *dead-letter queues* (DLQ), which other queues (*source queues*) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.\r\n\r\nThe *redrive policy* specifies the *source queue*, the *dead-letter queue*, and the conditions under which Amazon SQS moves messages from the former to the latter if the consumer of the source queue fails to process a message a specified number of times.\r\n\r\nYou can set your DLQ as an event source to the Lambda function to drain your DLQ. This will ensure that all failed invocations are automatically retried.\r\n\r\n**CORRECT:** \"Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group\" is incorrect.\r\n\r\nThe information in the logs may not be sufficient for processing the event. This is not an automated or ideal solution.\r\n\r\n**INCORRECT:** \"Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again\" is incorrect.\r\n\r\nAmazon EventBridge can be configured as a failure destination and can send to SNS. SNS can also be configured with Lambda as a target. However, this solution requires more operational overhead compared to using a DLQ.\r\n\r\n**INCORRECT:** \"Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events\" is incorrect.\r\n\r\nS3 is not a supported failure destination. Supported destinations are Amazon SNS, Amazon SQS, and Amazon EventBridge.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\r\n\r\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html"
  },
  {
    "id": 63,
    "question": "A product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2 Spot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain software license keys in the template each time it is needed.\r\n\r\nWhich solution meets this requirement while offering the most secure and cost-effective approach?",
    "options": [
      "Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the secret in the CloudFormation template.",
      "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.",
      "Pass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho attribute on the parameter.",
      "Embed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key using the Parameter section. Enable the NoEcho attribute on the parameter."
    ],
    "question_type": "single",
    "correct_answers": [
      "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template."
    ],
    "correct_answer": "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.",
    "explanation": "Dynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as the AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations and passes the value to the appropriate resource. CloudFormation does not store the actual reference value.\r\n\r\nThe following snippet shows how you can use the `ssm-secure` dynamic reference to retrieve an IAM user\u2019s password from the Parameter Store for console login. `IAMUserPassword` pertains to the parameter name followed by the version number.\r\n\r\n```\r\nMyIAMUser:\r\n  Type: AWS::IAM::User\r\n  Properties:\r\n    UserName: 'MyUserName'\r\n    LoginProfile:\r\n      Password: '{{resolve:ssm-secure:IAMUserPassword:10}}'\r\n```\r\n\r\nIn the scenario, storing the license key as `SecureString` means encrypting it using a KMS key, making it more secure than storing it in plaintext. It\u2019s also more cost-effective than Secrets Manager since you don\u2019t pay for the number of parameters you store in the Parameter Store (Standard tier).\r\n\r\nHence, the correct answer is: **Store the license key as a `SecureString` in AWS Systems Manager (SSM) Parameter Store. Use the `ssm-secure` dynamic reference to retrieve the secret in the CloudFormation template.**\r\n\r\nThe option that says: **Pass the license key in the `Parameter` section of the CloudFormation template during stack creation. Enable the `NoEcho` attribute on the parameter** is incorrect. Although using the `NoEcho` attribute can help prevent the license key from being displayed in plaintext in the CloudFormation logs and console outputs,\r\n\r\nThe option that says: **Store the license key as a secret in AWS Secrets Manager. Use the `secretsmanager` dynamic reference to retrieve the secret in the CloudFormation template** is incorrect. Although using Secrets Manager is a valid approach, it\u2019s less cost-effective compared to using SSM Parameter Store. With Secrets Manager, there is a monthly cost associated with storing secrets, whereas SSM Parameter Store (Standard tier) is free of charge.\r\n\r\nThe option that says: **Embed the license keys in the `Mapping` section of the CloudFormation template. Let users choose the correct license key using the `Parameter` section. Enable the `NoEcho` attribute on the parameter** is incorrect. Embedding sensitive data, such as license keys, within CloudFormation templates poses a security risk, as the data can be viewed by anyone with access to the template.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#creds\r\n\r\nhttps://catalog.workshops.aws/cfn101/en-US/intermediate/templates/dynamic-references"
  },
  {
    "id": 64,
    "question": "A developer has recently launched a new API Gateway service which is integrated with AWS Lambda. He enabled API caching and per-key cache invalidation features in the API Gateway to comply with the requirement of the front-end development team which will use the API. The front-end team will have to invalidate an existing cache entry in some scenarios and fetch the latest data from the integration endpoint.\r\n\r\nWhich of the following should the consumers of the API do to invalidate the cache in API Gateway?",
    "options": [
      "Configure the front-end application to clear the browser cache before fetching data from API Gateway.",
      "Send a request with the Cache-Control: no-cache header.",
      "Send a request with the Cache-Control: max-age=0 header.",
      "Send a request with the Cache-Control: INVALIDATE_CACHE header."
    ],
    "question_type": "single",
    "correct_answers": [
      "Send a request with the Cache-Control: max-age=0 header."
    ],
    "correct_answer": "Send a request with the Cache-Control: max-age=0 header.",
    "explanation": "A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the `Cache-Control: max-age=0` header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/27241e8b49c94b4caae9d22f681eac77.png)\r\n\r\nTicking the `Require authorization` checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API.\r\n\r\nHence, to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests, you can just **send a request with the `Cache-Control: max-age=0` header**.\r\n\r\n**Sending a request with the `Cache-Control: no-cache` header** is incorrect because you have to use value of the max-age directive in API Gateway instead of the `no-cache` directive. This just forces the cache to submit the request to the origin server for validation before releasing a cached copy.\r\n\r\n**Configuring the frontend application to clear the browser cache before fetching data from API Gateway** is incorrect because the browser cache and the API Gateway cache are not connected with each other. The correct method of invalidating the cache is to add the `Cache-Control: max-age=0` header*.*\r\n\r\n**Sending a request with the `Cache-Control: INVALIDATE_CACHE` header** is incorrect because there is no directive called `INVALIDATE_CACHE` in the `Cache-Control` header.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\r\n\r\nhttps://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching"
  },
  {
    "id": 65,
    "question": "In the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and GraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the underlying components.\r\n\r\nTo achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer (ALB) and must be instrumented to send trace data to the AWS X-Ray.\r\n\r\nWhich of the following options is the MOST suitable way to satisfy this requirement?",
    "options": [
      "Refactor your application to send segment documents directly to X-Ray by using the PutTraceSegments API.",
      "Use a user data script to install the X-Ray daemon.",
      "Enable AWS X-Ray tracing on the ASG\u2019s launch template.",
      "Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use a user data script to install the X-Ray daemon."
    ],
    "correct_answer": "Use a user data script to install the X-Ray daemon.",
    "explanation": "The AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application.\r\n\r\nTo properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will install and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/5f8e08dd724c4b33bd60544d4eadef0f.png)\r\n\r\nThe AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.\r\n\r\nHence, the correct answer is: **Use a user data script to install the X-Ray daemon.**\r\n\r\nThe option that says: **Enable AWS X-Ray tracing on the ASG\u2019s launch template** is incorrect. There\u2019s no option to enable X-Ray tracing in a launch template of an ASG.\r\n\r\nThe option that says: **Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests** is incorrect. Although it can help monitor and protect the application from common web exploits, it\u2019s not capable of instrumenting the application.\r\n\r\nThe option that says: **Refactor your application to send segment documents directly to X-Ray by using the `PutTraceSegments` API** is incorrect. Although this solution will work, it entails a lot of manual effort to perform. You don\u2019t need to do this because you can just install the X-Ray daemon on the instance to automate this process.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html#xray-daemon-permissions"
  },
  {
    "id": 66,
    "question": "A financial mobile application has a serverless backend API which consists of DynamoDB, Lambda, and Cognito. Due to the confidential financial transactions handled by the mobile application, there is a new requirement provided by the company to add a second authentication method that doesn\u2019t rely solely on user name and password.\r\n\r\nWhich of the following is the MOST suitable solution that the developer should implement?",
    "options": [
      "Use a new IAM policy to a user pool in Cognito.",
      "Create a custom application that integrates with Amazon Cognito which implements the second layer of authentication.",
      "Use Cognito with SNS to allow additional authentication via SMS.",
      "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users."
    ],
    "question_type": "single",
    "correct_answers": [
      "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users."
    ],
    "correct_answer": "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.",
    "explanation": "You can add multi-factor authentication (MFA) to a user pool to protect the identity of your users. MFA adds a second authentication method that doesn\u2019t rely solely on usernames and passwords. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users. You can also use adaptive authentication with its risk-based model to predict when you might need another authentication factor. It\u2019s part of the user pool\u2019s advanced security features, which also include protections against compromised credentials.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/fc55c345612440ab8568814afc8aa90b.png)\r\n\r\nMulti-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on user name and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users.\r\n\r\nWith adaptive authentication, you can configure your user pool to require second-factor authentication in response to an increased risk level.\r\n\r\nHence, the correct answer in this scenario is to **integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.**\r\n\r\n**Creating a custom application that integrates with Amazon Cognito which implements the second layer of authentication** is incorrect. Although this option is viable, it is not the most suitable solution in this scenario since you can simply use MFA as a second-factor authentication for the mobile app.\r\n\r\n**Using a new IAM policy to a user pool in Cognito** is incorrect because an IAM Policy alone cannot implement a second-factor authentication. You have to configure Cognito to use MFA instead.\r\n\r\n**Using Cognito with SNS to allow additional authentication via SMS** is incorrect. Although this is part of the MFA setup, using this solution alone is not enough if you didn\u2019t enable MFA in the first place.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/managing-security.html\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"
  }
]