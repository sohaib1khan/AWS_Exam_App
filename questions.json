[
  {
    "id": 1,
    "question": "Which AWS service is primarily used for storing static files?",
    "options": [
      "EC2",
      "S3",
      "DynamoDB",
      "RDS"
    ],
    "correct_answer": "S3",
    "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance for storing static files."
  },
  {
    "id": 2,
    "question": "Which AWS service would you use to run containers?",
    "options": [
      "EC2",
      "S3",
      "ECS/EKS",
      "Lambda"
    ],
    "correct_answer": "ECS/EKS",
    "explanation": "Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service) are services designed specifically for running containers in AWS."
  },
  {
    "id": 3,
    "question": "A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.\r\n\r\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?",
    "options": [
      "4 shards : 2 instances",
      "1 shard : 6 instances",
      "6 shards : 1 instance",
      "4 shards : 8 instances"
    ],
    "correct_answer": "4 shards : 2 instances",
    "explanation": "A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\n\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nSince the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer.\r\n\r\nThe 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.\r\n\r\nThe 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn\u2019t have a backup instance to process the shards in the event of an outage.\r\n\r\nThe 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well.\r\n\r\n"
  },
  {
    "id": 4,
    "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\r\n\r\nHow should the developer retrieve the variables with the FEWEST application changes? ",
    "options": [
      "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
      " Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment. ",
      " Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment. ",
      "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process. "
    ],
    "correct_answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
    "explanation": "A is correct:\r\n\r\n- It uses the appropriate services for the right types of data (Parameter Store for configuration, Secrets Manager for sensitive credentials)\r\n\r\n- It provides a centralized approach that requires minimal application changes\r\n- It supports hierarchical organization for different environments\r\n- It provides robust security controls through IAM\r\n- It enables changes to parameters without application redeployment\r\n\r\nThe application would only need to be updated once to retrieve variables from these services, and then all future changes to the variables would be managed through the services without additional application changes."
  },
  {
    "id": 5,
    "question": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.\r\n\r\nWhich section of a CloudFormation template cannot be associated with Condition?",
    "options": [
      "Conditions",
      "Resources",
      "Outputs",
      "Parameters"
    ],
    "correct_answer": "Parameters",
    "explanation": "Parameters\r\n\r\nParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\r\n\r\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\r\n\r\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.\r\n\r\nConditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.\r\n\r\nPlease review this note for more details:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\r\n\r\nPlease visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.\r\n\r\nIncorrect options:\r\n\r\nResources - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.\r\n\r\nConditions - You actually define conditions in this section of the CloudFormation template\r\n\r\nOutputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create."
  },
  {
    "id": 6,
    "question": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\r\n\r\nAs a Developer Associate, which of the following options would you recommend for the given use-case?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Cognito User Pools",
      "Lambda Authorizer",
      "API Gateway User Pools",
      "IAM permissions with sigv4"
    ],
    "correct_answer": "Lambda Authorizer",
    "explanation": "Correct option:\r\n\r\n\"Lambda Authorizer\"\r\n\r\nAn Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.\r\n\r\n via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nIncorrect options:\r\n\r\n\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.\r\n\r\n\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.\r\n\r\n\"API Gateway User Pools\" - This is a made-up option, added as a distractor.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"
  },
  {
    "id": 7,
    "question": "You're designing an application that processes data from an Amazon Kinesis Data Stream. Your current architecture includes a Kinesis stream with 8 shards. Each EC2 instance in your application runs a single Kinesis Client Library (KCL) consumer application.\r\n\r\nWhat is the optimal number of EC2 instances you should deploy to efficiently process this Kinesis stream?",
    "options": [
      "4 instances",
      " 8 instances",
      "16 instances",
      "32 instances"
    ],
    "correct_answer": " 8 instances",
    "explanation": "When designing applications that process Amazon Kinesis Data Streams, it's important to understand the relationship between shards and KCL workers for optimal performance.\r\n\r\nThe best practice is to match the number of instances (each running one KCL worker) to the number of shards in your Kinesis stream. This is because:\r\n\r\n- Each shard in a Kinesis Data Stream can be processed by exactly one KCL worker at any given time\r\n\r\n- A single KCL worker can process multiple shards, but this may not be optimal for performance\r\n\r\n- Having more KCL workers than shards is inefficient as some workers would be idle\r\n\r\n- Having fewer KCL workers than shards means some workers would be processing multiple shards, which might create a bottleneck\r\n\r\nIn this scenario, with 8 shards in your Kinesis stream, the optimal configuration would be 8 EC2 instances, each running one KCL worker application. This provides a 1:1 mapping between shards and KCL workers, ensuring maximum throughput and parallel processing capability.\r\n\r\nIf your data processing needs change, you would typically adjust both the number of shards and the number of instances accordingly to maintain this optimal ratio."
  },
  {
    "id": 8,
    "question": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\r\n\r\n\r\nWhich of the following does **NOT** help with debugging the issue?",
    "options": [
      "X-Ray sampling",
      "EC2 Instance Role",
      "EC2 X-Ray Daemon",
      "CloudTrail"
    ],
    "correct_answer": "X-Ray sampling",
    "explanation": "Correct option:\r\n\r\n**X-Ray sampling**\r\n\r\nWhy X-Ray sampling WON'T help:\r\nX-Ray sampling only controls which requests get traced, not whether those traces successfully reach X-Ray. Adjusting sampling rules is like deciding how many photos to take, but won't help if the camera can't transmit pictures to your cloud storage.\r\n\r\n\r\n**EC2 X-Ray Daemon**\r\n\r\n- The X-Ray daemon is the component that actually sends trace data to AWS\r\n\r\n- Checking daemon logs would show connection errors, timeouts, or permission issues\r\n\r\n- You could verify if the daemon is running correctly with ps aux | grep xray\r\n\r\n- The daemon log file at /var/log/xray/xray.log might contain error messages\r\n\r\n\r\n#### 2. EC2 Instance Role\r\n* The X-Ray daemon needs proper IAM permissions to send data to X-Ray\r\n* The EC2 instance role provides these permissions automatically\r\n* Checking the attached role and its policies would reveal missing permissions\r\n* You could verify the policy includes `xray:PutTraceSegments` and `xray:PutTelemetryRecords`\r\n\r\n\r\n#### 3. CloudTrail\r\n* CloudTrail logs all API calls made to AWS services\r\n* It would show denied API calls due to permission issues\r\n* You could search for X-Ray related API calls from your EC2 instance\r\n* Failed API calls would include detailed error messages explaining why they failed\r\n\r\n## Key Troubleshooting Steps\r\n\r\nIn this scenario, you should:\r\n1. Check if the X-Ray daemon is running on the EC2 instance\r\n2. Verify the EC2 instance role has appropriate X-Ray permissions\r\n3. Look at CloudTrail logs for denied X-Ray API calls\r\n4. Check security groups and network ACLs to ensure outbound traffic to X-Ray endpoints is allowed\r\n\r\nAdjusting sampling rules would not provide any useful diagnostic information since no data is being transmitted at all."
  },
  {
    "id": 9,
    "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket, the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).\r\n\r\nWhich solution will guarantee that any upload request without the mandated encryption is not processed?",
    "options": [
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `aws:kms`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `sse:s3`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
      "Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header."
    ],
    "correct_answer": "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.",
    "explanation": "### Why Option C is Correct:\r\nWhen using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), you need to:\r\n\r\n1. Set the `x-amz-server-side-encryption` header to `AES256` in your PutObject requests\r\n2. Implement a bucket policy that enforces this requirement\r\n\r\nThis approach works because:\r\n* `AES256` is the correct value that specifies SSE-S3 encryption (Amazon S3-managed keys)\r\n* The bucket policy can deny any requests that either:\r\n  * Don't include the encryption header at all\r\n  * Include the header but with an incorrect value\r\n\r\nA typical bucket policy would look like this:\r\n```json\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis policy uses two statements:\r\n* The first statement denies requests where the header exists but has an incorrect value\r\n* The second statement denies requests where the header is missing entirely\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option A is incorrect** because:\r\n* `aws:kms` is used for SSE-KMS (Server-Side Encryption with AWS KMS keys)\r\n* The question specifically requires SSE-S3 (Server-Side Encryption with Amazon S3-managed keys)\r\n* While this would enforce encryption, it would use the wrong type of encryption\r\n\r\n**Option B is incorrect** because:\r\n* `sse:s3` is not a valid value for the `x-amz-server-side-encryption` header\r\n* The valid values are `AES256` (for SSE-S3) or `aws:kms` (for SSE-KMS)\r\n* Using an invalid header value would cause all requests to fail\r\n\r\n**Option D is incorrect** because:\r\n* With SSE-S3, you don't specify or manage the encryption keys\r\n* Amazon S3 automatically handles key management, generating a unique key for each object\r\n* There is no way to \"set the encryption key for SSE-S3\" in an HTTP header\r\n\r\n### Key Concept:\r\nWhen using SSE-S3, remember that:\r\n1. Amazon S3 handles all key management automatically\r\n2. Each object is encrypted with a unique key\r\n3. These keys are themselves encrypted with a master key that AWS rotates regularly\r\n4. The encryption standard used is AES-256\r\n5. You only need to specify the encryption method (`AES256`), not any keys\r\n\r\n## AWS Documentation Reference\r\nFor more information, refer to the [Amazon S3 Developer Guide on Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)."
  },
  {
    "id": 10,
    "question": "A development team has created a serverless application that uses Amazon API Gateway and AWS Lambda. They want to use a single Lambda function across multiple API Gateway stages (development, testing, and production), but they need the function to read from a different DynamoDB table depending on which stage is being called. \r\n\r\nWhat is the MOST appropriate way for the developer to pass these configuration parameters to the Lambda function?",
    "options": [
      "Use Stage Variables in API Gateway and reference them in mapping templates",
      "Set up an API Gateway Private Integration to the Lambda function",
      "Create environment variables in the Lambda function for each table name",
      "Configure traffic shifting with Lambda Aliases for each stage"
    ],
    "correct_answer": "Use Stage Variables in API Gateway and reference them in mapping templates",
    "explanation": "### Why Option A is Correct:\r\nStage variables are name-value pairs that function as configuration attributes for different deployment stages of your API Gateway REST API. They effectively work like environment variables that can be accessed from various parts of your API configuration, including mapping templates.\r\n\r\nThis solution works perfectly for the scenario because:\r\n\r\n1. **Dynamic Configuration Per Stage**: Stage variables allow you to set different values for each deployment stage (development, testing, production)\r\n\r\n2. **Accessible in Mapping Templates**: You can reference stage variables in the mapping templates that generate the request for your Lambda function\r\n\r\n3. **No Code Changes Needed**: The Lambda function code remains the same across all environments, making maintenance easier\r\n\r\n4. **Implementation Example**:\r\n   ```json\r\n   // API Gateway mapping template example\r\n   {\r\n     \"tableName\": \"$stageVariables.dynamoDBTableName\",\r\n     \"operation\": \"read\",\r\n     \"key\": {\r\n       \"id\": \"$input.params('id')\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In this example, `$stageVariables.dynamoDBTableName` would contain different values in different stages:\r\n   - In development: \"dev-customer-table\"\r\n   - In testing: \"test-customer-table\"\r\n   - In production: \"prod-customer-table\"\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option B is incorrect** because:\r\n* API Gateway Private Integration is used to connect API Gateway to private resources in your VPC\r\n* It doesn't provide a way to dynamically configure which DynamoDB table the Lambda function should use\r\n* It's designed for network connectivity, not for configuration parameter passing\r\n\r\n**Option C is incorrect** because:\r\n* Lambda environment variables are static for a given function and don't change based on which API stage called the function\r\n* While you could check the stage name in your Lambda code and use different tables based on that, this would require code changes and additional logic\r\n* This approach would be less maintainable as it mixes configuration with application code\r\n\r\n**Option D is incorrect** because:\r\n* Lambda Aliases are used to point to specific versions of Lambda functions\r\n* Traffic shifting with aliases is for gradually moving traffic between different versions of a function\r\n* This doesn't solve the problem of using different DynamoDB tables without modifying the function code\r\n* This approach would require maintaining multiple versions of essentially the same code with different table names hardcoded\r\n\r\n### Key Concept:\r\nAPI Gateway Stage Variables provide a clean separation of configuration from code, allowing you to deploy the same Lambda function code to multiple environments while dynamically changing its behavior based on which stage is calling it.\r\n\r\n## AWS Documentation Reference\r\nFor more information on using stage variables with Lambda functions, refer to the [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html)."
  },
  {
    "id": 11,
    "question": "A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department.\r\n\r\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.\r\n\r\nWhich of the following options is the MOST suitable solution that the developer implements?\r\n\r\n",
    "options": [
      "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
      "Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.",
      "Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.",
      "Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls."
    ],
    "correct_answer": "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
    "explanation": "The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.\r\n\r\nIt is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following:\r\n\r\n-The ability to send traces to multiple different tracing backends without having to re-instrument your code\r\n\r\n-Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community\r\n\r\n-Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js\r\n\r\n\r\nConversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following:\r\n\r\n-A tightly integrated single-vendor solution\r\n\r\n-Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET\r\n\r\nAn account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default:\r\n\r\nhttps://Your_AWS_Account_ID.signin.aws.amazon.com/console/\r\n\r\nIf you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example.\r\n\r\nhttps://Your_Alias.signin.aws.amazon.com/console/\r\n\r\nThe original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account:\r\n\r\naws iam create-account-alias --account-alias demosite.com\r\n\r\n**Hence, for this scenario, the correct answer is:** Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\r\n\r\n**The option that says:** Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature.\r\n\r\n**The option that says:** Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA).\r\n\r\n**The option that says:**Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn\u2019t have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry."
  },
  {
    "id": 12,
    "question": "An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\r\n\r\n\r\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?\r\n\r\n",
    "options": [
      "Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.",
      "Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.",
      "Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.",
      "Update the stacks on multiple AWS accounts using CloudFormation StackSets."
    ],
    "correct_answer": "Update the stacks on multiple AWS accounts using CloudFormation StackSets.",
    "explanation": "**AWS CloudFormation StackSets** extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\r\n\r\n\r\n\r\nA ***stack set*** lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set\u2019s AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.\r\n\r\nHence, the correct solution in this scenario is to **update the stacks on multiple AWS accounts using CloudFormation StackSets.**\r\n\r\n&nbsp;\r\n\r\nAfter you\u2019ve defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions.\r\n\r\nThe option that says: **Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets** is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets.\r\n\r\nThe option that says: **Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances** is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets.\r\n\r\nThe option that says: **Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts** is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"
  },
  {
    "id": 13,
    "question": "A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition.\r\n\r\nWhich of the following is the EASIEST way to accomplish this task?\r\n\r\n",
    "options": [
      "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",
      "Use AWS SAM to migrate and deploy the company's web services to API Gateway.",
      "Create models and templates for request and response mappings based on the company's API definitions.",
      "Use CodeDeploy to migrate and deploy the company's web services to API Gateway."
    ],
    "correct_answer": "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",
    "explanation": "You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.\r\n\r\nYou can paste a [Swagger](http://swagger.io/) API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below:\r\n\r\n\r\nYou can also import your Swagger definition through the AWS CLI and SDKs.\r\n\r\nHence, the correct answer in this scenario is to **import their Swagger or OpenAPI definitions to API Gateway using the AWS Console**.\r\n\r\n**Using CodeDeploy to migrate and deploy the company\u2019s web services to API Gateway** is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment.\r\n\r\n**Using AWS SAM to migrate and deploy the company\u2019s web services to API Gateway** is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS.\r\n\r\n**Creating models and templates for request and response mappings based on the company\u2019s API definitions** is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file."
  },
  {
    "id": 14,
    "question": "A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don\u2019t need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring.\r\n\r\nWhich service will you use to easily deploy and manage the application?\r\n\r\n",
    "options": [
      "AWS Elastic Beanstalk",
      "AWS CloudFormation",
      "AWS CodeDeploy",
      "AWS SAM"
    ],
    "correct_answer": "AWS Elastic Beanstalk",
    "explanation": "With **Elastic Beanstalk**, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n&nbsp;\r\n\r\nYou can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).\r\n\r\nTo use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**AWS CloudFormation** is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can\u2019t host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring.\r\n\r\n**AWS CodeDeploy** is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."
  },
  {
    "id": 15,
    "question": "A mobile game has a DynamoDB table named `AWSDevScores` which keeps track of the users and their respective scores. Each item in the table is identified by the `FighterId` attribute as its partition key and the `FightTitle` attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named `AWSDevTopScores` and `AWSDevDateTime` attributes.\r\n\r\n\r\nWhich type of index should the developer add in the table to speed up queries on non-key attributes?\r\n\r\n",
    "options": [
      "Primary Index",
      "Sparse Index",
      "Global Secondary Index",
      "Local Secondary Index"
    ],
    "correct_answer": "Global Secondary Index",
    "explanation": "Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue `Query` or `Scan` requests against these indexes.\r\n\r\n&nbsp;\r\n\r\nA ***secondary index*** is a data structure that contains a subset of attributes from a table, along with an alternate key to support `Query` operations. You can retrieve data from the index using a `Query`, in much the same way as you use `Query` with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nTo speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn\u2019t even need to have the same key schema as a table.\r\n\r\nHence, the correct answer in this scenario is to add a **Global Secondary Index**.\r\n\r\n**Sparse index** is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn\u2019t appear in every table item, the index is said to be *\u201csparse\u201d*.\r\n\r\n**Local Secondary Index** is incorrect because this is used for queries which use the same partition key value, and in addition, you can\u2019t add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.\r\n\r\n**Primary Index** is incorrect because this one actually refers to the partition key, which is the `FighterId` attribute in this scenario."
  },
  {
    "id": 16,
    "question": "A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture.\r\n\r\nWhich of the following services is the MOST suitable one to use?",
    "options": [
      "AWS SAM",
      "EC2",
      "Elastic Beanstalk",
      "ECS"
    ],
    "correct_answer": "ECS",
    "explanation": "**Amazon Elastic Container Service (Amazon ECS)** is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.\r\n\r\nYou can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\r\n\r\nElastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more **fine-grained** control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **ECS.**\r\n\r\n**Elastic Beanstalk** is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS.\r\n\r\n**AWS SAM** is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications.\r\n\r\n**EC2** is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB.\r\n\r\n"
  },
  {
    "id": 17,
    "question": "An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available.\r\n\r\nWhich of the following is the most suitable feature that you have to use to meet this requirement?",
    "options": [
      "AWS Device Farm",
      "Amazon Cognito Identity Pools",
      "Amazon Cognito Sync",
      "Amazon Cognito User Pools"
    ],
    "correct_answer": "Amazon Cognito Sync",
    "explanation": "Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.\r\n\r\nAmazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user\u2019s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.\r\n\r\nThe Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize.\r\n\r\nAmazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.\r\n\r\nHence, the correct answer is to **Amazon Cognito Sync***.*\r\n\r\n**Amazon Cognito User Pools** is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito.\r\n\r\n**Amazon Cognito Identity Pools** is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\n**AWS Device Farm** is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.\r\n\r\n&nbsp;"
  },
  {
    "id": 18,
    "question": "A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing.\r\n\r\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?",
    "options": [
      "Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.",
      "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
      "Postpone the delivery of new messages by using a delay queue.",
      "Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0."
    ],
    "correct_answer": "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
    "explanation": "The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.\r\n\r\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\u2019t automatically delete the message. Because Amazon SQS is a distributed system, there\u2019s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.\r\n\r\nImmediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a ***visibility timeout***, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.\r\n\r\nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn\u2019t call the `[DeleteMessage](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html)` action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout.\r\n\r\nEvery Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout.\r\n\r\nHence, the best solution in this scenario is to **set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue**.\r\n\r\n**Configuring the queue to use short polling by setting the `WaitTimeSeconds` parameter of the `ReceiveMessage` request to 0** is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario.\r\n\r\n**Configuring the queue to use long polling by setting the `Receive Message Wait Time` parameter to a value greater than 0** is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (*when there are no messages available for a ReceiveMessage request*) and false empty responses (*when messages are available but aren\u2019t included in a response*). A more appropriate solution in this scenario is to configure the visibility timeout of the messages.\r\n\r\n**Postponing the delivery of new messages by using a delay queue** is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts."
  },
  {
    "id": 19,
    "question": "A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\r\n\r\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?",
    "options": [
      "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
      "Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.",
      "Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.",
      "Enable auto-scaling on the Fargate tasks."
    ],
    "correct_answer": "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
    "explanation": "CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).\r\n\r\n&nbsp;\r\n\r\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:\r\n\r\n**\u2013 Cache key normalization** \u2013 You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.\r\n\r\n**\u2013 Header manipulation** \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a `True-Client-IP` header to every request.\r\n\r\n**\u2013 Status code modification and body generation** \u2013 You can evaluate headers and respond back to viewers with customized content.\r\n\r\n**\u2013 URL redirects or rewrites** \u2013 You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.\r\n\r\n**\u2013 Request authorization** \u2013 You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\r\n\r\nWhen you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.\r\n\r\nCloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response)."
  },
  {
    "id": 20,
    "question": "A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application\u2019s performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading.\r\n\r\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?",
    "options": [
      "Refactor the application to use DynamoDB transactional read and write APIs .",
      "Enable DynamoDB Streams.",
      "Upgrade the EC2 instances to a higher instance type.",
      "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations."
    ],
    "correct_answer": "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.",
    "explanation": "For applications that need to read or write multiple items, DynamoDB provides the `BatchGetItem` and `BatchWriteItem` operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\r\n\r\nThe batch operations are essentially wrappers around multiple read or write requests. For example, if a `BatchGetItem` request contains five items, DynamoDB performs five `GetItem` operations on your behalf. Similarly, if a `BatchWriteItem` request contains two put requests and four delete requests, DynamoDB performs two `PutItem` and four `DeleteItem` requests.\r\n\r\nIn general, a batch operation does not fail unless *all* of the requests in the batch fail. For example, suppose you perform a `BatchGetItem`operation but one of the individual `GetItem` requests in the batch fails. In this case, `BatchGetItem` returns the keys and data from the `GetItem`request that failed. The other `GetItem` requests in the batch are not affected.\r\n\r\nHence, the correct answer is to **use DynamoDB Batch Operations API for GET, PUT, and DELETE operations** in this scenario.\r\n\r\n**Upgrading the EC2 instances to a higher instance type** is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation.\r\n\r\n**Enabling DynamoDB Streams** is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch.\r\n\r\n**Refactoring the application to use DynamoDB transactional read and write APIs** is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario."
  },
  {
    "id": 21,
    "question": "You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\r\n\r\nWhich of the following statements are TRUE regarding this scenario?",
    "options": [
      "There will be at most 100 Lambda function invocations running concurrently.",
      "The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.",
      "The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.",
      "The Lambda function has 500 concurrent executions."
    ],
    "correct_answer": "There will be at most 100 Lambda function invocations running concurrently.",
    "explanation": "You can use an **AWS Lambda function** to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.\r\n\r\n***Concurrent executions*** refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source.\r\n\r\nFor Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard\u2019s events in sequence.\r\n\r\nHence, the correct answer in this scenario is that: **there will be at most 100 Lambda function invocations running concurrently.**\r\n\r\nThe option that says: **the Lambda function has 500 concurrent executions** is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function.\r\n\r\nThe option that says: **the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards** is incorrect because, by default, AWS Lambda will automatically scale the function\u2019s concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream.\r\n\r\nThe option that says: **the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function** is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function."
  },
  {
    "id": 22,
    "question": "A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity.\r\n\r\nWhich of the following is the MOST appropriate service that the developer should use in this scenario?",
    "options": [
      "Serverless Application Framework",
      "AWS CloudFormation",
      "AWS SAM",
      "AWS Systems Manager"
    ],
    "correct_answer": "AWS SAM",
    "explanation": "The AWS Serverless Application Model (AWS SAM) is an open source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML.\r\n\r\nAWS SAM is natively supported by AWS CloudFormation and provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.\r\n\r\nHence, the correct answer is **AWS SAM**.\r\n\r\n**AWS CloudFormation** is incorrect. Although this service can deploy the serverless application to AWS, it is still more appropriate to use AWS SAM instead. AWS SAM can simplify the deployment of the serverless application by deploying all related resources together as a single, versioned entity.\r\n\r\n**AWS Systems Manager** is incorrect because it is more focused on management and operations of AWS resources, such as automation, patching, and configuration, but it is not a deployment or application modeling tool.\r\n\r\n**Serverless Application Framework** is incorrect. Although it is a well-known framework for building and deploying serverless applications into the AWS cloud, this is not an AWS native solution. It also does not allow configuration of DynamoDB databases or API Gateway APIs, unlike AWS SAM."
  },
  {
    "id": 23,
    "question": "A developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with millisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.\r\n\r\nWhich of the following is the MOST suitable solution to reduce the cost and capacity of the stream?",
    "options": [
      "Split cold shards",
      "Split hot shards",
      "Merge cold shards",
      "Merge hot shards"
    ],
    "correct_answer": "Merge cold shards",
    "explanation": "The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\r\n\r\nOne approach to resharding could be to split every shard in the stream\u2014which would double the stream\u2019s capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/93bb0d37c31e4f369a3ec81e0279248f.png)\r\n\r\nYou can also use metrics to determine which are your \u201chot\u201d or \u201ccold\u201d shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could ***merge cold shards*** to make better use of their unused capacity.\r\n\r\nYou can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream.\r\n\r\nHence, the correct answer is to **merge cold shards** to reduce the capacity and the cost of running your Kinesis Data Stream.\r\n\r\n**Splitting cold shards** is incorrect because a cold shard is the one that receives fewer data which means that you have to merge them to reduce the capacity rather than split them.\r\n\r\n**Merging hot shards** is incorrect. Although merging shards is correct, the type of shard to be merged is wrong. A hot shard is the one that receives more data in the stream. Merging hot shards could potentially overload the newly merged shard with a high volume of data, causing a bottleneck in processing and degrading the overall performance of the stream.\r\n\r\n**Splitting hot shards** is incorrect because this will actually further increase both the cost and capacity of the stream rather than reduce it. Moreover, there are no hot shards in the stream since the scenario specifically mentioned that the shards are way underutilized.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html"
  },
  {
    "id": 24,
    "question": "You have two users concurrently accessing a DynamoDB table and submitting updates. If a user will modify a specific item in the table, she needs to make sure that the operation will not affect another user\u2019s attempt to modify the same item. You have to ensure that your update operations will only succeed if the item attributes meet one or more expected conditions.\r\n\r\nWhich of the following DynamoDB features should you use in this scenario?",
    "options": [
      "Conditional writes",
      "Batch Operations",
      "Update Expressions",
      "Projection Expressions"
    ],
    "correct_answer": "Conditional writes",
    "explanation": "By default, the **DynamoDB** write operations (`PutItem`, `UpdateItem`, `DeleteItem`) are *unconditional*: each of these operations will overwrite an existing item that has the specified primary key.\r\n\r\nDynamoDB optionally supports **conditional writes** for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in cases where multiple users attempt to modify the same item.\r\n\r\nFor example, by adding a conditional expression that checks if the current value of the item is still the same, you can be sure that your update will not affect the operations of other users:\r\n\r\n```\r\naws dynamodb update-item \\\r\n --table-name ProductCatalog \\\r\n --key '{\"Id\":{\"N\":\"1\"}}' \\\r\n --update-expression \"SET Price = :newval\" \\\r\n --condition-expression \"Price = :currval\" \\\r\n --expression-attribute-values [file://expression-attribute-values.json](file://expression-attribute-values.json/)\r\n\r\n\r\n\r\n<img loading=\"lazy\" decoding=\"async\" src=\"file:///home/skworkstation/.config/joplin-desktop/resources/45f64df20f1b4df2a475bffa6dce5583.png?t=1748305400704\" width=\"674\" height=\"663\" style=\"box-sizing: border-box; border: 0px; font-style: italic; height: auto; max-width: 100%; vertical-align: sub !important; display: block; margin-left: auto; margin-right: auto;\">\r\n```\r\n\r\nHence, the correct answer is **conditional writes**.\r\n\r\n**Using projection expressions** is incorrect because this is just a string that identifies the attributes you want to retrieve during a `GetItem`, `Query`, or `Scan` operation. Take note that the scenario calls for a feature that can be used during a write operation hence, this option is irrelevant.\r\n\r\n**Using update expressions** is incorrect because this simply specifies how `UpdateItem` will modify the attributes of an item such as for setting a scalar value or removing elements from a list or a map. This feature doesn\u2019t use any conditions which is what the scenario is looking for. Therefore, this option is incorrect.\r\n\r\n**Using batch operations** is incorrect because these are essentially wrappers for multiple read or write requests. Batch operations are primarily used when you want to retrieve or submit multiple items in DynamoDB through a single API call, which reduces the number of network round trips from your application to DynamoDB.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ReadingData\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.html\r\n\r\n**Check out this Amazon DynamoDB Cheat Sheet:**\r\n\r\nhttps://tutorialsdojo.com/amazon-dynamodb/"
  },
  {
    "id": 25,
    "question": "A company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the different processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each process to ensure application performance.\r\n\r\nWhich of the following is the MOST suitable solution that the developer should implement?",
    "options": [
      "Use Enhanced Monitoring in RDS.",
      "Develop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance.",
      "Use CloudWatch to track the CPU Utilization of your database.",
      "Track the CPU% and MEM% metrics which are readily available in the Amazon RDS console."
    ],
    "correct_answer": "Use Enhanced Monitoring in RDS.",
    "explanation": "**Amazon RDS** provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the `RDSOSMetrics` log group in the CloudWatch console.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/dd2718487c7c4d85ad6a05d9da823421.png)\r\n\r\nTake note that there are certain differences between CloudWatch and Enhanced Monitoring Metrics. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work.\r\n\r\nThe differences can be greater if your DB instances use smaller instance classes because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.\r\n\r\nHence, the correct answer is to **use Enhanced Monitoring in RDS**.\r\n\r\n**Developing a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance** is incorrect. Although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database process. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.\r\n\r\n**Using CloudWatch to track the CPU Utilization of your database** is incorrect. Although you can use Amazon CloudWatch to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.\r\n\r\n**Tracking the `CPU%` and `MEM%` metrics which are readily available in the Amazon RDS console** is incorrect because these metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch\r\n\r\n**Check out these Amazon CloudWatch and RDS Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/amazon-cloudwatch/\r\n\r\n**https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/**"
  },
  {
    "id": 26,
    "question": "A company has a suite of web applications that is heavily using RDS database in Multi-AZ Deployments configuration with several Read Replicas. For improved security, you were instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis. You should also configure your applications to use the latest version of the encrypted credentials when connecting to the RDS database.\r\n\r\nWhich of the following is the MOST appropriate solution to secure the credentials?\r\n\r\n",
    "options": [
      "Store the credentials in AWS KMS.",
      "Store the credentials to AWS ACM.",
      "Store the credentials to Systems Manager Parameter Store with a SecureString data type.",
      "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation."
    ],
    "correct_answer": "Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation.",
    "explanation": "**AWS Secrets Manager** is an AWS service that makes it easier for you to manage secrets. *Secrets* can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs.\r\n\r\nIn the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/fe3ac9000d2641d98d3d532b640e1ff4.png)\r\n\r\n**Secrets Manager** enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can\u2019t be compromised by someone examining your code, because the secret simply isn\u2019t there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.\r\n\r\nHence, **using AWS Secrets Manager to store and encrypt the credentials and enabling automatic rotation** is the most appropriate solution for this scenario.\r\n\r\n**Storing the credentials to Systems Manager Parameter Store with a `SecureString` data type** is incorrect because, by default, Systems Manager Parameter Store doesn\u2019t rotate its parameters which is one of the requirements in the above scenario.\r\n\r\n**Storing the credentials to AWS ACM** is incorrect because it is just a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates to allow SSL communication to your application. This is not a suitable service to store database or any other confidential credentials.\r\n\r\n**Storing the credentials in AWS KMS** is incorrect because this only makes it easy for you to create and manage encryption keys and control the use of encryption across a wide range of AWS services. This is primarily used for encryption and not for hosting your credentials.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\r\n\r\nhttps://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/\r\n\r\n**Check out these AWS Systems Manager and Secrets Manager Cheat Sheets:**\r\n\r\nhttps://tutorialsdojo.com/aws-systems-manager/\r\n\r\nhttps://tutorialsdojo.com/aws-secrets-manager/\r\n\r\n&nbsp;"
  },
  {
    "id": 27,
    "question": "A developer is utilizing AWS X-Ray to generate a visual representation of the requests flowing through their enterprise web application. Since the application interacts with multiple services, all requests must be traced in X-Ray, including any downstream calls made to AWS resources.\r\n\r\nWhich of the following actions should the developer implement for this scenario?",
    "options": [
      "Install AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls.",
      "Pass multiple trace segments as a parameter of PutTraceSegments API.",
      "Use AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API.",
      "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches."
    ],
    "correct_answer": "Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches.",
    "explanation": "You can send trace data to X-Ray in the form of segment documents. A **segment document** is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments or work that uses downstream services and resources in subsegments.\r\n\r\nA segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the `PutTraceSegments` API. An alternative is, instead of sending segment documents to the X-Ray API, you can send segments and subsegments to an X-Ray daemon, which will buffer them and upload to the X-Ray API in batches. The X-Ray SDK sends segment documents to the daemon to avoid making calls to AWS directly. This is the correct option among the choices.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/322d4919e11e4f1cb6282f3d963f180e.png)\r\n\r\nHence, **using X-Ray SDK to generate segment documents with subsegments and sending them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches** is the correct answer in this scenario.\r\n\r\n**Using AWS X-Ray SDK to upload a trace segment by executing `PutTraceSegments` API** is incorrect because you should upload the segment documents with subsegments instead. A trace segment is just a JSON representation of a request that your application serves.\r\n\r\n**Installing AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls** is incorrect because you cannot run a trace on the application and the services at the same time as this will produce two different results. You simply have to send the segment documents with subsegments to get the information about downstream calls that your application makes to AWS resources.\r\n\r\n**Passing multiple trace segments as a parameter of `PutTraceSegments` API** is incorrect because, contrary to the API\u2019s name, you have to upload **segment** documents and not trace segments. The API has a single parameter: `TraceSegmentDocuments`, that takes a list of JSON segment documents.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\r\n\r\n"
  },
  {
    "id": 28,
    "question": "A developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities of data, the write capacity units must be specified for the expected workload on both the base table and its secondary index.\r\n\r\nWhich of the following should the developer do to avoid any potential request throttling?",
    "options": [
      "Ensure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.",
      "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",
      "Ensure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.",
      "Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table."
    ],
    "correct_answer": "Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",
    "explanation": "A **global secondary index (GSI)** is an index with a partition key and a sort key that can be different from those on the base table. It is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions.\r\n\r\nEvery global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.\r\n\r\nWhen you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A `Query` operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/b6e6a4f0ec9a4a738194046eca1d9cc5.gif)\r\n\r\nFor example, if you `Query` a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled.\r\n\r\nTo avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.\r\n\r\nHence, the correct answer in this scenario is to **ensure that the global secondary index\u2019s provisioned WCU is equal to or greater than the WCU of the base table**.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned WCU is equal or less than the WCU of the base table** is incorrect because it should be the other way around, just as what is mentioned above. The provisioned write capacity for a global secondary index should be equal to or greater than the write capacity of the base table.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal to or greater than the RCU of the base table** is incorrect because you have to set the WCU and not the RCU.\r\n\r\n**Ensuring that the global secondary index\u2019s provisioned RCU is equal or less than the RCU of the base table** is incorrect because this should be WCU and in addition, the global secondary index\u2019s provisioned WCU should be set to a value that is equal or greater than the WCU of the base table to prevent request throttling.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes\r\n"
  },
  {
    "id": 29,
    "question": "You currently have an IAM user for working in the development environment using shell scripts that call the AWS CLI. The EC2 instance that you are using already contains the access key credential set and an IAM role, which are used to run the CLI and access the development environment. You were given a new set of access key credentials with another IAM role that allows you to access and manage the production environment.\r\n\r\nWhich of the following is the EASIEST way to switch from one role to another?",
    "options": [
      "Store the production access key credentials set in the instance metadata and call this whenever you need to access the production environment.",
      "Store the production access key credentials set in the user data of the instance and call this whenever you need to access the production environment.",
      "Create a new instance profile in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",
      "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command."
    ],
    "correct_answer": "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",
    "explanation": "Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications.\r\n\r\nThis extra step is the creation of an *[instance profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)* that is attached to the instance. The instance profile contains the role and can provide the role\u2019s temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application\u2019s API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/52366299d8854f979e988e23c01c2a3a.png)\r\n\r\nUsing roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don\u2019t have to manage credentials, and you don\u2019t have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances.\r\n\r\nImagine that you have an IAM user for working in the development environment and you occasionally need to work with the production environment at the command line with the [AWS CLI](http://aws.amazon.com/cli/). You already have an access key credential set available to you. This can be the access key pair that is assigned to your standard IAM user. Or, if you signed in as a federated user, it can be the access key pair for the role that was initially assigned to you. If your current permissions grant you the ability to assume a specific IAM role, then you can identify that role in a \u201cprofile\u201d in the AWS CLI configuration files. That command is then run with the permissions of the specified IAM role, not the original identity.\r\n\r\nNote that when you specify that profile in an AWS CLI command, you are using the new role. In this situation, you cannot make use of your original permissions in the development account at the same time. The reason is that only one set of permissions can be in effect at a time.\r\n\r\nHence, the correct answer is to **create a new profile for the role in the AWS CLI configuration file then append the `--profile` parameter, along with the new profile name, whenever you run the CLI command**.\r\n\r\n**Storing the production access key credentials set in the instance metadata and calling this whenever you need to access the production environment** is incorrect because instance metadata is primarily used to fetch the data about your instance that you can use to configure or manage the running instance. This is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**Creating a new instance profile in the AWS CLI configuration file then appending the `--profile` parameter along with the new profile name whenever you run the CLI command** is incorrect because an instance profile is just a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is different from an AWS CLI **profile**, which you can use for switching to various profiles. In addition, an instance profile is associated with the instance and not configured in the AWS CLI.\r\n\r\n**Storing the production access key credentials set in the user data of the instance and calling this whenever you need to access the production environment** is incorrect because user data is primarily used to configure an instance during launch, or to run a configuration script. Just like instance metadata, this is not suitable for use in storing the access keys of your AWS CLI.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\r\n\r\n**Check out this AWS Identity & Access Management (IAM) Cheat Sheet:**\r\n\r\n",
    "question_type": "single",
    "correct_answers": [
      "Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command."
    ]
  },
  {
    "id": 30,
    "question": "A company has an application hosted in an On-Demand EC2 instance in your VPC. The developer has been instructed to create a shell script that fetches the instance\u2019s associated public and private IP addresses.\r\n\r\nWhat should the developer do to complete this task?",
    "options": [
      "Get the public and private IP addresses from AWS CloudTrail.",
      "Get the public and private IP addresses from Amazon CloudWatch.",
      "Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint.",
      "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint."
    ],
    "correct_answer": "Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint.",
    "explanation": "**Instance metadata** is data about your EC2 instance that you can use to configure or manage the running instance. Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you\u2019re writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/91b0c543781c40faaa38c368e3616c23.png)\r\n\r\nTo view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL: `http://169.254.169.254/latest/meta-data/`.\r\n\r\nHence, the correct answer is: **Get the public and private IP addresses from the instance metadata service using the `http://169.254.169.254/latest/meta-data/` endpoint.**\r\n\r\nThe option that says: **Get the public and private IP addresses from Amazon CloudWatch** is incorrect because there is no direct way to fetch the public and private IP addresses of the EC2 instance using CloudWatch.\r\n\r\nThe option that says: **Get the public and private IP addresses from AWS CloudTrail** is incorrect because CloudTrail is primarily used to track the API activity of each AWS service. Just like CloudWatch, there is no easy way to get the associated IP addresses of the EC2 instance using CloudTrail.\r\n\r\nThe option that says: **Get the public and private IP addresses from the instance user data service using the `http://169.254.169.254/latest/userdata/` endpoint** is incorrect because a user data is mainly used to perform common automated configuration tasks and run scripts after the instance starts. You will not find the associated IP addresses of the EC2 instance from its user data. You have to use the metadata service instead.\r\n\r\n**References:**\r\n\r\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html"
  },
  {
    "id": 31,
    "question": "A leading financial company has recently deployed its application to AWS using Lambda and API Gateway. However, they noticed that all metrics are being populated in their CloudWatch dashboard except for `CacheHitCount` and `CacheMissCount`.\r\n\r\nWhat could be the MOST likely cause of this issue?",
    "options": [
      "API Caching is not enabled in API Gateway.",
      "The provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch.",
      "They have not provided an IAM role to their API Gateway yet.",
      "API Gateway Private Integrations has not been configured yet."
    ],
    "correct_answer": "API Caching is not enabled in API Gateway.",
    "explanation": "You can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics. These statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your web application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/e7a5dad826b342b68617a7de98552fbc.png)\r\n\r\nThe metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list.\r\n\r\n\u2013 Monitor the **IntegrationLatency** metrics to measure the responsiveness of the backend.\r\n\r\n\u2013 Monitor the **Latency** metrics to measure the overall responsiveness of your API calls.\r\n\r\n\u2013 Monitor the **CacheHitCount** and **CacheMissCount** metrics to optimize cache capacities to achieve a desired performance. CacheMissCount tracks the number of requests served from the backend in a given period, <ins>when API caching is enabled</ins>. On the other hand, CacheHitCount track the number of requests served from the API cache in a given period.\r\n\r\nHence, the root cause of this issue is that the **API Caching is not enabled in API Gateway** which is why the ***CacheHitCount*** and ***CacheMissCount*** metrics are not populated.\r\n\r\nThe option that says: **they have not provided an IAM role to their API Gateway yet** is incorrect because, in the first place, the scenario already mentioned that all metrics are being populated in their CloudWatch dashboard except for two metrics. This implies that some of the metrics are populated which means that the API Gateway already has an IAM Role associated with it.\r\n\r\nThe option that says: **the provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch** is incorrect because just as what is mentioned above, there is no issue with the IAM Role since all metrics are being populated except only for `CacheHitCount` and `CacheMissCount`. This means that the associated IAM Role already has *write* privileges to write logs to CloudWatch to begin with. The only reason why those two metrics are not being populated is that the API Caching is not enabled.\r\n\r\nThe option that says: **API Gateway Private Integrations has not been configured yet** is incorrect because this feature only makes it easier to expose your HTTP/HTTPS resources behind an Amazon VPC for access by clients outside of the VPC.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring-cloudwatch.html"
  },
  {
    "id": 32,
    "question": "Your application is hosted on an Auto Scaling group of EC2 instances with a DynamoDB database. There were a lot of data discrepancy issues where the changes made by one user were always overwritten by another user. You noticed that this usually happens whenever there are a lot of people updating the same data.\r\n\r\nWhat should you do to solve this problem?",
    "options": [
      "Implement a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
      "Use DynamoDB global tables and implement a pessimistic locking strategy.",
      "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
      "Use DynamoDB global tables and implement an optimistic locking strategy."
    ],
    "correct_answer": "Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",
    "explanation": "**Optimistic locking** is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others \u2014 and vice-versa. Take note that:\r\n\r\n\u2013 DynamoDB global tables use a \u201clast writer wins\u201d reconciliation between concurrent updates. If you use Global Tables, last writer policy wins. So in this case, the locking strategy does not work as expected.\r\n\r\n\u2013 DynamoDBMapper transactional operations do not support optimistic locking.\r\n\r\nWith optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes.\r\n\r\nHence, **implementing an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is the correct answer in this scenario.\r\n\r\n**Using DynamoDB global tables and implementing a pessimistic locking strategy** is incorrect because you have to use optimistic locking here just as what was explained above.\r\n\r\n**Implementing a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table** is incorrect because an optimistic locking strategy is a more suitable solution for this scenario. Although the provided steps here are correct, the name of the strategy is wrong.\r\n\r\n**Using DynamoDB global tables and implementing an optimistic locking strategy** is incorrect. Although it is correct to use the optimistic locking strategy, the use of DynamoDB global tables is wrong. This uses a *\u201clast writer wins\u201d* reconciliation between concurrent updates. If you use Global Tables, the last writer policy is in effect so in this case, the locking strategy will not work as expected.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html"
  },
  {
    "id": 33,
    "question": "A company has recently developed a containerized application that uses a multicontainer Docker platform which supports multiple containers per instance. They need a service that automatically handles tasks such as provisioning of the resources, load balancing, auto-scaling, monitoring, and placing the containers across the cluster.\r\n\r\nWhich of the following services provides the EASIEST way to accomplish the above requirement?",
    "options": [
      "Elastic Beanstalk",
      "EKS",
      "Lambda",
      "ECS"
    ],
    "correct_answer": "Elastic Beanstalk",
    "explanation": "You can create docker environments that support multiple containers per Amazon EC2 instance with multicontainer Docker platform for Elastic Beanstalk.\r\n\r\n**Elastic Beanstalk** uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multicontainer Docker environments. Amazon ECS provides tools to manage a cluster of instances running Docker containers. Elastic Beanstalk takes care of Amazon ECS tasks including cluster creation, task definition and execution.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/ba1c0749eacd4161a6e951f787301a23.png)\r\n\r\n**AWS Elastic Beanstalk** is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links.\r\n\r\nElastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures.\r\n\r\nHence, the correct answer in this scenario is **Elastic Beanstalk.**\r\n\r\n**ECS** is incorrect. Although it can host Docker applications, it doesn\u2019t automatically handle all the details such as resource provisioning, balancing load, auto-scaling, monitoring, and placing your containers across your cluster, unlike Elastic Beanstalk. Take note that even though you can use Service Auto Scaling in ECS, you still have to enable and configure it. Elastic Beanstalk still provides the easiest way to accomplish the requirements.\r\n\r\n**Lambda** is incorrect because this is primarily used for serverless applications and not for Docker or any other containerized applications.\r\n\r\n**EKS** is incorrect because Amazon EKS just provides you an easy way to run Kubernetes on AWS without needing to install and operate your own Kubernetes clusters.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\r\n\r\nhttps://aws.amazon.com/ecs/faqs/"
  },
  {
    "id": 34,
    "question": "A leading insurance firm is hosting its customer portal in Elastic Beanstalk, which has an RDS database in AWS. The support team in your company discovered a lot of SQL injection attempts and cross-site scripting attacks on the portal, which is starting to affect the production environment.\r\n\r\nWhich of the following services should you implement to mitigate this attack?",
    "options": [
      "AWS WAF",
      "Amazon Guard\u200bDuty",
      "Network Access Control List",
      "AWS Firewall Manager"
    ],
    "correct_answer": "AWS WAF",
    "explanation": "**AWS WAF** is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an Application Load Balancer responds to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/39fd073d4cfb4dd89e38274ccb4e79c6.png)\r\n\r\nAt the simplest level, AWS WAF lets you choose one of the following behaviors:\r\n\r\n**Allow all requests except the ones that you specify** \u2013 This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers.\r\n\r\n**Block all requests except the ones that you specify** \u2013 This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website.\r\n\r\n**Count the requests that match the properties that you specify** \u2013 When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn\u2019t accidentally configure AWS WAF to block all the traffic to your website. When you\u2019re confident that you specified the correct properties, you can change the behavior to allow or block requests.\r\n\r\nHence, the correct answer in this scenario is **AWS WAF.**\r\n\r\n**Amazon Guard\u200bDuty** is incorrect because this is just a threat detection service that continuously monitors malicious activity and unauthorized behavior to protect your AWS accounts and workloads.\r\n\r\n**AWS Firewall Manager** is incorrect because this just simplifies your AWS WAF and AWS Shield Advanced administration and maintenance tasks across multiple accounts and resources.\r\n\r\n**Network Access Control List** is incorrect because this is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/waf/\r\n\r\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\r\n\r\nhttps://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/"
  },
  {
    "id": 35,
    "question": "A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games.\r\n\r\nYou have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases:\r\n\r\nFor a given name and version number, get all details about the game that has that name and version number.\r\n\r\nFor a given name, get all details about all games that have that name.\r\n\r\nFor a given category, get all details about all games in that category.\r\n\r\nWhat will you recommend as the most efficient solution?",
    "options": [
      "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
      "Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key",
      "Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance",
      "Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key"
    ],
    "correct_answer": "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
    "explanation": "**Correct option**:\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key**\r\n\r\nWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.\r\n\r\n**Incorrect options:**\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key** - The DynamoDB table for this option has the primary key and GSI that do not solve for the condition - \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multi**Correct option**:\r\n\r\n**Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key**\r\n\r\nWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.ple queries which would be less efficient than the single query allowed by the correct option.\r\n\r\n**Set up an Amazon RDS MySQL instance having a `games` table that contains columns for name, version number, and category. Configure the name column as the primary key** - This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.\r\n\r\n**Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance** - You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/primary-key-dynamodb-table/"
  },
  {
    "id": 36,
    "question": "You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.\r\n\r\nWhen creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?",
    "options": [
      ".config_<mysettings>.ebextensions",
      ".ebextensions_<mysettings>.config",
      ".ebextensions/<mysettings>.config",
      ".config/<mysettings>.ebextensions"
    ],
    "correct_answer": ".ebextensions/<mysettings>.config",
    "explanation": "**Correct option:**\r\n\r\n**`.ebextensions/<mysettings>.config`** : You can add AWS Elastic Beanstalk configuration files (`.ebextensions`) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.\r\n\r\n**Incorrect options:**\r\n\r\n**`.ebextensions_<mysettings>.config`**\r\n\r\n**`.config/<mysettings>.ebextensions`**\r\n\r\n**`.config_<mysettings>.ebextensions`**\r\n\r\nThese three options contradict the explanation provided earlier. So these are incorrect.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\r\n\r\n&nbsp;"
  },
  {
    "id": 37,
    "question": "You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).\r\n\r\nWhich AWS service will help you with token handling and management?",
    "options": [
      "Cognito Identity Pools",
      "Cognito Sync",
      "API Gateway",
      "Cognito User Pools"
    ],
    "correct_answer": "Cognito User Pools",
    "explanation": "**Correct option:**\r\n\r\n\"Cognito User Pools\"\r\n\r\nAfter successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.\r\n\r\nAmazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.\r\n\r\nThe ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.\r\n\r\n**Incorrect options:**\r\n\r\n\"API Gateway\" - If you are processing tokens server-side and using other programming languages not supported in AWS it may be a good choice. Other than that, go with a service already providing the functionality.\r\n\r\n\"Cognito Identity Pools\" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.\r\n\r\n\"Cognito Sync\" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend."
  },
  {
    "id": 38,
    "question": "A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.\r\n\r\nAs a Developer Associate, which AWS service would you recommend for the given use-case?",
    "options": [
      "CloudFormation",
      "CodeDeploy",
      "Serverless Application Model (SAM)",
      "Elastic Beanstalk"
    ],
    "correct_answer": "Elastic Beanstalk",
    "explanation": "**Correct option:**\r\n\r\n**Elastic Beanstalk**\r\n\r\nAWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.\r\n\r\nAWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.\r\n\r\n&nbsp;\r\n\r\n**Incorrect options:**\r\n\r\n**CloudFormation** - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.\r\n\r\n**CodeDeploy** - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.\r\n\r\n**Serverless Application Model** - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\r\n\r\nhttps://aws.amazon.com/cloudformation/"
  },
  {
    "id": 39,
    "question": "You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week.\r\n\r\nWhat should you use?",
    "options": [
      "Use DynamoDB Streams",
      "Use TTL",
      "Use DynamoDB Streams",
      "Use DAX"
    ],
    "correct_answer": "Use TTL",
    "explanation": "**Correct option:**\r\n\r\n**Use TTL**\r\n\r\nTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.\r\n\r\n**Incorrect options:**\r\n\r\n**Use DynamoDB Streams** - These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.\r\n\r\n**Use DAX** - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.\r\n\r\n**Use a Lambda function** - This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This band-aid solution would never be as good as using the TTL feature in DynamoDB.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
  },
  {
    "id": 40,
    "question": "A retail company is migrating its on-premises database to Amazon RDS for PostgreSQL. The company has read-heavy workloads. The development team at the company is looking at refactoring the code to achieve optimum read performance for SQL queries.\r\n\r\nWhich solution will address this requirement with the least current as well as future development effort?",
    "options": [
      "Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint",
      "Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint",
      "Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint",
      "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas"
    ],
    "correct_answer": "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas",
    "explanation": "**Correct option:**\r\n\r\n**Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas**\r\n\r\nAmazon RDS uses the PostgreSQL DB engine's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the given use case, you can achieve optimum read performance for SQL queries by using the read-replica endpoint for the read-heavy workload.\r\n\r\n**Incorrect options:**\r\n\r\n**Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint**\r\n\r\n**Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint**\r\n\r\nBoth Redis and Memcached are popular, open-source, in-memory data stores (also known as in-memory caches). These are not relational databases and cannot be used to run SQL queries. So, both these options are incorrect.\r\n\r\n**Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint** - In an Amazon RDS Multi-AZ deployment with a single standby instance, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention. You cannot route the read queries from an application to the standby instance of a multi-AZ RDS database as it's not accessible for the read traffic in the single standby instance configuration.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/\r\n\r\nhttps://aws.amazon.com/rds/features/multi-az/\r\n\r\nhttps://aws.amazon.com/blogs/database/readable-standby-instances-in-amazon-rds-multi-az-deployments-a-new-high-availability-option/"
  },
  {
    "id": 41,
    "question": "You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.\r\n\r\nWhich of the following options represents the best solution to address this use-case?",
    "options": [
      "Use VPC Flow Logs",
      "Use AWS CloudWatch Events",
      "Use AWS Lambda",
      "Enable CodeBuild timeouts"
    ],
    "correct_answer": "Enable CodeBuild timeouts",
    "explanation": "**Correct option:**\r\n\r\n**Enable CodeBuild timeouts**\r\n\r\nA build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).\r\n\r\nThe following rules apply when you run multiple builds:\r\n\r\nWhen possible, builds run concurrently. The maximum number of concurrently running builds can vary.\r\n\r\nBuilds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.\r\n\r\nA build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.\r\n\r\nBy setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.\r\n\r\nIncorrect options:\r\n\r\n**Use AWS Lambda** - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.\r\n\r\n**Use AWS CloudWatch Events** - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.\r\n\r\n**Use VPC Flow Logs** - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html"
  },
  {
    "id": 42,
    "question": "An application running on an EC2 instance regularly fetches large amounts of data from multiple S3 buckets. A data analysis team will perform ad-hoc queries on the data. To reduce costs and optimize the process, the application requires a solution that can perform serverless queries directly on the data stored in S3 without the need to load it into a database first.\r\n\r\nWhich is the MOST suitable service that will help accomplish this requirement?",
    "options": [
      "Amazon Athena",
      "AWS Step Functions",
      "Amazon EMR",
      "Amazon Redshift Spectrum"
    ],
    "correct_answer": "Amazon Athena",
    "explanation": "**Correct**\r\n\r\n**Amazon Athena** is a serverless query service which enables fast analysis of data stored in Amazon S3 using standard SQL. With minimal configuration through the AWS Management Console, Athena allows you to run ad-hoc SQL queries on S3 data and obtain results in seconds. It supports various file formats, including JSON, CSV, ORC, and Parquet. Athena is ideal for ad-hoc queries and operates on a pay-per-query pricing model, making it highly cost-effective for analyzing large datasets without the need to manage any infrastructure.\r\n\r\n**Athena** directly addresses the need for querying data in S3 without moving it to a database, which significantly lowers costs and optimizes data analysis processes.\r\n\r\nHence, the correct answer is: **Amazon Athena.**\r\n\r\nThe option that says: **Amazon EMR** is incorrect because this service is a managed Hadoop framework that helps process large datasets using tools like Apache Spark and Hive. While it can analyze data in S3, it requires setting up clusters, adding infrastructure management, and is not as serverless or cost-efficient as Athena for ad-hoc SQL queries.\r\n\r\nThe option that says: **Amazon Redshift Spectrum** is incorrect. Although it allows querying S3 data, it requires a Redshift cluster and involves additional setup and cost, making it less ideal for serverless ad-hoc querying.\r\n\r\nThe option that says: **AWS Step Functions** is incorrect because this service only lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. It doesn\u2019t provide a function to do an in-place query to an S3 bucket.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html\r\n\r\nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html"
  },
  {
    "id": 43,
    "question": "A developer is building an AI-based traffic monitoring application using Lambda in AWS. Due to the complexity of the application, the developer must do certain modifications such as the way Lambda runs the function\u2019s setup code and how the invocation events are read from the Lambda runtime API.\r\n\r\nIn this scenario, which feature of Lambda should you take advantage of to meet the above requirement?",
    "options": [
      "Layers",
      "Custom Runtime",
      "DLQ",
      "Lambda@Edge"
    ],
    "correct_answer": "Custom Runtime",
    "explanation": "You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function\u2019s handler method when the function is invoked. You can include a runtime in your function\u2019s deployment package in the form of an executable file named `bootstrap`.\r\n\r\nA runtime is responsible for running the function\u2019s setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler and posts the response from the handler back to Lambda.\r\n\r\nYour custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that\u2019s included in Amazon Linux, or a binary executable file that\u2019s compiled in Amazon Linux.\r\n\r\nTake note that Lambda has a deployment package size limit of 50 MB for direct upload (zipped file) and 250 MB for layers (unzipped).\r\n\r\nHence, the correct answer in this scenario is: **Custom Runtime.**\r\n\r\n**Layers** is incorrect because this just enables you to use libraries and other dependencies in your function without having to include them in your deployment package.\r\n\r\n**Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/)** is incorrect because this is actually a feature of Amazon CloudFront and not Lambda, which lets you run code closer to users of your application to improve performance and reduce latency.\r\n\r\n**DLQ** is incorrect because the Dead Letter Queue (DLQ) only directs unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure if the retries fail. This feature does not meet the required capabilities mentioned in the scenario.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html"
  },
  {
    "id": 44,
    "question": "You are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should also allow multiple users to synchronize and collaborate shared data in real time.\r\n\r\nWhich of the following is the MOST appropriate solution that you should implement in this scenario?",
    "options": [
      "Integrate Amazon Cognito Sync to your mobile app.",
      "Integrate Amazon Pinpoint to your mobile app.",
      "Integrate AWS AppSync to your mobile app.",
      "Integrate AWS Amplify to your mobile app."
    ],
    "correct_answer": "Integrate AWS AppSync to your mobile app.",
    "explanation": "**AWS AppSync** simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.\r\n\r\nWith AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.\r\n\r\nAWS AppSync is quite similar with Amazon Cognito Sync which is also a service for synchronizing application data across devices. It enables user data like app preferences or game state to be synchronized as well however, the key difference is that, it also extends these capabilities by allowing multiple users to synchronize and collaborate in real time on shared data.\r\n\r\nHence, the correct answer is to **integrate AWS AppSync to your mobile app**.\r\n\r\n**Integrating AWS Amplify to your mobile app** is incorrect because this service just makes it easy to create, configure, and implement scalable mobile and web apps powered by AWS. This is primarily used to automate the application release process of both your frontend and backend allowing you to deliver features faster, and not for synchronizing application data across devices.\r\n\r\n**Integrating Amazon Cognito Sync to your mobile app** is incorrect. Although this service can also be used in synchronizing application data across devices, it does not allow multiple users to synchronize and collaborate in real-time on shared data, unlike AWS AppSync.\r\n\r\n**Integrating Amazon Pinpoint to your mobile app** is incorrect because this service simply allows you to engage with your customers across multiple messaging channels. This is primarily used to send push notifications, emails, SMS text messages, and voice messages."
  },
  {
    "id": 45,
    "question": "A media company seeks to protect its copyrighted images from unauthorized distribution. They want images uploaded to their Amazon S3 bucket to be automatically watermarked. A developer has already prepared the Lambda function for this image-processing job.\r\n\r\nWhich option must the developer configure to automatically invoke the function at each upload?",
    "options": [
      "Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation.",
      "Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket.",
      "Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users.",
      "Configure an S3 Lifecycle policy to transition images to the INTELLIGENT_TIERING storage class. Use S3 Inventory to generate a report of images that weren\u2019t watermarked and set up the Lambda function to process the report."
    ],
    "correct_answer": "Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket.",
    "explanation": "You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, add a notification configuration that identifies the events that you want Amazon S3 to publish. Make sure that it also identifies the destinations where you want Amazon S3 to send the notifications.\r\n\r\nAmazon S3 can send event notification messages to the following destinations:\r\n\r\n\u2013 Amazon SQS queue\r\n\r\n\u2013 AWS Lambda function\r\n\r\n\u2013 Amazon SNS topic\r\n\r\n\u2013 Amazon EventBridge\r\n\r\nIn the given scenario, you can set up a notification for the `ObjectCreated:Put` event to immediately trigger a Lambda function when an object is uploaded to the S3 bucket.\r\n\r\nHence, the correct answer is: **Set up an Amazon S3 Event Notification to trigger the Lambda function when an `ObjectCreated:Put` event is detected in the bucket.**\r\n\r\nThe option that says: **Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation** is incorrect. S3 Storage Lens just provide visibility into storage usage and activity trends. It does not trigger actions or Lambda functions based on object operations.\r\n\r\nThe option that says: **Configure an S3 Lifecycle policy to transition images to the `INTELLIGENT_TIERING` storage class. Use S3 Inventory to generate a report of images that weren\u2019t watermarked and set up the Lambda function to process the report** is incorrect. S3 Lifecycle policies simply manage storage transitions and object expirations, not event-driven actions like invoking Lambda functions upon uploads. Moreover, S3 Inventory just provides object lists and their metadata, but it doesn\u2019t automatically invoke Lambda functions upon image uploads.\r\n\r\nThe option that says: **Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users** is incorrect because S3 Object Lambda is primarily designed to transform objects at retrieval, not at upload. While it can dynamically apply watermarks, it does so when the object is accessed, not as part of the upload process, which would lead to watermarking every time the image is retrieved rather than just once upon upload.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-event-notifications.html"
  },
  {
    "id": 46,
    "question": "A developer has recently deployed an application, which is hosted in an Auto Scaling group of EC2 instances and processes data from an Amazon Kinesis Data Stream. Each of the EC2 instances has exactly one KCL worker processing one Kinesis data stream which has 10 shards. Due to performance issues, the systems operations team has resharded the data stream to increase the number of open shards to 20.\r\n\r\nWhat is the maximum number of running EC2 instances that should ideally be kept to maintain application performance?",
    "options": [
      "10",
      "30",
      "40",
      "20"
    ],
    "correct_answer": "20",
    "explanation": "Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nTo scale up processing in your application, you should test a combination of these approaches:\r\n\r\n\u2013 Increasing the instance size (because all record processors run in parallel within a process)\r\n\r\n\u2013 Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently)\r\n\r\n\u2013 Increasing the number of shards (which increases the level of parallelism)\r\n\r\nThus, the maximum number of instances you can launch is **20**, to match the number of open shards with a ratio of 1:1.\r\n\r\nAlthough you can launch **10** instances in which each instance handles 2 shards, this is not the maximum number of instances you can deploy for your application. Hence, this option is incorrect. Take note that the maximum number of your instances is not half the number of open shards.\r\n\r\nLaunching **30** or **40** instances is incorrect because you should ensure that the number of instances does not exceed the number of open shards. The maximum number of instances that you should deploy is 20.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html"
  },
  {
    "id": 47,
    "question": "A developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is the MOST secure way to achieve this?",
    "options": [
      "Move your RDS instance to a public subnet.",
      "Ensure that the Lambda function has proper IAM permission to access RDS.",
      "Configure the Lambda function to connect to your VPC.",
      "Expose an endpoint of your RDS to the Internet using an Elastic IP."
    ],
    "correct_answer": "Configure the Lambda function to connect to your VPC.",
    "explanation": "You can configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources during execution.\r\n\r\nAWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces [(ENIs)](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_ElasticNetworkInterfaces.html) that enable your function to connect securely to other resources within your private VPC.\r\n\r\nDon\u2019t put your Lambda function in a VPC unless you have to. There is no benefit outside of using this to access resources you cannot expose publicly, like a private Amazon Relational Database instance. Services like Amazon OpenSearch Service can be secured over IAM with access policies, so exposing the endpoint publicly is safe and wouldn\u2019t require you to run your function in the VPC to secure it.\r\n\r\nHence, **configuring the Lambda function to connect to your VPC** is the correct answer for this scenario.\r\n\r\n**Ensuring that the Lambda function has proper IAM permission to access RDS** is incorrect. Even though you grant the necessary IAM permissions to the Lambda function to access RDS, the function would still not be able to connect to RDS since there is no established connection between Lambda and the private subnet of your VPC.\r\n\r\n**Exposing an endpoint of your RDS to the Internet using an Elastic IP** is incorrect because this is not the most secure way of granting access to your Lambda function. It will be able to connect to RDS but so will the billions of people on the public Internet.\r\n\r\n**Moving your RDS instance to a public subnet** is incorrect because this is an unnecessary change and not a best practice from a security perspective. You only need to configure your Lambda function to your VPC so it can connect to the RDS in the private subnet. If you move your RDS instance to a public subnet, it will introduce a critical security flaw to your entire architecture since your database will become accessible publicly."
  },
  {
    "id": 49,
    "question": "A company has assigned a developer to automate its department\u2019s patch management, data synchronization, and other recurring tasks. The developer needs a service to coordinate multiple AWS services into serverless workflows.\r\n\r\nWhich of the following is the MOST cost-effective service the developer should implement in this scenario?",
    "options": [
      "AWS Batch",
      "AWS Elastic Beanstalk",
      "AWS Step Functions",
      "AWS Lambda"
    ],
    "correct_answer": "AWS Step Functions",
    "explanation": "**AWS Step Functions** provides serverless orchestration for modern applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintain the application state, tracking exactly which workflow step your application is in, and store an event log of data that is passed between application components. That means that if networks fail or components hang, your application can pick up right where it left off.\r\n\r\nApplication development is faster and more intuitive with Step Functions because you can define and manage the workflow of your application independently from its business logic. Making changes to one does not affect the other. You can easily update and modify workflows in one place, without having to struggle with managing, monitoring, and maintaining multiple point-to-point integrations. Step Functions frees your functions and containers from excess code, so your applications are faster to write, more resilient, and easier to maintain.\r\n\r\nHence, the correct answer is: **AWS Step Functions.**\r\n\r\n**AWS Elastic Beanstalk** is incorrect because this service is for deploying and scaling web applications and services. However, it\u2019s not designed to coordinate multiple AWS services into serverless workflows.\r\n\r\n**Lambda** is incorrect. Although it is typically used for serverless computing, it does not provide a direct way to coordinate multiple AWS services into serverless workflows.\r\n\r\n**AWS Batch** is incorrect because it is primarily used to efficiently run hundreds of thousands of batch computing jobs in AWS.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/step-functions/features/\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
  },
  {
    "id": 50,
    "question": "A developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a SAM template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS.\r\n\r\nWhich combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)",
    "options": [
      "Deploy the SAM template from AWS CodePipeline.",
      "Build the SAM template in the local environment",
      "Build the SAM template using the AWS SDK for AWS CodeDeploy.",
      "Deploy the SAM template from an Amazon S3 bucket.",
      "Build the SAM template in an Amazon EC2 instance.",
      "Package the SAM application for deployment."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Build the SAM template in the local environment",
      "Deploy the SAM template from an Amazon S3 bucket.",
      "Package the SAM application for deployment."
    ],
    "correct_answer": "Build the SAM template in the local environment",
    "explanation": "**AWS SAM** uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.\r\n\r\nThe typical AWS SAM deployment workflow starts with the `sam build` command, which compiles source code and readies deployment artifacts. Once built for deployment, the SAM template and the associated artifacts need to be stored in an S3 bucket. The `sam deploy` command takes care of this by first uploading the CloudFormation template to the S3 bucket. Though historically, the `sam package` command was used for this purpose, it\u2019s become somewhat legacy, as `sam deploy` , now implicitly handles the packaging. Once the template is in the S3 bucket, AWS CloudFormation references it to create or update the defined resources.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Build the SAM template in the local environment**\r\n\r\n**\u2013 Package the SAM application for deployment.**\r\n\r\n**\u2013 Deploy the SAM template from an Amazon S3 bucket.**\r\n\r\nThe option that says: **Deploy the SAM template from AWS CodePipeline** is incorrect. AWS CodePipeline is primarily a continuous integration and continuous delivery (CI/CD) service that automates the build, test, and deploy phases of your release process. While CodePipeline can deploy SAM applications, it is not a required step for a local SAM deployment workflow.\r\n\r\nThe option that says: **Build the SAM template using the AWS SDK for AWS CodeDeploy** is incorrect. The AWS SDK for CodeDeploy is typically used for management operations of the CodeDeploy service, not for building SAM templates. Building the SAM application is a separate process, typically done using the SAM CLI.\r\n\r\nThe option that says: **Build the SAM template in an Amazon EC2 instance** is incorrect. This option is unnecessary. While you can technically build on an EC2 instance, it\u2019s not a requirement for SAM deployment. In the scenario, there\u2019s no condition that warrants the use of an EC2 instance.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\r\n\r\nhttps://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html"
  },
  {
    "id": 51,
    "question": "The company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda application. It is your responsibility to ensure that application works effectively in AWS.\r\n\r\nWhich of the following are the best practices in developing Lambda functions? (Select TWO.)",
    "options": [
      "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
      "Take advantage of Execution Context reuse to improve the performance of your function.",
      "Use Amazon Inspector for troubleshooting.",
      "Include the core logic in the Lambda handler.",
      "Use recursive code."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
      "Take advantage of Execution Context reuse to improve the performance of your function."
    ],
    "correct_answer": "Use AWS Lambda Environment Variables to pass operational parameters to your function.",
    "explanation": "Below are some of the best practices in working with AWS Lambda Functions:\r\n\r\n\u2013 Separate the Lambda handler (entry point) from your core logic.\r\n\r\n\u2013 Take advantage of Execution Context reuse to improve the performance of your function\r\n\r\n\u2013 Use AWS Lambda Environment Variables to pass operational parameters to your function.\r\n\r\n\u2013 Control the dependencies in your function\u2019s deployment package.\r\n\r\n\u2013 Minimize your deployment package size to its runtime necessities.\r\n\r\n\u2013 Reduce the time it takes Lambda to unpack deployment packages\r\n\r\n\u2013 Minimize the complexity of your dependencies\r\n\r\n\u2013 Avoid using recursive code\r\n\r\nHence, the correct answers in this scenario are:\r\n\r\n**\u2013 Take advantage of Execution Context reuse to improve the performance of your function**\r\n\r\n**\u2013 Use AWS Lambda Environment Variables to pass operational parameters to your function**\r\n\r\n**Using recursive code** is incorrect because this is a situation wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs.\r\n\r\n**Including the core logic in the Lambda handler** is incorrect because you have to separate the Lambda handler (entry point) from your core logic instead.\r\n\r\n**Using Amazon Inspector for troubleshooting** is incorrect because this service is primarily used for EC2 and not for Lambda. You have to use X-Ray instead of troubleshooting your functions.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html"
  },
  {
    "id": 52,
    "question": "A software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller\u2019s identity.\r\n\r\nWhich of the features of API Gateway is the MOST suitable one that she should use to build this feature?",
    "options": [
      "Lambda Authorizers",
      "Cross-Account Lambda Authorizer",
      "Cross-Origin Resource Sharing (CORS)",
      "Resource Policy"
    ],
    "question_type": "single",
    "correct_answers": [
      "Lambda Authorizers"
    ],
    "correct_answer": "Lambda Authorizers",
    "explanation": "A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API\u2019s methods, API Gateway calls your Lambda authorizer, which takes the caller\u2019s identity as input and returns an IAM policy as output.\r\n\r\nThere are two types of Lambda authorizers:\r\n\r\n\u2013 A **token-based** Lambda authorizer (also called a TOKEN authorizer) receives the caller\u2019s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.\r\n\r\n\u2013 A **request parameter-based** Lambda authorizer (also called a REQUEST authorizer) receives the caller\u2019s identity in a combination of headers, query string parameters, stageVariables, and $context variables.\r\n\r\nIt is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function by using a Cross-Account Lambda Authorizer.\r\n\r\nTherefore, the correct answer in this scenario is to use **Lambda Authorizers** since this feature is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML.\r\n\r\n**Resource Policy** is incorrect because this is simply a JSON policy document that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. This can\u2019t be used to implement a custom authorization scheme.\r\n\r\n**Cross-Origin Resource Sharing (CORS)** is incorrect because this just defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.\r\n\r\n**Cross-Account Lambda Authorizer** is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html"
  },
  {
    "id": 53,
    "question": "You are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20 eventually consistent reads per second where all the items have a size of 2 KB for both operations.\r\n\r\nWhich of the following are the most optimal WCU and RCU that you should provision to the table?",
    "options": [
      "40 RCU and 20 WCU",
      "40 RCU and 40 WCU",
      "20 RCU and 20 WCU",
      "10 RCU and 20 WCU"
    ],
    "question_type": "single",
    "correct_answers": [
      "10 RCU and 20 WCU"
    ],
    "correct_answer": "10 RCU and 20 WCU",
    "explanation": "When you create a new provisioned table in DynamoDB, you must specify its *provisioned throughput capacity*\u2014the amount of read and write activity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput requirements.\r\n\r\nYou can optionally allow DynamoDB auto-scaling to manage your table\u2019s throughput capacity. However, you still must provide initial settings for read and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them dynamically in response to your application\u2019s requirements. You specify throughput requirements in terms of capacity units\u2014the amount of data your application needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify them automatically.\r\n\r\n**1 WCU** can do **1 write per second** for an item up to **1KB.** To get the required WCU, simply multiply the given average item size by the required writes per second. In the scenario, the DynamoDB table is expected to perform 10 writes per second of a 2KB item. Multiplying 10 by 2 gives **20 WCU**.\r\n\r\n**1 RCU** can do **1 strongly consistent read** or **2 eventually consistent reads** for an item up to **4KB**.\r\n\r\n**To get the RCU with eventually consistent reads, do the following steps:**\r\n\r\n**Step #1 Divide the average item size by 4 KB. Round up the result**\r\n\r\nAverage Item Size = 2 KB\r\n\r\n= **2KB/4KB**\r\n\r\n**= 0.5 \u2248 1**\r\n\r\n**Step #2 Multiply the number of reads per second by the resulting value from Step 1. Divide the product by 2 for eventually consistent reads.**\r\n\r\n= **20** reads per second **x** **1**\r\n\r\n= **20** RCU\r\n\r\nSince the type of read being asked is eventually consistent, we get half of 20, which is 10.\r\n\r\n= 20/2 = **10 RCU**\r\n\r\nHence, the correct answer is to provision **10 RCU and 20 WCU** to your DynamoDB table.\r\n\r\nThe **20 RCU and 20 WCU** setting is incorrect because this would be the result if you use strong consistency reads. Remember that the scenario explicitly said that eventual consistency reads would be used.\r\n\r\nThe **40 RCU and 20 WCU** is incorrect because 40 RCU is overkill for the required eventual consistency reads. If the scenario was asking for transactional read requests, then this option could have been correct.\r\n\r\nThe **40 RCU and 40 WCU** setting is incorrect because this would be the result if you chose transactional requests both on your reads and writes. Take note that the scenario didn\u2019t say that the database is using DynamoDB Transactions.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"
  },
  {
    "id": 54,
    "question": "An organization is selling memorabilia that is illegal in specific countries. How can a developer restrict access to the website to countries where the memorabilia are illegal?",
    "options": [
      "Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification.",
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification.",
      "Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access.",
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access."
    ],
    "correct_answer": "Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access.",
    "explanation": "**Overall explanation:**\r\nAWS WAF can be used to set up a WEB ACL that can be used to block statements that originate from a specific country.\r\n\r\n**CORRECT:** \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access\" is incorrect. AWS Shield is used to protect from DDoS attacks.\r\n\r\n**INCORRECT:** \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.\r\n\r\n**INCORRECT:** \" Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.\r\n\r\nReferences:\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/"
  },
  {
    "id": 55,
    "question": "A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found.\r\n\r\nWhat is the BEST explanation for the duplicate entries?",
    "options": [
      "The Lambda function failed, and the Lambda service retried the invocation with a delay",
      "The S3 bucket name was specified incorrectly",
      "The application stopped intermittently and then resumed",
      "There was an S3 outage, which caused duplicate entries of the same log file"
    ],
    "question_type": "single",
    "correct_answers": [
      "The Lambda function failed, and the Lambda service retried the invocation with a delay"
    ],
    "correct_answer": "The Lambda function failed, and the Lambda service retried the invocation with a delay",
    "explanation": "**Overall explanation\r\nFrom the AWS documentation:**\r\n\r\n\u201cWhen an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.\r\n\r\nFor asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a dead-letter queue.\u201d\r\n\r\nTherefore, the most likely explanation is that the function failed, and Lambda retried the invocation.\r\n\r\n**CORRECT:** \"The Lambda function failed, and the Lambda service retried the invocation with a delay\" is the correct answer.\r\n\r\n**INCORRECT:** \"The S3 bucket name was specified incorrectly\" is incorrect. If this was the case all attempts would fail but this is not the case.\r\n\r\n**INCORRECT:** \"There was an S3 outage, which caused duplicate entries of the same log file\" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have a different request ID.\r\n\r\n**INCORRECT:** \"The application stopped intermittently and then resumed\" is incorrect. The issue is duplicate entries of the same request ID.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html"
  },
  {
    "id": 56,
    "question": "A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Linear",
      "In-place",
      "Blue/green",
      "Canary"
    ],
    "question_type": "single",
    "correct_answers": [
      "Blue/green"
    ],
    "correct_answer": "Blue/green",
    "explanation": "**Overall explanation**\r\n**CodeDeploy** provides two deployment type options \u2013 in-place and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.\r\n\r\nThe Blue/green deployment type on an Amazon ECS compute platform works like this:\r\n\r\nTraffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.\r\n\r\nYou can set the traffic shifting to linear or canary through the deployment configuration.\r\n\r\nThe protocol and port of a specified load balancer listener is used to reroute production traffic.\r\n\r\nDuring a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.\r\n\r\n**CORRECT:** \"Blue/green\" is the correct answer.\r\n\r\n**INCORRECT:** \"Canary\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.\r\n\r\n**INCORRECT:** \"Linear\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.\r\n\r\n**INCORRECT:** \"In-place\" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"
  },
  {
    "id": 57,
    "question": "A developer has deployed an application on AWS Lambda. The application uses Python and must generate and then upload a file to an Amazon S3 bucket. The developer must implement the upload functionality with the least possible change to the application code.\r\n\r\nWhich solution BEST meets these requirements?",
    "options": [
      "Make an HTTP request directly to the S3 API to upload the file.",
      "Include the AWS SDK for Python in the Lambda function code.",
      "Use the AWS CLI that is installed in the Lambda execution environment.",
      "Use the AWS SDK for Python that is installed in the Lambda execution environment."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use the AWS SDK for Python that is installed in the Lambda execution environment."
    ],
    "correct_answer": "Use the AWS SDK for Python that is installed in the Lambda execution environment.",
    "explanation": "**Overall explanation**\r\nThe best practice for Lambda development is to bundle all dependencies used by your Lambda function, including the AWS SDK. However, since this question specifically requests that the least possible changes are made to the application code, the developer can instead use the SDK for Python that is installed in the Lambda environment to upload the file to Amazon S3.\r\n\r\n**CORRECT:** \"Use the AWS SDK for Python that is installed in the Lambda execution environment\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Include the AWS SDK for Python in the Lambda function code\" is incorrect.\r\n\r\nThis is the best practice for deployment. However, in this case the developer must minimize changes to code and including the SDK as a dependency in the code would require potential updates to existing Python code.\r\n\r\n**INCORRECT:** \"Make an HTTP request directly to the S3 API to upload the file\" is incorrect.\r\n\r\nAWS supports uploads to S3 using the console, AWS SDKs, REST API, and the AWS CLI.\r\n\r\n**INCORRECT:** \"Use the AWS CLI that is installed in the Lambda execution environment\" is incorrect.\r\n\r\nThe AWS CLI is not installed in the Lambda execution environment.\r\n\r\nReferences:\r\n\r\nhttps://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/"
  },
  {
    "id": 58,
    "question": "A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit.\r\nWhat needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?\r\n\r\n",
    "options": [
      "A public and private SSH key file",
      "An Amazon EC2 IAM role with CodeCommit permissions",
      "A set of Git credentials generated with IAM",
      "A GitHub secure authentication token"
    ],
    "question_type": "single",
    "correct_answers": [
      "A set of Git credentials generated with IAM"
    ],
    "correct_answer": "A set of Git credentials generated with IAM",
    "explanation": "**Overall explanation**\r\nAWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:\r\n\r\nGit credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.\r\n\r\nSSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.\r\n\r\nAWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.\r\n\r\nIn this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAM\r\n\r\n**CORRECT:** \"A set of Git credentials generated with IAM\" is the correct answer.\r\n\r\n**INCORRECT:** \"A GitHub secure authentication token\" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub (they have already accessed and cloned the repository).\r\n\r\n**INCORRECT:** \"A public and private SSH key file\" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.\r\n\r\n**INCORRECT:** \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html"
  },
  {
    "id": 59,
    "question": "A Developer needs to scan a full DynamoDB 50GB table within non-peak hours. About half of the strongly consistent RCUs are typically used during non-peak hours and the scan duration must be minimized.\r\n\r\nHow can the Developer optimize the scan execution time without impacting production workloads?",
    "options": [
      "Use sequential scans",
      "Use parallel scans while limiting the rate",
      "Change to eventually consistent RCUs during the scan operation",
      "Increase the RCUs during the scan operation"
    ],
    "question_type": "single",
    "correct_answers": [
      "Use parallel scans while limiting the rate"
    ],
    "correct_answer": "Use parallel scans while limiting the rate",
    "explanation": "**Overall explanation**\r\nPerforming a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn\u2019t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.\r\n\r\nFirstly, the Limit parameter can be used to reduce the page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.\r\n\r\nSecondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table\u2019s partitions.\r\n\r\nA parallel scan can be the right choice if the following conditions are met:\r\n\r\nThe table size is 20 GB or larger.\r\n\r\nThe table's provisioned read throughput is not being fully used.\r\n\r\nSequential Scan operations are too slow.\r\n\r\nTherefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.\r\n\r\n**CORRECT:** \"Use parallel scans while limiting the rate\" is the correct answer.\r\n\r\n**INCORRECT:** \"Use sequential scans\" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.\r\n\r\n**INCORRECT:** \"Increase the RCUs during the scan operation\" is incorrect as the table is only using half of the RCUs during non-peak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.\r\n\r\n**INCORRECT:** \"Change to eventually consistent RCUs during the scan operation\" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan"
  },
  {
    "id": 60,
    "question": "A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80.\r\n\r\nWhat is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "Specify port 80 for the container port and port 0 for the host port",
      "Leave both the container port and host port configuration blank",
      "Specify port 80 for the container port and a unique port number for the host port",
      "Specify a unique port number for the container port and port 80 for the host port"
    ],
    "question_type": "single",
    "correct_answers": [
      "Specify port 80 for the container port and port 0 for the host port"
    ],
    "correct_answer": "Specify port 80 for the container port and port 0 for the host port",
    "explanation": "**Overall explanation**\r\nPort mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the user-specified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.\r\n\r\n\r\n\r\nAs we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.\r\n\r\nCORRECT: \"Specify port 80 for the container port and port 0 for the host port\" is the correct answer.\r\n\r\nINCORRECT: \"Specify port 80 for the container port and a unique port number for the host port\" is incorrect as this is more difficult to manage as you have to manually assign the port number.\r\n\r\nINCORRECT: \"Specify a unique port number for the container port and port 80 for the host port\" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.\r\n\r\nINCORRECT: \"Leave both the container port and host port configuration blank\" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.\r\n\r\nReferences:\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html"
  },
  {
    "id": 61,
    "question": "A developer is updating an Amazon Aurora MySQL database to allow more clients to connect. What database parameter needs to be updated to support a higher number of client connections?\r\n\r\n\r\n\r\n\r\n\r\n",
    "options": [
      "max_connections",
      "max_join_size",
      "max_user_connections",
      "max_allowed_packet"
    ],
    "question_type": "single",
    "correct_answers": [
      "max_connections"
    ],
    "correct_answer": "max_connections",
    "explanation": "The maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.\r\n\r\nYou can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\r\n\r\n**CORRECT:** \"max_connections\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"max_allowed_packet\" is incorrect. This parameter sets the maximum size of one packet or any generated or intermediate string.\r\n\r\n**INCORRECT:** \"max_join_size\" is incorrect. This option is used to set a limit on the maximum number of row accesses.\r\n\r\n**INCORRECT:** \"max_user_connections\" is incorrect. This option limits the number of simultaneous connections that the user can make.\r\n\r\n**References**:\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/"
  },
  {
    "id": 62,
    "question": "An application asynchronously invokes an AWS Lambda function. The application has recently been experiencing occasional errors that result in failed invocations. A developer wants to store the messages that resulted in failed invocations such that the application can automatically retry processing them.\r\n\r\nWhat should the developer do to accomplish this goal with the LEAST operational overhead?",
    "options": [
      "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.",
      "Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again.",
      "Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group.",
      "Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function."
    ],
    "correct_answer": "Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.",
    "explanation": "Overall explanation\r\n\r\nAmazon SQS supports *dead-letter queues* (DLQ), which other queues (*source queues*) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.\r\n\r\nThe *redrive policy* specifies the *source queue*, the *dead-letter queue*, and the conditions under which Amazon SQS moves messages from the former to the latter if the consumer of the source queue fails to process a message a specified number of times.\r\n\r\nYou can set your DLQ as an event source to the Lambda function to drain your DLQ. This will ensure that all failed invocations are automatically retried.\r\n\r\n**CORRECT:** \"Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function\" is the correct answer (as explained above.)\r\n\r\n**INCORRECT:** \"Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group\" is incorrect.\r\n\r\nThe information in the logs may not be sufficient for processing the event. This is not an automated or ideal solution.\r\n\r\n**INCORRECT:** \"Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again\" is incorrect.\r\n\r\nAmazon EventBridge can be configured as a failure destination and can send to SNS. SNS can also be configured with Lambda as a target. However, this solution requires more operational overhead compared to using a DLQ.\r\n\r\n**INCORRECT:** \"Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events\" is incorrect.\r\n\r\nS3 is not a supported failure destination. Supported destinations are Amazon SNS, Amazon SQS, and Amazon EventBridge.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\r\n\r\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html"
  },
  {
    "id": 63,
    "question": "A product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2 Spot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain software license keys in the template each time it is needed.\r\n\r\nWhich solution meets this requirement while offering the most secure and cost-effective approach?",
    "options": [
      "Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the secret in the CloudFormation template.",
      "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.",
      "Pass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho attribute on the parameter.",
      "Embed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key using the Parameter section. Enable the NoEcho attribute on the parameter."
    ],
    "question_type": "single",
    "correct_answers": [
      "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template."
    ],
    "correct_answer": "Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.",
    "explanation": "Dynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as the AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations and passes the value to the appropriate resource. CloudFormation does not store the actual reference value.\r\n\r\nThe following snippet shows how you can use the `ssm-secure` dynamic reference to retrieve an IAM user\u2019s password from the Parameter Store for console login. `IAMUserPassword` pertains to the parameter name followed by the version number.\r\n\r\n```\r\nMyIAMUser:\r\n  Type: AWS::IAM::User\r\n  Properties:\r\n    UserName: 'MyUserName'\r\n    LoginProfile:\r\n      Password: '{{resolve:ssm-secure:IAMUserPassword:10}}'\r\n```\r\n\r\nIn the scenario, storing the license key as `SecureString` means encrypting it using a KMS key, making it more secure than storing it in plaintext. It\u2019s also more cost-effective than Secrets Manager since you don\u2019t pay for the number of parameters you store in the Parameter Store (Standard tier).\r\n\r\nHence, the correct answer is: **Store the license key as a `SecureString` in AWS Systems Manager (SSM) Parameter Store. Use the `ssm-secure` dynamic reference to retrieve the secret in the CloudFormation template.**\r\n\r\nThe option that says: **Pass the license key in the `Parameter` section of the CloudFormation template during stack creation. Enable the `NoEcho` attribute on the parameter** is incorrect. Although using the `NoEcho` attribute can help prevent the license key from being displayed in plaintext in the CloudFormation logs and console outputs,\r\n\r\nThe option that says: **Store the license key as a secret in AWS Secrets Manager. Use the `secretsmanager` dynamic reference to retrieve the secret in the CloudFormation template** is incorrect. Although using Secrets Manager is a valid approach, it\u2019s less cost-effective compared to using SSM Parameter Store. With Secrets Manager, there is a monthly cost associated with storing secrets, whereas SSM Parameter Store (Standard tier) is free of charge.\r\n\r\nThe option that says: **Embed the license keys in the `Mapping` section of the CloudFormation template. Let users choose the correct license key using the `Parameter` section. Enable the `NoEcho` attribute on the parameter** is incorrect. Embedding sensitive data, such as license keys, within CloudFormation templates poses a security risk, as the data can be viewed by anyone with access to the template.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#creds\r\n\r\nhttps://catalog.workshops.aws/cfn101/en-US/intermediate/templates/dynamic-references"
  },
  {
    "id": 64,
    "question": "A developer has recently launched a new API Gateway service which is integrated with AWS Lambda. He enabled API caching and per-key cache invalidation features in the API Gateway to comply with the requirement of the front-end development team which will use the API. The front-end team will have to invalidate an existing cache entry in some scenarios and fetch the latest data from the integration endpoint.\r\n\r\nWhich of the following should the consumers of the API do to invalidate the cache in API Gateway?",
    "options": [
      "Configure the front-end application to clear the browser cache before fetching data from API Gateway.",
      "Send a request with the Cache-Control: no-cache header.",
      "Send a request with the Cache-Control: max-age=0 header.",
      "Send a request with the Cache-Control: INVALIDATE_CACHE header."
    ],
    "question_type": "single",
    "correct_answers": [
      "Send a request with the Cache-Control: max-age=0 header."
    ],
    "correct_answer": "Send a request with the Cache-Control: max-age=0 header.",
    "explanation": "A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the `Cache-Control: max-age=0` header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/27241e8b49c94b4caae9d22f681eac77.png)\r\n\r\nTicking the `Require authorization` checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API.\r\n\r\nHence, to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests, you can just **send a request with the `Cache-Control: max-age=0` header**.\r\n\r\n**Sending a request with the `Cache-Control: no-cache` header** is incorrect because you have to use value of the max-age directive in API Gateway instead of the `no-cache` directive. This just forces the cache to submit the request to the origin server for validation before releasing a cached copy.\r\n\r\n**Configuring the frontend application to clear the browser cache before fetching data from API Gateway** is incorrect because the browser cache and the API Gateway cache are not connected with each other. The correct method of invalidating the cache is to add the `Cache-Control: max-age=0` header*.*\r\n\r\n**Sending a request with the `Cache-Control: INVALIDATE_CACHE` header** is incorrect because there is no directive called `INVALIDATE_CACHE` in the `Cache-Control` header.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\r\n\r\nhttps://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching"
  },
  {
    "id": 65,
    "question": "In the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and GraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the underlying components.\r\n\r\nTo achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer (ALB) and must be instrumented to send trace data to the AWS X-Ray.\r\n\r\nWhich of the following options is the MOST suitable way to satisfy this requirement?",
    "options": [
      "Refactor your application to send segment documents directly to X-Ray by using the PutTraceSegments API.",
      "Use a user data script to install the X-Ray daemon.",
      "Enable AWS X-Ray tracing on the ASG\u2019s launch template.",
      "Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use a user data script to install the X-Ray daemon."
    ],
    "correct_answer": "Use a user data script to install the X-Ray daemon.",
    "explanation": "The AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application.\r\n\r\nTo properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will install and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/5f8e08dd724c4b33bd60544d4eadef0f.png)\r\n\r\nThe AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.\r\n\r\nHence, the correct answer is: **Use a user data script to install the X-Ray daemon.**\r\n\r\nThe option that says: **Enable AWS X-Ray tracing on the ASG\u2019s launch template** is incorrect. There\u2019s no option to enable X-Ray tracing in a launch template of an ASG.\r\n\r\nThe option that says: **Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests** is incorrect. Although it can help monitor and protect the application from common web exploits, it\u2019s not capable of instrumenting the application.\r\n\r\nThe option that says: **Refactor your application to send segment documents directly to X-Ray by using the `PutTraceSegments` API** is incorrect. Although this solution will work, it entails a lot of manual effort to perform. You don\u2019t need to do this because you can just install the X-Ray daemon on the instance to automate this process.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html#xray-daemon-permissions"
  },
  {
    "id": 66,
    "question": "A financial mobile application has a serverless backend API which consists of DynamoDB, Lambda, and Cognito. Due to the confidential financial transactions handled by the mobile application, there is a new requirement provided by the company to add a second authentication method that doesn\u2019t rely solely on user name and password.\r\n\r\nWhich of the following is the MOST suitable solution that the developer should implement?",
    "options": [
      "Use a new IAM policy to a user pool in Cognito.",
      "Create a custom application that integrates with Amazon Cognito which implements the second layer of authentication.",
      "Use Cognito with SNS to allow additional authentication via SMS.",
      "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users."
    ],
    "question_type": "single",
    "correct_answers": [
      "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users."
    ],
    "correct_answer": "Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.",
    "explanation": "You can add multi-factor authentication (MFA) to a user pool to protect the identity of your users. MFA adds a second authentication method that doesn\u2019t rely solely on usernames and passwords. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users. You can also use adaptive authentication with its risk-based model to predict when you might need another authentication factor. It\u2019s part of the user pool\u2019s advanced security features, which also include protections against compromised credentials.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/fc55c345612440ab8568814afc8aa90b.png)\r\n\r\nMulti-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on user name and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users.\r\n\r\nWith adaptive authentication, you can configure your user pool to require second-factor authentication in response to an increased risk level.\r\n\r\nHence, the correct answer in this scenario is to **integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.**\r\n\r\n**Creating a custom application that integrates with Amazon Cognito which implements the second layer of authentication** is incorrect. Although this option is viable, it is not the most suitable solution in this scenario since you can simply use MFA as a second-factor authentication for the mobile app.\r\n\r\n**Using a new IAM policy to a user pool in Cognito** is incorrect because an IAM Policy alone cannot implement a second-factor authentication. You have to configure Cognito to use MFA instead.\r\n\r\n**Using Cognito with SNS to allow additional authentication via SMS** is incorrect. Although this is part of the MFA setup, using this solution alone is not enough if you didn\u2019t enable MFA in the first place.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/managing-security.html\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"
  },
  {
    "id": 67,
    "question": "A serverless application, which uses a DynamoDB database, is experiencing throttling issues during peak times. To troubleshoot the problem, you were instructed to get the total number of write capacity units consumed for the table and any secondary indexes whenever the `UpdateItem` operation is sent.\r\n\r\nIn this scenario, what is the MOST appropriate value for the `ReturnConsumedCapacity` parameter that you should set in the update request?",
    "options": [
      "NONE",
      "TOTAL",
      "TRUE",
      "INDEXES"
    ],
    "question_type": "single",
    "correct_answers": [
      "INDEXES"
    ],
    "correct_answer": "INDEXES",
    "explanation": "To create, update, or delete an item in a DynamoDB table, use one of the following operations:\r\n\r\n`- PutItem`\r\n\r\n`- UpdateItem`\r\n\r\n`- DeleteItem`\r\n\r\nFor each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key (partition key and sort key), you must supply a value for the partition key and a value for the sort key.\r\n\r\nTo return the number of write capacity units consumed by any of these operations, set the `ReturnConsumedCapacity` parameter to one of the following:\r\n\r\n`TOTAL` \u2014 returns the total number of write capacity units consumed.\r\n\r\n`INDEXES` \u2014 returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation.\r\n\r\n`NONE` \u2014 no write capacity details are returned. (This is the default.)\r\n\r\nHence, the correct answer is to add the ReturnConsumedCapacity parameter with a value of **`INDEXES`** in every update request.\r\n\r\nSetting the parameter to **`TRUE`** is incorrect because the ReturnConsumedCapacity parameter is not a boolean type. The valid values that you can use are **`TOTAL`**, **`INDEXES`** and **`NONE`** only.\r\n\r\nSetting the parameter to **`TOTAL`** is incorrect because this just returns the total number of write capacity units consumed but not the subtotals for the table and any secondary indexes that were affected by the operation.\r\n\r\nSetting the parameter to **`NONE`** is incorrect because this is the default value where no write capacity details are returned.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters"
  },
  {
    "id": 68,
    "question": "A developer recently deployed a serverless application, which consists of a Lambda function, API Gateway, and DynamoDB using the `sam deploy` CLI command. The Lambda function is invoked through the API Gateway and then processes and stores the data in a DynamoDB table with an average time of 20 minutes. However, the IT Support team noticed that there are several terminated Lambda invocations that happen every day, which is causing data discrepancies.\r\n\r\nWhich of the following options is the MOST likely root cause of this problem?",
    "options": [
      "The concurrent execution limit has been reached.",
      "The Lambda function contains a recursive code and has been running for over 15 minutes.",
      "The failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time.",
      "The serverless application should be deployed using the sam publish CLI command instead."
    ],
    "question_type": "single",
    "correct_answers": [
      "The failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time."
    ],
    "correct_answer": "The failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time.",
    "explanation": "A Lambda function consists of code and any associated dependencies. In addition, a Lambda function also has configuration information associated with it. Initially, you specify the configuration information when you create a Lambda function. Lambda provides an API for you to update some of the configuration data.\r\n\r\nYou pay for the AWS resources that are used to run your Lambda function. To prevent your Lambda function from running indefinitely, you specify a **timeout**. When the specified timeout is reached, AWS Lambda terminates execution of your Lambda function. It is recommended that you set this value based on your expected execution time.\r\n\r\nTake note that you can invoke a Lambda function synchronously either by calling the `Invoke` operation or by using an AWS SDK in your preferred runtime. If you anticipate a long-running Lambda function, your client may time out before function execution completes. To avoid this, update the client timeout or your SDK configuration.\r\n\r\nThe default timeout is 3 seconds and the maximum execution duration per request in AWS Lambda is 900 seconds, which is equivalent to 15 minutes. Hence, the most likely root cause in this scenario is that **the failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time**.\r\n\r\nThe option that says: **The serverless application should be deployed using the `sam publish` CLI command instead** is incorrect as this CLI command just publishes an AWS SAM application to the AWS Serverless Application Repository. The fact that some invocations of the Lambda function work fine means that the deployment is successful. Hence, there is no issue on the deployment process of the serverless application but only on its maximum execution time.\r\n\r\nThe option that says: **The concurrent execution limit has been reached** is incorrect because, by default, the AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. By setting a concurrency limit on a function, Lambda guarantees that allocation will be applied specifically to that function, regardless of the amount of traffic processing the remaining functions. If that limit is exceeded, the function will be throttled but not terminated, which is in contrast with what is happening in the scenario.\r\n\r\nThe option that says: **The Lambda function contains a recursive code and has been running for over 15 minutes** is incorrect because having a recursive code in your Lambda function does not directly result to an abrupt termination of the function execution. This is a scenario wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs, but not an abrupt termination because Lambda will throttle all invocations to the function.\r\n\r\n**Reference:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/limits.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/resource-model.html"
  },
  {
    "id": 69,
    "question": "A startup recently launched a high-quality photo-sharing portal using Amazon Lightsail and Amazon S3. The team noticed that other external websites are linking and using the photos without permission. This situation has caused an increase in data transfer costs and potential revenue loss.\r\n\r\nWhich of the following is the MOST effective method to solve this issue?",
    "options": [
      "Use an Amazon CloudFront web distribution with signed URLs or signed cookies.",
      "Block the IP addresses of the offending websites using Network Access Control List.",
      "Enable cross-origin resource sharing (CORS) which allows cross-origin GET requests from all origins.",
      "Configure the S3 bucket to remove public read access and use pre-signed URLs with expiry dates."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use an Amazon CloudFront web distribution with signed URLs or signed cookies."
    ],
    "correct_answer": "Use an Amazon CloudFront web distribution with signed URLs or signed cookies.",
    "explanation": "A signed URL contains extra information, such as an expiration date and time, providing greater control over access to your content. This additional information is presented in a policy statement, which is derived from either a predefined (canned) policy or a personalized (custom) policy.\r\n\r\nCloudFront signed cookies allow you to control who can access your content when you don\u2019t want to change your current URLs or when you want to provide access to multiple restricted files, such as all of the files in a website\u2019s subscribers\u2019 area. This topic explains the considerations when using signed cookies and describes how to set signed cookies using canned and custom policies.\r\n\r\nThe most effective method to control unauthorized access to the photos and manage data transfer costs is to use an Amazon CloudFront web distribution with signed URLs or signed cookies. This approach allows the startup to enforce access controls at the CDN layer, ensuring that only authorized users can access the content.\r\n\r\n![Amazon CloudFront Restrict viewer access](file:///home/skworkstation/.config/joplin-desktop/resources/e35cff805d7c418ebf9ed164ef985b2d.png)\r\n\r\nAdditionally, CloudFront Functions can be used to validate referrer headers, adding another layer of protection by ensuring that only requests originating from the startup\u2019s own domain are allowed to access the content.\r\n\r\n![CloudFront Function](file:///home/skworkstation/.config/joplin-desktop/resources/2c9af64c248a4f7b912843b9423231b0.png)\r\n\r\nBy using CloudFront with signed URLs or signed cookies, the startup can manage access effectively, providing a scalable solution that prevents unauthorized use while controlling data transfer costs. The use of CloudFront Functions to validate referrer headers adds an extra layer of security, ensuring that the photos are only accessible through authorized channels, thereby protecting the startup\u2019s content and reducing unnecessary data transfer costs.\r\n\r\nHence, the correct answer is: **Use an Amazon CloudFront web distribution with signed URLs or signed cookies.**\r\n\r\nThe option that says: **Enable cross-origin resource sharing (CORS) which allows cross-origin GET requests from all origins** is incorrect as this will typically make the problem worse since you are allowing any website or origin to fetch the objects in the S3 bucket. Although using CORS is a valid solution, it should be properly configured to only enable access to your trusted domains.\r\n\r\nThe option that says: **Configure the S3 bucket to remove public read access and use pre-signed URLs with expiry dates** is incorrect because it is not primarily scalable for large numbers of objects, as it would require generating pre-signed URLs for potentially thousands or millions of objects, making it impractical.\r\n\r\n**Blocking the IP addresses of the offending websites using Network Access Control List** is incorrect because a quick change in IP address would easily bypass this configuration; hence, this is not an efficient method to implement.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html"
  },
  {
    "id": 70,
    "question": "You are a newly hired developer in a leading investment bank which uses AWS as its cloud infrastructure. One of your tasks is to develop an application that will store financial data to an already existing S3 bucket, which has the following bucket policy:\r\n\r\n```\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Id\": \"PutObjPolicy\",\r\n  \"Statement\": [\r\n    {\r\n      \"Sid\": \"AllowUploadCheck\",\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::awsdev\\/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Sid\": \"AllowNullCheck\",\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::awsdev\\/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nWhich of the following statements is true about uploading data to this S3 bucket?",
    "options": [
      "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of Null.",
      "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256.",
      "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of true.",
      "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of aws:kms."
    ],
    "question_type": "single",
    "correct_answers": [
      "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256."
    ],
    "correct_answer": "The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256.",
    "explanation": "Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\r\n\r\nIf you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. For example, the following bucket policy denies permissions to upload an object to the tutorialsdojo bucket unless the request includes the x-amz-server-side-encryption header to request server-side encryption:\r\n\r\n \r\n\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Id\": \"PutObjPolicy\",\r\n  \"Statement\": [\r\n    {\r\n      \"Sid\": \"DenyIncorrectEncryptionHeader\",\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::tutorialsdojo/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Sid\": \"DenyUnEncryptedObjectUploads\",\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::tutorialsdojo/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n \r\n\r\nTake note that the Sid (statement ID) is just an optional identifier that you provide for the policy statement. The Effect element is required and specifies whether the statement results in an allow or an explicit deny. The valid values for the Effect element are Allow and Deny. The value of the Sid element does not affect nor override the Effect element.\r\n\r\nHence, the correct answer is: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256.\r\n\r\nThe option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of Null is incorrect because the value of the header should be AES256 and not Null. Take note that the Null in the policy actually means that the request will be denied if there is no x-amz-server-side-encryption header included.\r\n\r\nThe option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of true is incorrect because this header is not a boolean type. It can only accept two values: AES256 and aws:kms.\r\n\r\nThe option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of aws:kms is incorrect. Based on the given bucket policy, the value of the header should be AES256, which means that the bucket is using Amazon S3-Managed Keys (SSE-S3). Conversely, if this header has a value of aws:kms, then it uses AWS KMS Keys (SSE-KMS).\r\n\r\n \r\n\r\nReferences:\r\n\r\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html\r\n\r\nhttps://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/"
  },
  {
    "id": 71,
    "question": "A developer is planning to build a serverless Rust application in AWS using AWS Lambda and Amazon DynamoDB. Much to his disappointment, AWS Lambda does not natively support the Rust programming language.\r\n\r\nCan the developer still proceed with creating serverless Rust applications in AWS given the situation above?",
    "options": [
      "Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function.",
      "Yes. The developer can submit a request ticket to AWS so that they can provide him a Lambda runtime environment that supports Rust.",
      "Yes. The developer will just have to use AWS Fargate instead of AWS Lambda.",
      "No. The developer will have to wait for a new support release in AWS Lambda."
    ],
    "question_type": "single",
    "correct_answers": [
      "Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function."
    ],
    "correct_answer": "Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function.",
    "explanation": "AWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code. It also provides a **Runtime API** which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function\u2019s handler method when the function is invoked. You can include a runtime in your function\u2019s deployment package in the form of an executable file named `bootstrap`.\r\n\r\nYour custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that\u2019s included in Amazon Linux, or a binary executable file that\u2019s compiled in Amazon Linux. Therefore, if the developer publishes a custom runtime for Rust, he can continue building his serverless application in AWS Lambda.\r\n\r\nHence, the correct answer in this scenario is: **Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function**.\r\n\r\nThe option that says: **Yes. The developer can submit a request ticket to AWS so that they can provide him a Lambda runtime environment that supports Rust** is incorrect because you cannot request specific runtime environments for AWS Lambda from AWS. You would need to create this yourself using the **Runtime API**.\r\n\r\nThe option that says: **Yes. The developer will just have to use AWS Fargate instead of AWS Lambda** is incorrect since this service is a serverless compute engine for containers. Unless the Rust application is running in Docker, which is not explicitly stated in the scenario, it\u2019ll be best to use AWS Lambda for serverless computing.\r\n\r\nThe option that says: **No. The developer will have to wait for a new support release in AWS Lambda** is incorrect because there is no need to wait for a new feature release or for code translation since AWS Lambda allows you to create a runtime that appropriately handles your function code when invoked.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/"
  },
  {
    "id": 72,
    "question": "The operating cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing, a Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The average size of items stored in the database is 17 KB.\r\n\r\nWhich of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its performance?",
    "options": [
      "Decrease the provisioned RCU down to 800.",
      "Switch the table from using provisioned mode to on-demand mode.",
      "Implement exponential backoff.",
      "Set the provisioned RCU to 1600."
    ],
    "question_type": "single",
    "correct_answers": [
      "Set the provisioned RCU to 1600."
    ],
    "correct_answer": "Set the provisioned RCU to 1600.",
    "explanation": "In this scenario, a Lambda function makes 320 strongly consistent read requests per second against a DynamoDB table which has a provisioned RCU of 5440. The average size of items stored in the database is 17 KB.\r\n\r\nIt seems that the RCU is quite high and the calculations to come up with this value are incorrect. If you multiply 320 x 17, then you\u2019ll get 5,440 which is a correct calculation for WCU but **not** for the RCU.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/1e8b5742ef2e43778b14c10d8fd98c1c.png)\r\n\r\n**1 RCU** can do **1 strongly consistent read** or **2 eventually consistent reads** for an item up to **4KB**.\r\n\r\n**To get the RCU with strongly consistent reads, do the following steps:**\r\n\r\n**Step #1 Divide the average item size by 4 KB. Round up the result**\r\n\r\nAverage Item Size = 17 KB\r\n\r\n= **17KB/4KB**\r\n\r\n**= 4.25 \u2248 5**\r\n\r\n**Step #2 Multiply the number of reads per second by the resulting value from Step 1. (Divide the product by 2 for eventually consistent reads)**\r\n\r\n= **320** reads per second **x** **5**\r\n\r\n= **1,600** **strongly consistent read requests**\r\n\r\nHence, the correct answer is to **set the provisioned RCU to 1600** as this will lower the cost and still maintain the performance of your application.\r\n\r\n**Implementing exponential backoff** is incorrect because this is only applicable for error retries and error handling of the serverless application.\r\n\r\n**Decreasing the provisioned RCU down to 800** is incorrect. Although this will lower the cost, it will not meet the strong consistency requirements of the application. Take note that the Lambda function makes read requests with a strong consistency type and not eventual consistency.\r\n\r\n**Switching the table from using provisioned mode to on-demand mode** is incorrect. Although this will lower the cost, the on-demand mode is more suitable for unpredictable application traffic. The scenario explicitly mentioned the exact application traffic, which is why the provisioned mode is more suitable to use.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads"
  },
  {
    "id": 73,
    "question": "Your request to increase your account\u2019s concurrent execution limit to 2000 has been recently approved by AWS. There are 10 Lambda functions running in your account and you already specified a concurrency execution limit on one function at 400 and on another function at 200.\r\n\r\nWhich of the following statements are TRUE in this scenario? (Select TWO.)",
    "options": [
      "The unreserved concurrency pool is 600.",
      "The remaining 1400 concurrent executions will be shared among the other 8 functions.",
      "You can still set a concurrency execution limit of 1300 to a third Lambda function.",
      "You can still set a concurrency execution limit of 1400 to a third Lambda function.",
      "The combined allocated 600 concurrent execution will be shared among the 2 functions."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "The remaining 1400 concurrent executions will be shared among the other 8 functions.",
      "You can still set a concurrency execution limit of 1300 to a third Lambda function."
    ],
    "correct_answer": "The remaining 1400 concurrent executions will be shared among the other 8 functions.",
    "explanation": "The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level.\r\n\r\nThe *concurrent executions* refer to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source.\r\n\r\n![](file:///home/skworkstation/.config/joplin-desktop/resources/17fabadfe4954b2eb11384b279976f7d.png)\r\n\r\nIf you create a Lambda function to process events from event sources that aren\u2019t poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency.\r\n\r\nIf you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account\u2019s concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions.\r\n\r\nAWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions.\r\n\r\nIn this scenario, you still have 1400 concurrent executions remaining which will be shared by the other 8 Lambda functions in your AWS account. Take note that the unreserved account concurrency can\u2019t go below **100**, which means that you only set a concurrency execution limit of 1300 to a single function or spread out to the remaining 8 functions.\r\n\r\nHence, the correct answers in this scenario are:\r\n\r\n**\u2013 The remaining 1400 concurrent executions will be shared among the other 8 functions.**\r\n\r\n**\u2013 You can still set a concurrency execution limit of 1300 to a third Lambda function.**\r\n\r\nThe option that says: **the unreserved concurrency pool is 600** is incorrect because this is the value of the total reserved concurrency that you have allocated to the 2 Lambda functions.\r\n\r\nThe option that says: **you can still set a concurrency execution limit of 1400 to a third Lambda function** is incorrect because the unreserved account concurrency cannot go below 100, which means that you only set a concurrency execution limit of 1300 to the third function or spread out to the remaining 8 functions.\r\n\r\nThe option that says: **the combined allocated 600 concurrent execution will be shared among the 2 functions** is incorrect because the execution limit is per function only and will not be shared with other functions, which also have reserved concurrent executions.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/scaling.html"
  },
  {
    "id": 74,
    "question": "A developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application environment. She will use X-Ray to construct a service map to help identify issues with her application and to provide insight on which application component to optimize. The environment is using a default Elastic Beanstalk instance profile.\r\n\r\nWhich IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?",
    "options": [
      "AWSXRayElasticBeanstalkWriteAccess",
      "AWSXrayReadOnlyAccess",
      "AWSXrayFullAccess",
      "AWSXRayDaemonWriteAccess"
    ],
    "question_type": "single",
    "correct_answers": [
      "AWSXRayDaemonWriteAccess"
    ],
    "correct_answer": "AWSXRayDaemonWriteAccess",
    "explanation": "You can use **AWS Identity and Access Management (IAM)** to grant X-Ray permissions to users and compute resources in your account. IAM controls access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI) your users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add the AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write permissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray daemon, the AWS CLI, and the AWS SDK.\r\n\r\n&nbsp;\r\n\r\nTo deploy your instrumented app to AWS, create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes permission to upload traces, and some read permissions as well to support the use of sampling rules.\r\n\r\nThe read and write policies do not include permission to configure encryption key settings and sampling rules. Use AWSXrayFullAccess to access these settings, or add configuration APIs in a custom policy. For encryption and decryption with a customer-managed key that you create, you also need permission to use the key.\r\n\r\nOn supported platforms, you can use a configuration option to run the X-Ray daemon on the instances in your environment. You can enable the daemon in the Elastic Beanstalk console or by using a configuration file. To upload data to X-Ray, the X-Ray daemon requires IAM permissions in the <ins>AWSXRayDaemonWriteAccess</ins> managed policy. These permissions are included in the Elastic Beanstalk instance profile.\r\n\r\nHence, the correct answer is the **`AWSXRayDaemonWriteAccess`** managed policy.\r\n\r\n`AWSXrayReadOnlyAccess` is incorrect because this policy is primarily used if you just want a read-only access to X-Ray.\r\n\r\n`AWSXrayFullAccess` is incorrect. Although this can provide the required access to the daemon, this is not being used in Elastic Beanstalk as it does not abide by the standard security advice of granting the least privilege.\r\n\r\n`AWSXRayElasticBeanstalkWriteAccess` is incorrect because this is not an available managed policy.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/security.html"
  },
  {
    "id": 75,
    "question": "A company is using a combination of CodeBuild, CodePipeline, and CodeDeploy services for its continuous integration and continuous delivery (CI/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the action is approved, the pipeline execution resumes, but if it is not, then the pipeline execution will not proceed.\r\n\r\nWhich of the following is the MOST suitable solution to implement in this scenario?",
    "options": [
      "Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.",
      "Remodel the pipeline using AWS Serverless Application Model (AWS SAM)",
      "Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval.",
      "Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic."
    ],
    "question_type": "single",
    "correct_answers": [
      "Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic."
    ],
    "correct_answer": "Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.",
    "explanation": "In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.\r\n\r\nIf the action is approved, the pipeline execution resumes. If the action is rejected \u2013 or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping \u2013 the result is the same as an action failing, and the pipeline execution does not continue.\r\n\r\nYou might use manual approvals for these reasons:\r\n\r\n\u2013 You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.\r\n\r\n\u2013 You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.\r\n\r\n\u2013 You want someone to review new or updated text before it is published to a company website.\r\n\r\n&nbsp;\r\n\r\nYou can configure an approval action to publish a message to an Amazon Simple Notification Service topic when the pipeline stops at the action. Amazon SNS delivers the message to every endpoint subscribed to the topic. You must use a topic created in the same AWS region as the pipeline that will include the approval action. When you create a topic, it is recommended that you give it a name that will identify its purpose, in formats such as `tutorialsdojoManualApprovalPHL-us-east-2-approval`.\r\n\r\nHence, the correct answer is to **Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.**\r\n\r\nThe option that says: **Remodel the pipeline using AWS Serverless Application Model (AWS SAM)** is incorrect because this service is just a framework for building serverless applications, not a replacement for a CI/CD pipeline.\r\n\r\nThe option that says: **Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue** is incorrect. Although setting up a manual approval is valid, the use of SQS is wrong because it doesn\u2019t have an integration with manual approval actions. Use SNS instead to send the approval action emails to the recipient who will either approve or deny the action.\r\n\r\nThe option that says: **Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval** is incorrect as this would only add unnecessary complexity to the CI/CD pipeline. The requirement in the scenario can be achieved using the built-in manual approval actions in CodePipeline.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\r\n\r\n[https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html](https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html)"
  },
  {
    "id": 76,
    "question": "You are working as an IT Consultant for a top investment bank in Europe which uses several serverless applications in their AWS account. They just launched a new API Gateway service with a Lambda proxy integration and you were instructed to test out the new API. However, you are getting a `Connection refused` error whenever you use this Invoke URL `` http://779protaw8.execute-api.us-east-1.amazonaws.com/passawsexam/ `of the API Gateway.` ``\r\n\r\nWhich of the following is the MOST likely cause of this issue?",
    "options": [
      "You are not using FTP in invoking the API.",
      "You are not using HTTPS in invoking the API.",
      "You are not using WebSocket in invoking the API.",
      "You are not using HTTP/2 in invoking the API."
    ],
    "question_type": "single",
    "correct_answers": [
      "You are not using HTTPS in invoking the API."
    ],
    "correct_answer": "You are not using HTTPS in invoking the API.",
    "explanation": "All of the APIs created with Amazon API Gateway expose **HTTPS** endpoints only. Amazon API Gateway does not support unencrypted (HTTP) endpoints. By default, Amazon API Gateway assigns an internal domain to the API that automatically uses the Amazon API Gateway certificate. When configuring your APIs to run under a custom domain name, you can provide your own certificate for the domain.\r\n\r\nCalling a deployed API involves submitting requests to the URL for the API Gateway component service for API execution, known as `execute-api`. The base URL for REST APIs is in the following format:\r\n\r\n``https://`{restapi_id}`.execute-api.`{region}`.amazonaws.com/`{stage_name}`/``\r\n\r\nwhere *`{restapi_id}`* is the API identifier, *`{region}`* is the region, and *`{stage_name}`* is the stage name of the API deployment.\r\n\r\nHence, the most likely cause of the issue in the scenario is that **you are not using HTTPS in invoking the API**.\r\n\r\nThe option that says: **you are not using HTTP/2 in invoking the API** is incorrect because API Gateway only supports HTTPS.\r\n\r\nThe option that says: **you are not using FTP in invoking the API** is incorrect because API Gateway is using HTTPS to expose the APIs. FTP is primarily used for accessing file servers and not Web APIs.\r\n\r\nThe option that says: **you are not using WebSocket in invoking the API** is incorrect because all of the APIs created with Amazon API Gateway expose HTTPS endpoints only.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-call-api.html\r\n\r\nhttps://aws.amazon.com/api-gateway/faqs/"
  },
  {
    "id": 77,
    "question": "You are a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. Most of the clients of your company are using serverless computing in which you are responsible for ensuring that their applications are working efficiently.\r\n\r\nWhich of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)",
    "options": [
      "You can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to false.",
      "The concurrent execution limit is enforced against the sum of the concurrent executions of all function.",
      "You have to install the X-Ray daemon in Lambda to enable active tracing.",
      "An increase in memory size triggers an equivalent increase in CPU available to your function.",
      "Lambda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private VPC."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "The concurrent execution limit is enforced against the sum of the concurrent executions of all function.",
      "An increase in memory size triggers an equivalent increase in CPU available to your function."
    ],
    "correct_answer": "The concurrent execution limit is enforced against the sum of the concurrent executions of all function.",
    "explanation": "You can use the AWS Lambda API or console to configure settings on your Lambda functions. Basic function settings include the description, role, and runtime that you specify when you create a function in the Lambda console. You can configure more settings after you create a function, or use the API to set things like the handler name, memory allocation, and security groups during creation.\r\n\r\nLambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions.\r\n\r\n&nbsp;\r\n\r\nDuration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function.\r\n\r\nIn the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function.\r\n\r\nThe unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level.\r\n\r\n&nbsp;\r\n\r\nHence, the valid considerations in improving the performance of Lambda functions are:\r\n\r\n***\u2013* An increase in memory size triggers an equivalent increase in CPU available to your function.**\r\n\r\n***\u2013* The concurrent execution limit is enforced against the sum of the concurrent executions of all functions.**\r\n\r\nThe option that says: **You have to install the X-Ray daemon in Lambda to enable active tracing** is incorrect because you only have to install the X-Ray daemon if you are using Elastic Beanstalk, ECS, or EC2 instances. You simply need to tick the *Enable AWS X-Ray* checkbox in the Lambda function to enable active tracing.\r\n\r\nThe option that says: **Lambda automatically creates Elastic IPs that enable your function to connect securely to other resources within your private VPC** is incorrect because Lambda actually creates ENI (Elastic Network Interface) and not Elastic IPs if the function is connected to your VPC.\r\n\r\nThe option that says: **You can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to `false`** is incorrect because the concurrency setting is not a boolean type which is why setting it as *false* is invalid. To throttle all incoming executions, you can manually set the concurrency to 0 or just click the \u2018Throttle\u2019 button in the Lambda console.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/resource-model.html\r\n\r\n[https://aws.amazon.com/lambda/pricing/](https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html)\r\n\r\n[https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html](https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html)"
  },
  {
    "id": 78,
    "question": "An aerospace engineering company has recently migrated to AWS for their cloud architecture. They are using CloudFormation and AWS SAM as deployment services for both of their monolithic and serverless applications. There is a new requirement where you have to dynamically install packages, create files, and start services on your EC2 instances upon the deployment of the application stack using CloudFormation.\r\n\r\nWhich of the following helper scripts should you use in this scenario?",
    "options": [
      "cfn-hup",
      "cfn-init",
      "cfn-get-metadata",
      "cfn-signal"
    ],
    "question_type": "single",
    "correct_answers": [
      "cfn-init"
    ],
    "correct_answer": "cfn-init",
    "explanation": "**AWS CloudFormation** provides the following Python helper scripts that you can use to install software and start services on an Amazon EC2 instance that you create as part of your stack:\r\n\r\n[cfn-init](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-init.html): Use to retrieve and interpret resource metadata, install packages, create files, and start services.\r\n\r\n[cfn-signal](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html): Use to signal with a CreationPolicy or WaitCondition, so you can synchronize other resources in the stack when the prerequisite resource or application is ready.\r\n\r\n[cfn-get-metadata](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-get-metadata.html): Use to retrieve metadata for a resource or path to a specific key.\r\n\r\n[cfn-hup](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html): Use to check for updates to metadata and execute custom hooks when changes are detected.\r\n\r\nYou call the scripts directly from your template. The scripts work in conjunction with resource metadata that\u2019s defined in the same template. The scripts run on the Amazon EC2 instance during the stack creation process. The scripts are not executed by default. You must include calls in your template to execute specific helper scripts.\r\n\r\n&nbsp;\r\n\r\nHence, **cfn-init** helper script is the correct answer since it interprets the metadata that contains the sources, packages, files, and services. You run the script on the EC2 instance when it is launched. The script is installed by default on Amazon Linux and Windows AMIs.\r\n\r\nThe **cfn-get-metadata** helper script is incorrect since it is only a wrapper script that retrieves either all metadata that is defined for a resource or path to a specific key or a subtree of the resource metadata, but does not interpret the resource metadata, install packages, create files, and start services.\r\n\r\nThe **cfn-signal** helper script is incorrect since it does not perform any retrieval and interpretation of resource metadata, installation of packages, creation of files, and starting of services. Instead, it is a wrapper thats signals an AWS CloudFormation WaitCondition for synchronizing other resources in the stack when the application is ready.\r\n\r\nThe **cfn-hup** helper script is incorrect because this is just a daemon that checks for updates to metadata and executes custom hooks when changes are detected. It does not retrieve and interpret the resource metadata, install packages, create files, and start services unlike cfn-init helper script.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-init.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html\r\n\r\nhttps://s3.amazonaws.com/cloudformation-examples/BoostrappingApplicationsWithAWSCloudFormation.pdf"
  },
  {
    "id": 79,
    "question": "A developer has a [Node.js](http://node.js/) function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database every time the Lambda function is executed, and closes the connection before the function ends.\r\n\r\nWhat feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the function is run?",
    "options": [
      "Environment variables",
      "Execution context",
      "AWS Lambda is not capable of maintaining existing database connections due to its transient data store.",
      "Event source mapping"
    ],
    "question_type": "single",
    "correct_answers": [
      "Execution context"
    ],
    "correct_answer": "Execution context",
    "explanation": "When AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you create a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to allow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration settings you provide.\r\n\r\nThe **execution context** is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to \u201ccold-start\u201d or initialize those external dependencies. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again.\r\n\r\nHence, the correct answer is: **Execution context.**\r\n\r\n**Environment variables** is incorrect because these are just variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration.\r\n\r\n**Event source mapping** is incorrect because an event source is just an entity that publishes events and is integrated with your Lambda function. Supported event sources are the AWS services that can be preconfigured to work with AWS Lambda. The configuration is referred to as *event source mapping*, which maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur.\r\n\r\nThe option that says: **AWS Lambda is not capable of maintaining existing database connections due to its transient data store** is incorrect because Lambda actually is capable of doing this using the execution context just as discussed above.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-code"
  },
  {
    "id": 80,
    "question": "A developer uses AWS X-Ray to create a trace on an instrumented web application to identify any performance bottlenecks. The segment documents being sent by the application contain annotations that the developer wants to utilize in order to identify and filter out specific data from the trace.\r\n\r\nWhich of the following should the developer do in order to satisfy this requirement with minimal configuration? (Select TWO.)",
    "options": [
      "Fetch the trace IDs and annotations using the GetTraceSummaries API.",
      "Fetch the data using the BatchGetTraces API.",
      "Send trace results to an S3 bucket then query the trace output using Amazon Athena.",
      "Configure Sampling Rules in the AWS X-Ray Console.",
      "Use filter expressions via the X-Ray console."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Fetch the trace IDs and annotations using the GetTraceSummaries API.",
      "Use filter expressions via the X-Ray console."
    ],
    "correct_answer": "Fetch the trace IDs and annotations using the GetTraceSummaries API.",
    "explanation": "The compute resources running your application logic send data about their work as **segments**. A segment provides the resource\u2019s name, details about the request, and details about the work done.\r\n\r\nA subset of segment fields are indexed by X-Ray for use with filter expressions. You can search for segments associated with specific information in the X-Ray console or by using the `GetTraceSummaries` API.\r\n\r\n&nbsp;\r\n\r\nEven with sampling, a complex application generates a lot of data. When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a **filter expression**. Running the `GetTraceSummaries` operation retrieves IDs and annotations for traces available for a specified time frame using an optional filter.\r\n\r\nHence, **using filter expressions via the X-Ray console** and **fetching the trace IDs and annotations using the `GetTraceSummaries` API** are the correct answers in this scenario.\r\n\r\n**Fetching the data using the `BatchGetTraces` API** is incorrect because this API simply retrieves a list of traces specified by ID. It does not support filter expressions nor returns the annotations.\r\n\r\n**Sending trace results to an S3 bucket then querying the trace output using Amazon Athena** is incorrect. Although this solution may work, this entails a lot of configuration which is contrary to what the scenario requires. There are other simpler methods of searching through traces in X-Ray such as using annotations and filter expressions.\r\n\r\n**Configuring Sampling Rules in the AWS X-Ray Console** is incorrect because sampling rules just tell the X-Ray SDK how many requests to record for a set of criteria.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/api/API_GetTraceSummaries.html"
  },
  {
    "id": 81,
    "question": "A company uses AWS Systems Manager (SSM) Parameter Store to manage configuration details for multiple applications. The parameters are currently stored in the Standard tier. The company wants its operations team to be notified if there are sensitive parameters that haven\u2019t been rotated within 90 days.\r\n\r\nWhich must be done to meet the requirement?",
    "options": [
      "Configure a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",
      "Set up an Amazon EventBridge (Amazon CloudWatch Events) event pattern that captures SSM Parameter-related events. Use Amazon SNS to send notifications.",
      "Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",
      "Convert the sensitive parameters from Standard tier into Advanced tier. Set a ExpirationNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS."
    ],
    "question_type": "single",
    "correct_answers": [
      "Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS."
    ],
    "correct_answer": "Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",
    "explanation": "Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter, such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. Take note that parameter policies are only available for parameters in the **Advanced tier**.\r\n\r\nParameter Store offers the following types of policies:\r\n\r\n**`Expiration` \u2013** deletes the parameter at a specific date\r\n\r\n**`ExpirationNotification` \u2013** sends an event to Amazon EventBridge (Amazon CloudWatch Events) when the specified expiration time is reached.\r\n\r\n**`NoChangeNotification` \u2013** sends an event to Amazon EventBridge (Amazon CloudWatch Events) when a parameter has not been modified for a specified period of time.  \r\n<br/>\r\n\r\nThe **`NoChangeNotification`** policy sends a notification based on the LastModifiedTime attribute of the parameter. If you change or edit a parameter, the system resets the notification time period based on the new value of LastModifiedTime. In the scenario\u2019s case, we want to be notified if specific parameters were not rotated in the last 90 days.\r\n\r\nIn the scenario, the goal is to be notified if specific sensitive parameters have not been rotated within the past 90 days. Configuring the **`NoChangeNotification`** policy with a value of 90 days allows SSM to emit a notification to EventBridge whenever the LastModifiedTime of the sensitive parameters exceeds the specified time frame. However, setting the notification policy alone is not enough. You must configure Amazon EventBridge (Amazon CloudWatch Events) to capture the emitted events and route them to an Amazon SNS topic.\r\n\r\nHence, the correct answer is: **Convert the sensitive parameters from Standard tier into Advanced tier. Set a `NoChangeNotification` policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.**\r\n\r\nThe option that says: **Configure a `NoChangeNotification` policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS** is incorrect because notification policies are not supported in the Standard tier. You must convert the parameters first into the Advanced tier.\r\n\r\nThe option that says: **Convert the sensitive parameters from Standard tier into Advanced tier. Set a `ExpirationNotification` policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS** is incorrect because the `ExpirationNotification` policy is for notifying when a parameter is about to expire, not when it hasn\u2019t been rotated. In this case, the `NoChangeNotification` policy should be used instead.\r\n\r\nThe option that says: **Set up an Amazon EventBridge (Amazon CloudWatch Events) event pattern that captures SSM Parameter-related events. Use Amazon SNS to send notifications** is incorrect. A notification policy must be enabled as well, otherwise, Amazon EventBridge (Amazon CloudWatch Events) won\u2019t be able to receive any notifications.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html\r\n\r\nhttps://aws.amazon.com/about-aws/whats-new/2019/04/aws_systems_manager_parameter_store_introduces_advanced_parameters/"
  },
  {
    "id": 82,
    "question": "An Elastic Beanstalk application becomes inaccessible for several minutes whenever a failed deployment is rolled back. A developer should recommend a strategy that will have the least impact on the application\u2019s availability if the deployment fails. Teams must be able to revert changes quickly as well.\r\n\r\nWhich deployment method should the developer suggest?\r\n\r\n",
    "options": [
      "Blue/Green",
      "All at Once",
      "Rolling with Additional Batches",
      "Rolling"
    ],
    "question_type": "single",
    "correct_answers": [
      "Blue/Green"
    ],
    "correct_answer": "Blue/Green",
    "explanation": "Because **AWS Elastic Beanstalk** performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.\r\n\r\nBlue/green deployments require that your environment runs independently of your production database if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment and will be lost if you terminate the original environment.\r\n\r\n&nbsp;\r\n\r\nIn Elastic Beanstalk, you can choose from a variety of deployment methods:\r\n\r\n**All at once** \u2013 Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs.\r\n\r\n**Rolling** \u2013 Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment\u2019s capacity by the number of instances in a batch.\r\n\r\n**Rolling with additional batch** \u2013 Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.\r\n\r\n**Immutable** \u2013 Deploy the new version to a fresh group of instances by performing an immutable update.\r\n\r\n**Traffic splitting** \u2013 Percentage of client traffic routed to new version temporarily impacted\r\n\r\n**Blue/Green** \u2013 Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.\r\n\r\nThe scenario is asking for the least impact on the application\u2019s availability if a deployment fails. Naturally, we\u2019d want to roll back to the last working version if a deployment does not succeed. The rollback process for a Blue/green deployment is the fastest since all you have to do is switch back to the working environment\u2019s URL.\r\n\r\nHence, the correct answer is **Blue/Green**.\r\n\r\n**All at once** is incorrect because this method deploys the new version to all instances simultaneously, which causes your instances to be out of service for a short time while the deployment occurs. This is also the case when you revert a failed deployment. In short, All at once has the MOST impact on your application\u2019s availability in case the deployment fails.\r\n\r\n**Rolling with additional batch** is incorrect. Although this method ensures full capacity during deployment, its rollback process is quite slow because the deployment is done on fresh instances alongside the existing ones.\r\n\r\n**Rolling** is incorrect. With Rolling, your environment\u2019s capacity to serve traffic is reduced by the number of instances the new version is being rolled out to, which may impact the availability of your application.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html"
  },
  {
    "id": 83,
    "question": "A web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to migrate the application to Elastic Beanstalk and the database to RDS for SQL Server. For additional security, you must configure your database to automatically encrypt the actual data before it is written to storage, and automatically decrypt data when the data is read from storage.\r\n\r\nWhich of the following services will you use to achieve this?",
    "options": [
      "Enable Transparent Data Encryption (TDE).",
      "Use Microsoft SQL Server Windows Authentication.",
      "Enable RDS Encryption.",
      "Use IAM DB Authentication."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable Transparent Data Encryption (TDE)."
    ],
    "correct_answer": "Enable Transparent Data Encryption (TDE).",
    "explanation": "Amazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server. TDE automatically encrypts data before it is written to storage, and automatically decrypts data when the data is read from storage.\r\n\r\nAmazon RDS supports TDE for the following SQL Server versions and editions:\r\n\r\n\u2013 SQL Server 2019 Standard and Enterprise Editions\r\n\r\n\u2013 SQL Server 2017 Enterprise Edition\r\n\r\n\u2013 SQL Server 2016 Enterprise Edition\r\n\r\n\u2013 SQL Server 2014 Enterprise Edition\r\n\r\n\u2013 SQL Server 2012 Enterprise Edition\r\n\r\nTransparent Data Encryption is used in scenarios where you need to encrypt sensitive data. For example, you might want to provide data files and backups to a third party, or address security-related regulatory compliance issues.\r\n\r\nTo enable transparent data encryption for an RDS SQL Server DB instance, specify the TDE option in an RDS option group that is associated with that DB instance.\r\n\r\nTransparent data encryption for SQL Server provides encryption key management by using a two-tier key architecture. A certificate, which is generated from the database master key, is used to protect the data encryption keys. The database encryption key performs the actual encryption and decryption of data on the user database. Amazon RDS backs up and manages the database master key and the TDE certificate. To comply with several security standards, Amazon RDS is working to implement automatic periodic master key rotation.\r\n\r\n&nbsp;\r\n\r\nTDE encrypts the actual data and log files at the database level. It automatically encrypts data before it is written to storage and automatically decrypts data when it is read from storage. Lastly, TDE provides encryption at the data level, protecting the data itself from unauthorized access.\r\n\r\nHence, the correct answer is to **Enable Transparent Data Encryption (TDE)***.*\r\n\r\nThe option that says: **Use IAM DB Authentication** is incorrect because this option just lets you authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. The more appropriate security feature to use here is TDE.\r\n\r\nThe option that says: **Enable RDS Encryption** is incorrect. This option enables encryption at rest for the underlying storage volumes, but it does not automatically encrypt and decrypt the data itself. It relies on the database engine (in this case, SQL Server) to handle the encryption and decryption of the actual data.\r\n\r\nThe option that says: **Use Microsoft SQL Server Windows Authentication** is incorrect because this option is primarily used if you want to integrate RDS with your AWS Directory Service for Microsoft Active Directory (also called AWS Managed Microsoft AD) to enable Windows Authentication to authenticate users.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.TDE.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html"
  },
  {
    "id": 84,
    "question": "A developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed is made in Python with just 3 lines of codes which can be written inline in the template.\r\n\r\nWhich parameter of the `AWS::Lambda::Function` resource should the developer use to place the Python code in the template?",
    "options": [
      "CodeUri",
      "ZipFile",
      "Handler",
      "Code"
    ],
    "question_type": "single",
    "correct_answers": [
      "ZipFile"
    ],
    "correct_answer": "ZipFile",
    "explanation": "To create a Lambda function you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any dependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the appropriate security permissions for the zip package.\r\n\r\nIf you are using a CloudFormation template, you can configure the `AWS::Lambda::Function` resource which creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing.\r\n\r\n&nbsp;\r\n\r\nUnder the `AWS::Lambda::Function` resource, you can use the `Code` property which contains the deployment package for a Lambda function. For all runtimes, you can specify the location of an object in Amazon S3.\r\n\r\nFor Node.js and Python functions, you can specify the function code inline in the template. Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, change the object key or version in the template.\r\n\r\nHence, the `ZipFile` parameter to is the correct one to be used in this scenario, which will allow the developer to place the python code inline in the template. If you include your function source inline with this parameter, AWS CloudFormation places it in a file named index and *zips* it to create a deployment package. This is the reason why it is called the \u201c`ZipFile\"` parameter, and not because it accepts zip files.\r\n\r\nThe **`Handler`** parameter is incorrect because this is not a valid property of `AWS::Lambda::Function` resource but of the `AWS::Serverless::Function` resource in AWS SAM. In addition, this parameter is primarily used to specify the name of the handler, which is just a function in your code that AWS Lambda can invoke when the service executes your code.\r\n\r\nThe **`Code`** parameter is incorrect because you should use the `ZipFile` parameter instead. Take note that the `Code` property is the parent property of the `ZipFile` parameter.\r\n\r\nThe **`CodeUri`** parameter is incorrect because this is not a valid property of `AWS::Lambda::Function` resource but of the `AWS::Serverless::Function` resource in AWS SAM. This parameter accepts the S3 URL of your code and not the actual code itself.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html"
  },
  {
    "id": 85,
    "question": "Your Lambda function initializes a lot of external dependencies such as database connections and HTTP endpoints, which are required for data processing. It also fetches static data with a size of 20 MB from a third-party provider over the Internet every time the function is invoked. This adds significant time in the total processing, which greatly affects the performance of their serverless application.\r\n\r\nWhich of the following should you do to improve the performance of your function?",
    "options": [
      "Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory.",
      "Use unreserved concurrency for your function.",
      "Allocate more memory to your function.",
      "Increase the CPU allocation of the function by submitting a service limit increase ticket to AWS."
    ],
    "question_type": "single",
    "correct_answers": [
      "Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory."
    ],
    "correct_answer": "Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory.",
    "explanation": "When AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you create a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to allow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration settings you provide. The execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to \u201ccold-start\u201d or initialize those external dependencies, as explained below.\r\n\r\nIt takes time to set up an execution context and do the necessary \u201cbootstrapping\u201d, which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function.\r\n\r\n&nbsp;\r\n\r\nAfter a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes and thaws the context for reuse if AWS Lambda chooses to reuse the context when the Lambda function is invoked again.\r\n\r\nEach execution context provides **512 MB \u2013 10,240 MB** of additional disk space in the `/tmp` directory. The directory content remains when the execution context is frozen, providing a transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored.\r\n\r\nHence, the correct answer in this scenario is: **Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the `/tmp` directory.**\r\n\r\nThe option that says: **Increase the CPU allocation of the function by submitting a service limit increase ticket to AWS** is incorrect because, in the first place, you cannot do that in AWS. In the AWS Lambda resource model, you choose the amount of memory you want for your function, which will then automatically allocate proportional CPU power to your function. An increase in memory size triggers an equivalent increase in CPU available to your function. This is the proper way to increase the CPU allocation and not by submitting a support ticket. In addition, the root cause of this issue is not the CPU nor the memory, but the 20 MB file that is always downloaded by your function.\r\n\r\nThe option that says: **Allocate more memory to your function** is incorrect because this will just increase the amount of memory available to the function during execution and not solve the underlying issue. The actual processing time may be reduced by having more memory but there is still a lot of time wasted in downloading the 20 MB file every time the function is invoked.\r\n\r\nThe option that says: **Use unreserved concurrency for your function** is incorrect because the issue does not relate to concurrency. Just as mentioned above, the root cause is that the function downloads a large file every time it is invoked, which causes significant delays and time-outs.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html\r\n\r\n[https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration](https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html)"
  },
  {
    "id": 86,
    "question": "You are planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.\r\n\r\nWhich of the following options is the MOST appropriate method use to meet this requirement?",
    "options": [
      "Lambda proxy integration",
      "HTTP Proxy integration",
      "Lambda custom integration",
      "HTTP custom integration"
    ],
    "question_type": "single",
    "correct_answers": [
      "Lambda custom integration"
    ],
    "correct_answer": "Lambda custom integration",
    "explanation": "You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have two types of integration:\r\n\r\n\u2013 Lambda proxy integration\r\n\r\n\u2013 Lambda custom integration\r\n\r\nIn Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration\u2019s HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf.\r\n\r\nIn Lambda non-proxy (or custom) integration, in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.\r\n\r\nFor an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request.\r\n\r\nThe Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the [function-invoking action](https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html) of the Lambda service.\r\n\r\n&nbsp;\r\n\r\nThe Lambda custom integration is a type of integration that lets an API expose AWS service actions. In `AWS` integration, you must configure both the integration request and integration response and set up necessary data mappings from the method request to the integration request, and from the integration response to the method response. To configure your API Gateway with this type of configuration, you have to set the resource with an AWS integration type.\r\n\r\nHence, **Lambda custom integration** is correct as it matches the description depicted in the scenario.\r\n\r\n**Lambda proxy integration** is incorrect as this type of integration is the one where you do **not** have to configure both the integration request and integration response.\r\n\r\n**HTTP custom integration** is incorrect because this type is only used where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Take note that the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead.\r\n\r\n**HTTP proxy integration** is incorrect because the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format"
  },
  {
    "id": 87,
    "question": "An e-commerce application, which is hosted in an ECS Cluster, contains the connection string of an external database and other sensitive configuration files. Since the application accepts credit card payments, the company has to meet strict security compliance which requires that the database credentials are encrypted and periodically rotated.\r\n\r\nWhich of the following should you do to comply to the requirements?",
    "options": [
      "Store the database credentials in AWS Secrets Manager and enable rotation.",
      "Store the database credentials in an encrypted ecs.config configuration file.",
      "Store the database credentials as a secure string parameter in Systems Manager Parameter Store.",
      "Store the database credentials in an encrypted dockerrun.aws.json configuration file."
    ],
    "question_type": "single",
    "correct_answers": [
      "Store the database credentials in AWS Secrets Manager and enable rotation."
    ],
    "correct_answer": "Store the database credentials in AWS Secrets Manager and enable rotation.",
    "explanation": "**Amazon ECS** enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.\r\n\r\nSecrets can be exposed to a container in the following ways:\r\n\r\n\u2013 To inject sensitive data into your containers as environment variables, use the `secrets` container definition parameter.\r\n\r\n\u2013 To reference sensitive information in the log configuration of a container, use the `secretOptions` container definition parameter.\r\n\r\n&nbsp;\r\n\r\nYou can configure AWS Secrets Manager to automatically rotate the secret for a secured service or database. Secrets Manager already natively knows how to rotate secrets for supported Amazon RDS databases. However, Secrets Manager also can enable you to rotate secrets for other databases or third-party services. Because each service or database can have a unique way of configuring its secrets, Secrets Manager uses a Lambda function that you can customize to work with whatever database or service that you choose. You customize the Lambda function to implement the service-specific details of how to rotate a secret.\r\n\r\n**AWS Secrets Manager** is the correct answer for this scenario because it can provide both the required encryption as well as the ability to periodically rotate the secrets.\r\n\r\n**Storing the database credentials as a secure string parameter in Systems Manager Parameter Store** is incorrect. Although this service can encrypt your sensitive database credentials, it doesn\u2019t have the capability to periodically rotate your secrets, unlike AWS Secrets Manager.\r\n\r\n**Storing the database credentials in an encrypted `ecs.config` configuration file** is incorrect because this file is primarily used to store the environment variables of the Amazon ECS container agent.\r\n\r\n**Storing the database credentials in an encrypted `dockerrun.aws.json` configuration file** is incorrect because this file is just an Elastic Beanstalk\u2013specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. This is primarily used in a multicontainer Docker environment and it is not suitable for storing sensitive database credentials, which requires periodic rotation.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-securestring.html\r\n\r\nhttps://aws.amazon.com/systems-manager/faq/"
  },
  {
    "id": 88,
    "question": "A web application hosted in Elastic Beanstalk has a configuration file named `.ebextensions/debugging.config` which has the following content:\r\n\r\n`option_settings:aws:elasticbeanstalk:xray:XRayEnabled: true`\r\n\r\nFor its database tier, it uses RDS with Multi-AZ deployments configuration and Read Replicas. There is a new requirement to record calls that your application makes to RDS and other internal or external HTTP web APIs. The tracing information should also include the actual SQL database queries sent by the application, which can be searched using the filter expressions in the X-Ray Console.\r\n\r\nWhich of the following should you do to satisfy the above task?",
    "options": [
      "Add annotations in the subsegment section of the segment document.",
      "Add annotations in the segment document.",
      "Add metadata in the segment document.",
      "Add metadata in the subsegment section of the segment document."
    ],
    "question_type": "single",
    "correct_answers": [
      "Add annotations in the subsegment section of the segment document."
    ],
    "correct_answer": "Add annotations in the subsegment section of the segment document.",
    "explanation": "Even with sampling, a complex application generates a lot of data. The AWS X-Ray console provides an easy-to-navigate view of the service graph. It shows health and performance information that helps you identify issues and opportunities for optimization in your application. For advanced tracing, you can drill down to traces for individual requests, or use **filter expressions** to find traces related to specific paths or users.\r\n\r\n&nbsp;\r\n\r\nWhen you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata.\r\n\r\n**Annotations** are simple key-value pairs that are indexed for use with [filter expressions](https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html). Use annotations to record data that you want to use to group traces in the console or when calling the [`GetTraceSummaries`](https://docs.aws.amazon.com/xray/latest/api/API_GetTraceSummaries.html) API. X-Ray indexes up to 50 annotations per trace.\r\n\r\n**Metadata** are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don\u2019t need to use for searching traces. You can view annotations and metadata in the segment or subsegment details in the X-Ray console.\r\n\r\nA trace segment is a JSON representation of a request that your application serves. A trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases.\r\n\r\nHence, **adding annotations in the subsegment section of the segment document** is the correct answer.\r\n\r\n**Adding annotations in the segment document** is incorrect. Although the use of annotations is correct, you have to add this in the ***subsegment*** section of the *segment* document since you want to trace the downstream call to RDS and not the actual request to your application.\r\n\r\n**Adding metadata in the segment document** is incorrect because metadata is primarily used to record custom data that you want to store in the trace but not for searching traces since this can\u2019t be picked up by filter expressions in the X-Ray Console. You have to use annotations instead. In addition, you have to add this in the *subsegment* section of the *segment* document since you want to trace the downstream call to RDS and not the actual request to your application.\r\n\r\n**Adding metadata in the subsegment section of the segment document** is incorrect because, just as mentioned above, metadata is just used to record custom data that you want to store in the trace but not for searching traces.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html"
  },
  {
    "id": 89,
    "question": "A software development company uses AWS CodePipeline as its CI/CD platform to build, test, and push deployments to its production environment. Recently, a developer created a Lambda function that will push the build details to a separate DynamoDB table. The Lambda function should be triggered after a successful build on the Pipeline.\r\n\r\nWhich of the following services will meet the specified requirement?",
    "options": [
      "AWS CodeBuild",
      "AWS Systems Manager",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "AWS CloudTrail Events"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon EventBridge (Amazon CloudWatch Events)"
    ],
    "correct_answer": "Amazon EventBridge (Amazon CloudWatch Events)",
    "explanation": "Monitoring is an important part of maintaining the reliability, availability, and performance of AWS CodePipeline. You should collect monitoring data from all parts of your AWS solution so that you can more easily debug a multi-point failure if one occurs.\r\n\r\nYou can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as Lambda functions and Simple Notification Service (SNS) topics.\r\n\r\n&nbsp;\r\n\r\nDepending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.\r\n\r\nHence, the correct answer is **Amazon EventBridge (Amazon CloudWatch Events).**\r\n\r\n**AWS CloudTrail Events** is incorrect because this is just an event in CloudTrail that records activity in your AWS account, which can be an action taken by a user, role, or service that is monitorable by CloudTrail. A more suitable solution is to use Amazon EventBridge (Amazon CloudWatch Events) instead.\r\n\r\n**AWS Systems Manager** is incorrect because it is primarily used for managing and configuring AWS resources, including EC2 instances, databases, and more. It\u2019s not designed for event-driven automation based on state changes in AWS resources.\r\n\r\n**AWS CodeBuild** is incorrect because it cannot trigger a Lambda function directly. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. You should use Amazon EventBridge (Amazon CloudWatch Events) for this scenario instead.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
  },
  {
    "id": 90,
    "question": "You are configuring the task definitions of your ECS Cluster in AWS to make sure that the tasks are scheduled on instances with enough resources to run them. It should also follow the constraints that you specified both implicitly or explicitly.\r\n\r\nWhich of the following options should you implement to satisfy the requirement which requires the LEAST amount of configuration?",
    "options": [
      "Use a random task placement strategy.",
      "Use a binpack task placement strategy.",
      "Use a spread task placement strategy with custom placement constraints.",
      "Use a spread task placement strategy which uses the instanceId and host attributes."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use a random task placement strategy."
    ],
    "correct_answer": "Use a random task placement strategy.",
    "explanation": "By default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones.\r\n\r\nA **task placement strategy** is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.\r\n\r\nAmazon ECS supports the following task placement strategies:\r\n\r\n**binpack** \u2013 Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.\r\n\r\n**random** \u2013 Place tasks randomly.\r\n\r\n**spread** \u2013 Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, *instanceId*, or *host*.\r\n\r\nThe *Random* task placement strategy is fairly straightforward as it doesn\u2019t require further parameters. The two other strategies, such as binpack and spread, take opposite actions. Binpack places tasks on as few instances as possible, helping to optimize resource utilization, while spread places tasks evenly across your cluster to help maximize availability. By default, ECS uses spread with the *ecs.availability-zone* attribute to place tasks.\r\n\r\n*Random* places tasks on instances at random yet still honors the other constraints that you specified, implicitly or explicitly. Specifically, it still makes sure that tasks are scheduled on instances with enough resources to run them.\r\n\r\nHence, the correct answer is to **use a `random` task placement strategy** for this scenario.\r\n\r\n**Using a `binpack` task placement strategy** is incorrect because this configuration will place the tasks based on the least available amount of CPU or memory. There are also additional configuration steps where you need to specify the type of field that ECS would be using such as `CPU` or `memory`.\r\n\r\n**Using a `spread` task placement strategy which uses the `instanceId` and `host` attributes** is incorrect because this entails a lot of configuration as compared to using the *Random* task placement strategy type.\r\n\r\n**Using a `spread` task placement strategy with custom placement constraints** is incorrect because a *task placement constraint* is just a rule that is considered during task placement.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html"
  },
  {
    "id": 91,
    "question": "A company is currently in the process of integrating their on-premises data center to their cloud infrastructure in AWS. One of the requirements is to integrate the on-premises Lightweight Directory Access Protocol (LDAP) directory service to their AWS VPC using IAM.\r\n\r\nWhich of the following provides the MOST suitable solution to implement if the identity store that they are using is not compatible with SAML?",
    "options": [
      "Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials.",
      "Create IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.",
      "Set up an IAM policy that references the LDAP identifiers and AWS credentials.",
      "Implement the AWS IAM Identity Center service to manage access between AWS and your LDAP."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials."
    ],
    "correct_answer": "Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials.",
    "explanation": "If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources.\r\n\r\nThe application verifies that employees are signed into the existing corporate network\u2019s identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees.\r\n\r\nTo get temporary security credentials, the identity broker application calls either **`AssumeRole`** or **`GetFederationToken`** to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire. The call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token. The identity broker application makes these temporary security credentials available to the internal company application. The app can then use the temporary credentials to make calls to AWS directly. The app caches the credentials until they expire, and then requests a new set of temporary credentials.\r\n\r\nHence, the correct answer is: **C****reate a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials**.\r\n\r\nThe option that says: **Setting up an IAM policy that references the LDAP identifiers and AWS credentials** is incorrect because using an IAM policy is not enough to integrate your LDAP service into IAM. You need to use SAML, STS, or a custom identity broker instead.\r\n\r\nThe option that says: **Implementing the AWS IAM Identity Center service to manage access between AWS and your LDAP** is incorrect because the identity store that you are using is not SAML-compatible. AWS IAM Identity Center does not support non-SAML authentication methods.\r\n\r\nThe option that says: **Creating IAM roles to rotate the IAM credentials whenever LDAP credentials are updated** is incorrect because manually rotating the IAM credentials is not an optimal solution to integrate your on-premises and VPC network. You need to use SAML, STS, or a custom identity broker.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/"
  },
  {
    "id": 92,
    "question": "A startup has recently launched their new mobile game and is gaining a lot of new users everyday. The founders plan to add a new feature which will enable cross-device syncing of user profile data across mobile devices to improve the user experience.\r\n\r\nWhich of the following services should they use to meet this requirement?",
    "options": [
      "Cognito Sync",
      "Cognito User Pools",
      "AWS Amplify",
      "Cognito Identity Pools"
    ],
    "question_type": "single",
    "correct_answers": [
      "Cognito Sync"
    ],
    "correct_answer": "Cognito Sync",
    "explanation": "**Amazon Cognito Sync** is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.\r\n\r\nAmazon Cognito lets you save end user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user\u2019s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.\r\n\r\nThe Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize.\r\n\r\nAmazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that, whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.\r\n\r\nHence, the correct answer is to use **Cognito Sync***.*\r\n\r\n**Cognito User Pools** is incorrect because this is just a user directory that allows your users to sign in to your web or mobile app through Amazon Cognito.\r\n\r\n**Cognito Identity Pools** is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\n**AWS Amplify** is incorrect because this just makes it easy for you to create, configure, and implement scalable mobile and web apps powered by AWS. It does not have the ability to synchronize user profile data across mobile devices.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html"
  },
  {
    "id": 93,
    "question": "You are a software developer for a multinational investment bank which has a hybrid cloud architecture with AWS. To improve the security of their applications, they decided to use AWS Key Management Service (KMS) to create and manage their encryption keys across a wide range of AWS services. You were given the responsibility to integrate AWS KMS with the financial applications of the company.\r\n\r\nWhich of the following are the recommended steps to locally encrypt data using AWS KMS that you should follow? (Select TWO.)",
    "options": [
      "Encrypt data locally using the Encrypt operation.",
      "Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.",
      "Erase the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.",
      "Use the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.",
      "Use the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.",
      "Use the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally."
    ],
    "correct_answer": "Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.",
    "explanation": "When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. *Envelope encryption* is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.\r\n\r\nYou can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext encryption key is known as the *root key*.\r\n\r\n&nbsp;\r\n\r\n**AWS KMS** helps you to protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS.\r\n\r\nIt is recommended that you use the following pattern to encrypt data locally in your application:\r\n\r\n1. Use the `GenerateDataKey` operation to get a data encryption key.\r\n\r\n2. Use the plaintext data key (returned in the `Plaintext` field of the response) to encrypt data locally, then erase the plaintext data key from memory.\r\n\r\n3. Store the encrypted data key (returned in the `CiphertextBlob` field of the response) alongside the locally encrypted data.\r\n\r\nHence, the valid steps in this scenario are the following:\r\n\r\n**\u2013 Use the `GenerateDataKey` operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.**\r\n\r\n**\u2013 Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.**\r\n\r\nThe option that says: **Use the `GenerateDataKeyWithoutPlaintext` operation to get a data encryption key then using the plaintext data key in the response to encrypt data locally** is incorrect because you have to use the `GenerateDataKey` operation instead. This is because the `GenerateDataKeyWithoutPlaintext` operation will not return the plaintext data key just as its name implies.\r\n\r\nThe option that says: **Erase the encrypted data key from memory and storing the plaintext data key alongside the locally encrypted data** is incorrect because it should be the other way around. You have to erase the **plaintext** data key from memory and store the **encrypted** data key alongside the locally encrypted data.\r\n\r\nThe option that says: **Encrypt data locally using the `Encrypt` operation** is incorrect because the `Encrypt` operation is primarily used to encrypt RSA keys, database passwords, or other sensitive information. This operation can also be used to move encrypted data from one AWS region to another; however, this is not recommended if you want to encrypt your data locally. You have to use the `GenerateDataKey` operation instead.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping"
  },
  {
    "id": 94,
    "question": "A development team has recently completed building their serverless application. They must zip their code artifacts, upload them to Amazon S3, produce the package template file for deployment, and deploy it to AWS.\r\n\r\nWhich command is the MOST suitable to use to automate the deployment steps?",
    "options": [
      "sam publish",
      "aws cloudformation deploy",
      "sam deploy",
      "sam package"
    ],
    "question_type": "single",
    "correct_answers": [
      "sam deploy"
    ],
    "correct_answer": "sam deploy",
    "explanation": "**AWS SAM** uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.\r\n\r\nAfter you develop and test your serverless application locally, you can package and deploy your application by using the `sam deploy` command.\r\n\r\nThe `sam deploy` command zips your code artifacts, uploads them to Amazon S3, and produces a packaged AWS SAM template file that it uses to deploy your application.\r\n\r\nTo deploy an application that contains one or more nested applications, you must include the `CAPABILITY_AUTO_EXPAND` capability in the `sam deploy` command.\r\n\r\nHence, the correct answer is: **`sam deploy`**\r\n\r\n`aws cloudformation deploy` is incorrect. While this command can be used to deploy a CloudFormation stack, it expects that your artifacts are already packaged and uploaded to S3. It doesn\u2019t handle the packaging process implicitly.\r\n\r\n**`sam package`** is incorrect. This command simply prepares the serverless application for deployment by zipping artifacts, uploading them to S3, and generating a CloudFormation template with references to the uploaded artifacts in S3. It doesn\u2019t deploy the application.\r\n\r\n`sam publish` is incorrect because this command publishes an AWS SAM application to the AWS Serverless Application Repository and does not generate the template file. It takes a packaged AWS SAM template and publishes the application to the specified region.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\r\n\r\nhttps://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html"
  },
  {
    "id": 95,
    "question": "You were recently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which already contains a few data. There is already an existing DynamoDB Table that has an attribute of `ArticleName` which acts as the partition key and a `Category` attribute as its sort key. You are instructed to develop a feature that will query the `ArticleName` attribute but will use a different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data.\r\n\r\nWhich of the following solutions should you implement?",
    "options": [
      "Create a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes.",
      "Create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key. Migrate the data from the existing table to the new table.",
      "Create a Local Secondary Index that uses the ArticleName attribute and a different sort key.",
      "Create a Global Secondary Index that uses the ArticleName attribute and a different sort key."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key. Migrate the data from the existing table to the new table."
    ],
    "correct_answer": "Create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key. Migrate the data from the existing table to the new table.",
    "explanation": "A **local secondary index** maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.\r\n\r\nSuppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to `Scan` the entire *Thread* table and discard any posts that were not within the specified time frame. With a local secondary index, a `Query` operation could use *LastPostDateTime* as a sort key and find the data quickly.\r\n\r\nTo create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as shown below. Then you must select an alternative sort key which is different from the sort key of the table.\r\n\r\nWhen you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent reads are not supported on global secondary indexes.\r\n\r\nThe primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single partition, as specified by the partition key value in the query.\r\n\r\nLocal secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.\r\n\r\nHence, the correct answer in this scenario is to **create a new DynamoDB table with a Local Secondary Index that uses the `ArticleName` attribute with a different sort key then migrate the data from the existing table to the new table**.\r\n\r\n**Creating a Global Secondary Index that uses the `ArticleName` attribute and a different sort key** is incorrect because it is stated in the scenario that you are still using the same partition key, but with an alternate sort key that warrants the use of a local secondary index instead of a global secondary index.\r\n\r\n**Creating a Global Secondary Index which uses the `ArticleName` attribute and your alternative sort key as projected attributes** is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected attributes are just attributes stored in the index that can be returned by queries and scans performed on the index hence, these are not useful in satisfying the provided requirement.\r\n\r\n**Creating a Local Secondary Index that uses the `ArticleName` attribute and a different sort key** is incorrect. Although it uses the correct type of index, you cannot add a local secondary index to an already existing table.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\r\n\r\n**Global Secondary Index vs. Local Secondary Index:**\r\n\r\nhttps://tutorialsdojo.com/global-secondary-index-vs-local-secondary-index/"
  },
  {
    "id": 96,
    "question": "An application is used to upload images to an Amazon S3 bucket. Once an event occurs, a Lambda function is triggered to compress the photos. However, it has been discovered that the processing time of the function is longer than expected.\r\n\r\nWhich change will improve the processing time of the function most effectively?",
    "options": [
      "Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image.",
      "Increase the memory allocation of the function.",
      "Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application\u2019s latency.",
      "Increase the timeout value of the function."
    ],
    "question_type": "single",
    "correct_answers": [
      "Increase the memory allocation of the function."
    ],
    "correct_answer": "Increase the memory allocation of the function.",
    "explanation": "Allocating more memory to a Lambda function also increases the amount of CPU, network, and other resources allocated to it. By provisioning more memory, you can improve the performance and speed of your function while potentially reducing your costs. You should benchmark your use case to determine where the breakeven point is for running faster and using more memory vs running slower and using less memory.\r\n\r\nIn the scenario, by increasing the memory allocation of the function, the CPU power and network throughput available to the function are also increased, which can speed up the execution of the function and result in faster processing times for the images.\r\n\r\nHence, the correct answer is: **Increase the memory allocation of the function.**\r\n\r\nThe option that says: **Increase the timeout value of the function** is incorrect. This can provide more time for the function to execute before it times out, which may be useful if the function is being terminated prematurely. However, simply increasing the timeout value may not necessarily improve the processing time of the function.\r\n\r\nThe option that says: **Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image** is incorrect. Adding an SQS queue to the solution won\u2019t necessarily make the Lambda function run faster but can improve the overall fault tolerance of the system.\r\n\r\nThe option that says: **Run the function with Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) which will run the code closer to the users of your application, reducing your application\u2019s latency** is incorrect. This may improve latency, but it may not necessarily improve the processing time of the function. Additionally, this solution will require you to set up a CloudFront distribution, which introduces additional costs.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html"
  },
  {
    "id": 97,
    "question": "A developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the function to return the corresponding log location of an invocation request.\r\n\r\nWhich approach should the developer take with the least amount of effort?",
    "options": [
      "Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results.",
      "Extract the log stream name from the Context object of the handler function.",
      "Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the request id to filter results.",
      "Extract the log stream name from the Event object of the handler function."
    ],
    "question_type": "single",
    "correct_answers": [
      "Extract the log stream name from the Context object of the handler function."
    ],
    "correct_answer": "Extract the log stream name from the Context object of the handler function.",
    "explanation": "When Lambda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information about the invocation, function, and execution environment. One of the properties that you can get from the context object is the **log_stream_name** which gives the log location of a function instance.  \r\nwe can easily retrieve the corresponding log stream of a request by returning the value of **context.log_stream_name.** For the full list of context methods and properties, see this [link](https://docs.aws.amazon.com/lambda/latest/dg/python-context.html).\r\n\r\nHence, the correct answer is: **Extract the log stream name from the `Context` object of the handler function.**\r\n\r\nThe option that says: **Extract the invocation request id from the `Context` object of the handler function. Then, call the `FilterLogEvents` API and pass the request id to filter results** is incorrect because this adds unnecessary steps to meet the requirement. The log stream name is directly available in the Context object.\r\n\r\nThe following options can be eliminated because the log stream name and request-id are not properties of the event object:\r\n\r\n**\u2013 Extract the log stream name from the `Event` object of the handler function.**\r\n\r\n**\u2013 Extract the invocation request id from the `Event` object of the handler function. Call the `FilterLogEvents` API and use the request id to filter results.**\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/python-handler.html"
  },
  {
    "id": 98,
    "question": "A developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users using the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the browser\u2019s local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint.\r\n\r\nWhat are the steps needed to implement the scenario above? (Select THREE.)",
    "options": [
      "Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.",
      "Create an Amazon Cognito User Pool.",
      "Create an Amazon Cognito Identity Pool.",
      "On the API Gateway Console, create an authorizer using the Cognito User Pool ID.",
      "Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.",
      "Choose and set the authentication provider for your website."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.",
      "Create an Amazon Cognito User Pool.",
      "On the API Gateway Console, create an authorizer using the Cognito User Pool ID."
    ],
    "correct_answer": "Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.",
    "explanation": "As an alternative to using IAM roles and policies or **Lambda Authorizers** (formerly known as custom authorizers), you can use an **Amazon Cognito User Pool** to control who can access your API in Amazon API Gateway.\r\n\r\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the `COGNITO_USER_POOLS` type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request\u2019s Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn\u2019t authorized to make the call because the client did not have credentials that could be authorized.\r\n\r\nHence, the correct steps to implement the solution are as follows:\r\n\r\n**\u2013 Create an Amazon Cognito User Pool.**\r\n\r\n**\u2013 On the API Gateway Console, create an authorizer using the Cognito User Pool ID.**\r\n\r\n**\u2013 Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization**.\r\n\r\nThe option that says: **Create an Amazon Cognito Identity Pool** is incorrect because you can not use Cognito Identity Pool as an authorizer for API Gateway. The only valid authorizers for API Gateway are AWS Lambda and Amazon Cognito User Pool.\r\n\r\nThe option that says: **Choose and set the authentication provider for your website** is incorrect because this step is done during the creation of a Cognito Identity Pool.\r\n\r\nThe option that says: **Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization** is incorrect because Cognito Identity Pool cannot be used as an authorizer for API Gateway. You should use the Cognito User Pool.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\r\n\r\n**Check out this Amazon Cognito Cheat Sheet:**\r\n\r\nhttps://tutorialsdojo.com/amazon-cognito/\r\n\r\nhttps://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/"
  },
  {
    "id": 99,
    "question": "A developer is building an application that uses Amazon CloudFront to distribute thousands of images stored in an S3 bucket. The developer needs a fast and cost-efficient solution that will allow him to update the images immediately without waiting for the object\u2019s expiration date.\r\n\r\nWhich solution meets the requirements?",
    "options": [
      "Disable the CloudFront distribution and re-enable it to update the images in all edge locations.",
      "Update the images by invalidating them from the edge caches.",
      "Update the images by using versioned file names.",
      "Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes."
    ],
    "question_type": "single",
    "correct_answers": [
      "Update the images by using versioned file names."
    ],
    "correct_answer": "Update the images by using versioned file names.",
    "explanation": "When you update existing files in a CloudFront distribution, AWS recommends that you include some sort of version identifier either in your file names or in your directory names to give yourself better control over your content. This identifier might be a date-time stamp, a sequential number, or some other method of distinguishing two versions of the same object.\r\n\r\nFor example, instead of naming a graphic file image.jpg, you might call it image_1.jpg. When you want to start serving a new version of the file, you\u2019d name the new file image_2.jpg, and you\u2019d update the links in your web application or website to point to image_2.jpg. Alternatively, you might put all graphics in an images_v1 directory and, when you want to start serving new versions of one or more graphics, you\u2019d create a new images_v2 directory, and you\u2019d update your links to point to that directory. With versioning, you don\u2019t have to wait for an object to expire before CloudFront begins to serve a new version of it, and you don\u2019t have to pay for object invalidation.\r\n\r\nHence, the correct answer is: **Update the images by using versioned file names.**\r\n\r\nThe option that says: **Update the images by invalidating them from the edge caches** is incorrect. While this will update the images, it is not a cost-efficient solution as you have to pay for the additional invalidation requests.\r\n\r\nThe option that says: **Disable the CloudFront distribution and re-enable it to update the images in all edge locations** is incorrect. CloudFront distributes files to edge locations only when the files are requested, not when you put new or updated files in your origin. Therefore, this is not the proper approach to update the images.\r\n\r\nThe option that says: **Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes** is incorrect. This a time-consuming solution as you have to wait for the objects to expire just to have them updated.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/simplified-multiple-object-invalidation-for-amazon-cloudfront/"
  },
  {
    "id": 100,
    "question": "A developer is managing an Application Load Balancer that targets a Lambda function. The developer needs to obtain all values of identical query parameters key that is supplied in a request.\r\n\r\nHow can the developer implement this?",
    "options": [
      "Set a custom HTTP response header in the Lambda function.",
      "Decode the URL encoded query string values in the Lambda function.",
      "Enable the multi-value headers on the Application Load Balancer.",
      "Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable the multi-value headers on the Application Load Balancer."
    ],
    "correct_answer": "Enable the multi-value headers on the Application Load Balancer.",
    "explanation": "**Application Load Balancers** provide two advanced options that you may want to configure when you use ALBs with AWS Lambda: support for multi-value headers and health check configurations. You can set up these options in `Target Groups` section on the Amazon EC2 console.\r\n\r\nIf requests from a client or responses from a Lambda function contain headers with multiple values or contains the same header multiple times or query parameters with multiple values for the same key, you can enable support for multi-value header syntax. After you enable multi-value headers, the headers and query parameters exchanged between the load balancer and the Lambda function use arrays instead of strings.\r\n\r\nFor example, suppose the client supplies a query string like:\r\n\r\n`?name=foo&name=bar`\r\n\r\nIf you\u2019ve enabled multi-value headers, ALB supplies these duplicate parameters in the `event` object as:\r\n\r\n`\u2018name\u2019: [\u2018foo\u2019, \u2018bar\u2019]`\r\n\r\nALB applies the same processing to duplicate HTTP headers.\r\n\r\nIf you do not enable multi-value header syntax and a header or query parameter has multiple values, the load balancer uses the last value that it receives.\r\n\r\nHence, the correct answer is: **Enable the multi-value headers on the Application Load Balancer.**\r\n\r\nThe option that says: **Set a custom HTTP response header in the Lambda function** is incorrect because this can only be done when integrated with API Gateway.\r\n\r\nThe option that says: **Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers** is incorrect. The Network Load Balancer does not support Lambda functions as a target type.\r\n\r\nThe option that says: **Decode the URL encoded query string values in the Lambda function** is incorrect. This won\u2019t change anything. The load balancer will still use the last value of the query parameter it receives regardless if its URL is encoded or not.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/\r\n\r\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html"
  },
  {
    "id": 101,
    "question": "An application uses the `PutObject` operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet security compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3 bucket. There is a noticeable performance degradation after making the change.\r\n\r\nWhich of the following is the most likely cause of the problem?",
    "options": [
      "The API request rate has exceeded the quota for AWS KMS API operations.",
      "The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS.",
      "The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.",
      "The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS)."
    ],
    "question_type": "single",
    "correct_answers": [
      "The API request rate has exceeded the quota for AWS KMS API operations."
    ],
    "correct_answer": "The API request rate has exceeded the quota for AWS KMS API operations.",
    "explanation": "**AWS KMS** establishes quotas for the number of API operations requested in each second.\r\n\r\nYou can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies to both kinds of requests.\r\n\r\nFor example, you might store data in Amazon S3 using server-side encryption with AWS KMS (SSE-KMS). Each time you upload or download an S3 object that\u2019s encrypted with SSE-KMS, Amazon S3 makes a `GenerateDataKey` (for uploads) or `Decrypt` (for downloads) request to AWS KMS on your behalf. These requests count toward your quota, so AWS KMS throttles the requests if you exceed a combined total of 5,500 (or 10,000 or 30,000 depending upon your AWS Region) uploads or downloads per second of S3 objects encrypted with SSE-KMS.\r\n\r\nHence, the correct answer is: **The API request rate has exceeded the quota for AWS KMS API operations.**\r\n\r\nThe option that says: **The Amazon S3 throttles the `PutObject` operation for objects encrypted with SSE-KMS** is incorrect because Amazon S3 can automatically scale to high request rates with or without server-side encryption through parallelization.\r\n\r\nThe option that says: **The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead** is incorrect. While it is true that AES 256 is technically slower than AES 128 (because it has a larger key size), the performance difference is hardly noticeable. That being said, this is unlikely to be the cause of the problem.\r\n\r\nThe option that says: **The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS)** is incorrect because an alias is simply a reference name that points to a key. It is not required for using a KMS key in SSE-KMS. The key can be referenced directly by its ARN or an alias, but the absence of an alias wouldn\u2019t cause performance degradation.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html#rps-from-service\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html"
  },
  {
    "id": 102,
    "question": "An application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region using the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a template.\r\n\r\nWhich of the following actions is the most suitable way to configure the CloudFormation template for the scenario?",
    "options": [
      "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key.",
      "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key.",
      "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key.",
      "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key."
    ],
    "question_type": "single",
    "correct_answers": [
      "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key."
    ],
    "correct_answer": "Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key.",
    "explanation": "The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the `Fn::FindInMap` intrinsic function to retrieve values in a map.\r\n\r\nHence, the correct answer is:\r\n\r\n**Copy the AMI of the instance from the us-east-1 region to the** **us-east-2****,** **ap-northeast-1****, and** **ap-southeast-1** **region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the `Fn::FindInMap` function to retrieve the desired Image Id from the region key**.\r\n\r\nThe option that says: **Copy the AMI of the instance from the us-east-1 region to the** **us-east-2****,** **ap-northeast-1****, and** **ap-southeast-1** **region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the `Fn::ImportValue` function to retrieve the desired Image Id from the region key** is incorrect because the `Fn::ImportValue` function is just used to return the value of an output exported by another stack. It can\u2019t be used to retrieve values from a Mappings section.\r\n\r\nThe option that says: **Copy the AMI of the instance from the us-east-1 region to the** **us-east-2****,** **ap-northeast-1****, and** **ap-southeast-1** **region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the `Fn::GetAtt` function to retrieve the desired Image Id from the region key** is incorrect because the `Fn::GetAtt` function is simply used to return the value of an attribute from a resource in the template and not in a Mappings section.\r\n\r\nThe option that says: **Copy the AMI of the instance from the us-east-1 region to the** **us-east-2****,** **ap-northeast-1****, and** **ap-southeast-1** **region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the `Ref`** **function to retrieve the desired Image Id from the region key** is incorrect because the Parameters section is mainly used to declare values within a specified parameter. For example, you can specify the allowed Amazon EC2 instance type for the stack to use when you create or update the stack. Although you can specify the values for the Image Id in the Parameters section, it does not give you the flexibility to map the Image Ids according to its correct region. The Mappings section is more suited for this type of use case.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html"
  },
  {
    "id": 103,
    "question": "A startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to receive webhook requests, which will be processed by a Lambda function.\r\n\r\nGiven that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda function executes the domain logic only when a webhook request comes from a valid user.\r\n\r\nWhich action would satisfy the requirement with the least amount of development effort?",
    "options": [
      "Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.",
      "Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"AWS_IAM\" condition is present.",
      "Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers.",
      "Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:CodeSigningConfigArn\": \"arn:aws:lambda:::code-signing-config:csc-\" condition is present."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers."
    ],
    "correct_answer": "Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.",
    "explanation": "If you need a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional services besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook handler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming requests.\r\n\r\nBy using Lambda function URLs, you can directly invoke your Lambda function using a simple HTTPS request without needing to set up and configure additional services like API Gateway. This approach can be a simple and efficient way to handle incoming requests and integrate with other services or third-party platforms that require a publicly accessible HTTPS endpoint.\r\n\r\nThere are two types of authorization available for Lambda function URLs:\r\n\r\n**AWS_IAM** \u2013 the function URL can only be invoked by an IAM user or role with the necessary permissions. This can be useful in cases where you need to restrict access to the Lambda function to a specific set of users or roles within your organization.\r\n\r\n**NONE** \u2013 anyone can invoke the Lambda function using the URL. This approach can be useful in cases where you want to make the Lambda function publicly accessible and do not require any additional authentication or authorization beyond the URL. However, you may still need to validate the incoming requests in the Lambda function to ensure that the request comes from a trusted source.\r\n\r\nBy setting the \u201clambda:FunctionUrlAuthType\u201d condition to \u201cNONE,\u201d the function will be publicly accessible without requiring any additional authentication. However, you still need to write custom authorization logic to verify the signature provided in the HTTP headers and ensure that the request is coming from a valid user.\r\n\r\nHence the correct answer is: **Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the `\"lambda:FunctionUrlAuthType\": \"NONE\"` condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.**\r\n\r\nThe option that says: **Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers** is incorrect. While it\u2019s a valid solution, this is not the best choice because it involves additional setup and configuration of API Gateway to only invoke a single Lambda function.\r\n\r\nThe option that says: **Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the `\"lambda:FunctionUrlAuthType\": \"AWS_IAM\"` condition is present** is incorrect. This authentication type means that the Lambda function can only be invoked by an authorized IAM user or role. The scenario specifically mentions that each request are signed before they are received by the Lambda function\r\n\r\nThe option that says: **Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the `\"lambda:CodeSigningConfigArn\": \"arn:aws:lambda:<AWS_REGION>:<ACCOUNT_NUMBER>:code-signing-config:csc-<SIGNING_SECRET>\"` condition is present** is incorrect, as code signing is a security feature that verifies the integrity of code running in your Lambda functions and helps ensure that only trusted code is deployed.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/announcing-aws-lambda-function-urls-built-in-https-endpoints-for-single-function-microservices/"
  },
  {
    "id": 104,
    "question": "Several development teams worldwide will be collaboratively working on a project hosted on an AWS Elastic Beanstalk environment. The developers need to be able to deploy incremental code updates without re-uploading the entire project.\r\n\r\nWhich of the following actions will reduce the upload and deployment time with the LEAST amount of effort?",
    "options": [
      "Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.",
      "Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs.",
      "Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk.",
      "Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk."
    ],
    "correct_answer": "Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.",
    "explanation": "**CodeCommit** is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure.\r\n\r\n**AWS CodeCommit** is designed for collaborative software development. You can easily commit, branch, and merge your code allowing you to easily maintain control of your team\u2019s projects. CodeCommit also supports pull requests, which provide a mechanism to request code reviews and discuss code with collaborators. You can create a repository from the AWS Management Console, AWS CLI, or AWS SDKs and start working with the repository using Git.\r\n\r\nYou can use the EB CLI to deploy your application directly from your AWS CodeCommit repository. With CodeCommit, you can upload only your changes to the repository when you deploy, instead of uploading your entire project.\r\n\r\nHence, the correct answer is: **Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.**\r\n\r\nThe option that says: **Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk** is incorrect because this solution entails a lot of effort \u2014 from selecting the AMI to managing the instance, setting up a code repository, and configuring scalability. AWS CodeCommit is already a managed and highly scalable source control service that has native integration with Elastic Beanstalk.\r\n\r\nThe option that says: **Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs** is incorrect. Amazon S3 does not have the functionality to modify stored objects. Incremental code updates are not possible with S3, which defies the requirement for the scenario.\r\n\r\nThe option that says: **Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk** is incorrect. Although EFS is a good use case for concurrent access, it is not a suitable solution for a source control service. EFS is less performant to workloads that require random access over large files. Use EFS if you want to distribute highly parallelized workloads like analytical workloads and media processing across several machines.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/deploy-codecommit-elastic-beanstalk/\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codecommit.html"
  },
  {
    "id": 105,
    "question": "A development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. The team requires a solution to extract custom metrics, such as processing times, directly from the logs generated by an AWS Lambda function. These metrics must be analyzed for operational insights, with alarms set up to detect anomalies in real-time.\r\n\r\nWhich approach should be used to meet this requirement?",
    "options": [
      "Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies.",
      "Use CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis.",
      "Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics.",
      "Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge rules to trigger actions based on the metrics."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics."
    ],
    "correct_answer": "Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics.",
    "explanation": "The **Amazon CloudWatch embedded metric format** allows you to generate custom metrics asynchronously in the form of logs written to CloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics so that you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted metrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events.\r\n\r\nEmbedded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By using the embedded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to instrument or maintain separate code, while gaining powerful analytical capabilities on your log data.\r\n\r\nThe **Amazon CloudWatch embedded metric format (EMF)** is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values. The following is an example of embedded metric format.\r\n\r\nBy using EMF, you can extract and monitor these custom metrics directly from your logs, enabling you to gain operational insights and set alarms based on these metrics. Additionally, Amazon\u2019s open-source libraries provide a convenient way to format logs in the EMF. These libraries can be integrated into your application to structure the log events with the necessary metric data, ensuring that CloudWatch can accurately extract and process these custom metrics.\r\n\r\nHence, the correct answer is: **Use Amazon\u2019s open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics.**\r\n\r\nThe option that says: **Send custom metric data directly to Amazon EventBridge using the `PutMetricData` API operation. Create EventBridge rules to trigger actions based on the metrics** is incorrect because Amazon EventBridge is typically used for event-driven architectures, not for monitoring or custom metrics. EventBridge rules are used to respond to events, not to monitor metrics.\r\n\r\nThe option that says: **Use CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis** is incorrect because Lambda Insights is primarily designed for monitoring and troubleshooting Lambda performance but does not directly extract custom metrics from logs.\r\n\r\nThe option that says: **Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies** is incorrect. While this approach can be used for log analysis, it is overly complex for the requirement of extracting custom metrics from logs in real-time. Kinesis Data Firehose and Amazon Redshift are more suited for large-scale data analysis rather than real-time monitoring and alerting.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format.html"
  },
  {
    "id": 106,
    "question": "A developer is writing a web application that will allow users to save and retrieve images in an Amazon S3 bucket. The users are required to register and log in to access the application.\r\n\r\nWhich combination of AWS Services should the Developer utilize for implementing the user authentication module of the application?",
    "options": [
      "Amazon Cognito Identity Pools and User Pools.",
      "Amazon Cognito Identity Pools and IAM Role.",
      "Amazon Cognito User Pools and AWS Key Management Service (KMS)",
      "Amazon User Pools and AWS Security Token Service (STS)"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon Cognito Identity Pools and User Pools."
    ],
    "correct_answer": "Amazon Cognito Identity Pools and User Pools.",
    "explanation": "A **user pool** is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.\r\n\r\nAmazon Cognito **identity pools** (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services.\r\n\r\nHence, the correct answer is: **Amazon Cognito Identity Pools and User Pools.**\r\n\r\n**Amazon Cognito Identity Pools and IAM Role** are incorrect. The solution is not enough to meet the requirements as you have to use Cognito User pools to allow users to sign in to your application.\r\n\r\n**Amazon Cognito User Pools and AWS Key Management Service (KMS)** are incorrect because AWS KMS is just a service that is used to encrypt data.\r\n\r\n**Amazon User Pools and AWS Security Token Service (STS)** are incorrect. While it is true that you need AWS STS to allow users to access Amazon S3, it is already abstracted by the Amazon Cognito Identity Pools. That being said, you have to configure an Identity Pool to accept users federated with your Cognito User Pool.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"
  },
  {
    "id": 107,
    "question": "A university is gradually migrating some of its physical documents to the AWS cloud. They will start by moving their alumnus\u2019 historical records to Amazon S3. The storage solution should provide a secure and durable object storage with the lowest cost.\r\n\r\nWhich of the following types of S3 storage should you recommend?",
    "options": [
      "Amazon S3 Glacier Deep Archive",
      "Amazon S3 One-Zone",
      "Amazon S3 Glacier",
      "Amazon S3 Infrequent Access"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon S3 Glacier Deep Archive"
    ],
    "correct_answer": "Amazon S3 Glacier Deep Archive",
    "explanation": "**Amazon S3 Glacier Deep Archive** is an S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), S3 Glacier Deep Archive offers the **lowest cost storage in the cloud**, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site.\r\n\r\nAmazon S3 Glacier and S3 Glacier Deep Archive are designed to be the lowest-cost Amazon S3 storage classes, allowing you to archive large amounts of data at a very low cost. This makes it feasible to retain all the data you want for use cases like data lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what you need, with no minimum commitments or up-front fees.\r\n\r\nHence, the correct answer is: **Amazon S3 Glacier Deep Archive.**\r\n\r\n**Amazon S3 Glacier** is incorrect. Although it is a valid solution for archiving data records, it is more expensive than the Amazon S3 Glacier Deep Archive. With Amazon S3 Glacier, storage is priced from $0.004 per gigabyte per month, while Amazon S3 Glacier Deep Archive is priced from $0.00099 per GB-month.\r\n\r\n**Amazon S3 Infrequent Access** and **Amazon S3 One-Zone** are both incorrect because these services are not suitable for data archiving.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/about-aws/whats-new/2019/03/S3-glacier-deep-archive/\r\n\r\nhttps://aws.amazon.com/s3/pricing/"
  },
  {
    "id": 108,
    "question": "A developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as well as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be done to the stack.\r\n\r\nWhich of the following actions must be done by the developer to meet the requirements above?",
    "options": [
      "Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",
      "Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",
      "Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",
      "Use AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda."
    ],
    "correct_answer": "Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",
    "explanation": "The **AWS Cloud Development Kit (AWS CDK)** is an open-source software development framework to model and provision your cloud application resources using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS CDK can also update your deployed resources after you modify your app using the appropriate CDK commands.\r\n\r\nYou can think of the CDK as a cloud infrastructure \u201ccompiler\u201d. It provides a set of high-level class libraries, called Constructs, that abstract AWS cloud resources and encapsulate AWS best practices. Constructs can be snapped together into object-oriented CDK applications that precisely define your application infrastructure and take care of all the complex boilerplate logic. When you run your CDK application, it is compiled into a CloudFormation Template, the \u201cassembly language\u201d for AWS cloud infrastructure. The template is then ready for processing by the CloudFormation provisioning engine for deployment into your AWS account. The CDK tools make it easy to *define* your application infrastructure stack, while the CloudFormation service takes care of the safe and dependable *provisioning* of your stack.\r\n\r\nSince the scenario requires the provisioning of cloud resources using a programming language (Python), the correct answer is: **Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.**\r\n\r\nThe option that says: **Use CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda** is incorrect because CloudFormation only allows JSON and YAML in defining cloud resources in a stack.\r\n\r\nThe option that says: **Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda** is incorrect because boto3 is just a library for Python that lets you use AWS resources programmatically, allowing easy integration with your application.\r\n\r\nThe option that says: **Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda** is incorrect because AWS CloudShell simply provides a browser-based terminal for managing AWS resources. It does not offer the ability to define and manage infrastructure as code in a reusable way, as required by the scenario.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/developer/aws-cdk-developer-preview/\r\n\r\nhttps://aws.amazon.com/cdk/"
  },
  {
    "id": 109,
    "question": "A developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account.\r\n\r\nWhich methods could be done to verify IAM access to describe instances? (Select TWO.)",
    "options": [
      "Run the get-group-policy command.",
      "Run the describe-instances command with the --dry-run parameter.",
      "Validate the IAM role\u2019s permission by querying the in-line policies within the EC2 instance metadata.",
      "Use the IAM Policy Simulator to validate the permission for the IAM role.",
      "Run the get-role command."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Run the describe-instances command with the --dry-run parameter.",
      "Use the IAM Policy Simulator to validate the permission for the IAM role."
    ],
    "correct_answer": "Run the describe-instances command with the --dry-run parameter.",
    "explanation": "The **`--dry-run`** parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is `DryRun-Operation`. Otherwise, it is `UnauthorizedOperation`.\r\n\r\nWith the IAM policy simulator, you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organizations service control policies (SCPs), and resource-based policies.\r\n\r\nYou can test policies that are attached to IAM users, groups, or roles in your AWS account. If more than one policy is attached to the user, group, or role, you can test all the policies or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Run the `describe-instances` command with the `--dry-run` parameter.**\r\n\r\n**\u2013 Use the IAM Policy Simulator to validate the permission for the IAM role.**\r\n\r\nThe option that says: **Validate the IAM role\u2019s permission by querying the in-line policies within the EC2 instance metadata** is incorrect because in-line policies are not available in the EC2 instance metadata.\r\n\r\nThe option that says: **Run the `get-group-policy` command** is incorrect because this command just retrieves the specified inline policy document that is embedded in the specified IAM group. Since the developer is using IAM roles, this command won\u2019t work because IAM roles can\u2019t be associated with an IAM group.\r\n\r\nThe option that says: **Run the `get-role` command** is incorrect. While this command retrieves information about an IAM role, it does not provide information about the permissions attached to that IAM role.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-usage-help.html"
  },
  {
    "id": 110,
    "question": "A company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business. The manager wants to move its operation into the Cloud using Amazon Web Services.\r\n\r\nWhich combination of services could be used to run the application that will require the least amount of configuration?",
    "options": [
      "Amazon EC2 and Amazon Aurora",
      "Amazon S3 and Amazon CloudFront",
      "Amazon API Gateway and Amazon RDS",
      "Amazon ECS and Amazon EFS"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon EC2 and Amazon Aurora"
    ],
    "correct_answer": "Amazon EC2 and Amazon Aurora",
    "explanation": "You can install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP stack). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database.\r\n\r\nTo decouple the database from the application, you can choose from the AWS Database services that support MySQL (e.g., Amazon RDS, Amazon Aurora)\r\n\r\nFrom the options given, we can deploy a LAMP web server by using an EC2 instance and an Amazon Aurora database for MySQL.\r\n\r\nHence, the correct answer is **Amazon EC2 and Amazon Aurora.**\r\n\r\n**Amazon S3 and Amazon CloudFront** are incorrect because Amazon S3 is only capable of serving static websites. You cannot use an S3 bucket to host a LAMP web server.\r\n\r\n**Amazon ECS and Amazon EFS** are incorrect. Although it is possible to containerize a LAMP web server and host it on Amazon ECS, it\u2019s not suitable for the scenario because the web server is just a monolithic application rather than a microservice. Configuring ECS entails more effort than EC2 and Amazon EFS is only used for POSIX-compliant storage.\r\n\r\n**Amazon API Gateway and Amazon RDS** are incorrect. While you can host a MySQL database using Amazon RDS, you can\u2019t use API Gateway to host the Apache web server.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-LAMP.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Tutorials.WebServerDB.CreateWebServer.html"
  },
  {
    "id": 111,
    "question": "A developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be invoked at a 7-day interval.\r\n\r\nWhich method will provide an automated and serverless approach to trigger the function?",
    "options": [
      "Add an environment variable named DAYS for the Lambda function and set its value to 7.",
      "Implement a task timer using Step Functions that will send a newsletter every week.",
      "Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.",
      "Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function."
    ],
    "correct_answer": "Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function.",
    "explanation": "**Amazon EventBridge (Amazon CloudWatch Events)** help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.\r\n\r\nTo solve the given problem, we can set up a Schedule event source that will invoke the Lambda function responsible for sending a newsletter every 7 days.\r\n\r\nHence, the correct answer is: **Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function.**\r\n\r\nThe option that says: **Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week** is incorrect because this approach does not comply with the required solution. Amazon EC2 is not a serverless compute service.\r\n\r\nThe option that says: **Add an environment variable named DAYS for the Lambda function and set its value to 7** is incorrect because an environment variable is just an optional key-value pair that is stored in the Lambda function\u2019s version-specific configuration.\r\n\r\nThe option that says: **Implement a task timer using Step Functions that will send a newsletter every week** is incorrect. Although serverless, using Step Functions for a basic application that pushes data is unnecessarily complex and may incur additional costs.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"
  },
  {
    "id": 112,
    "question": "A company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the application on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still appears as a resource in the ECS cluster.\r\n\r\nWhat is the possible cause of this?",
    "options": [
      "After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console.",
      "When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.",
      "After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the EC2 launch type.",
      "When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster."
    ],
    "question_type": "single",
    "correct_answers": [
      "When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster."
    ],
    "correct_answer": "When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.",
    "explanation": "If you terminate a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster. However, if you terminate a container instance in the STOPPED state, that container instance isn\u2019t automatically removed from the cluster.\r\n\r\nTo deregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS Console or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster.\r\n\r\nHence, the correct answer is: **When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.**\r\n\r\nThe option that says: **When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster** is incorrect because terminating a container instance in the RUNNING state will automatically deregister it from the cluster.\r\n\r\nThe option that says: **After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console** is incorrect because you do not have to manually deregister a container instance terminated in the RUNNING state. It will automatically be deregistered from the cluster.\r\n\r\nThe option that says: **After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the the EC2 launch type** is incorrect because while it is true that you should manually deregister the container instance terminated in the STOPPED state, this should be done in the Amazon ECS Console and not on the Amazon EC2 Console.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html"
  },
  {
    "id": 113,
    "question": "A developer uses Amazon ECS to orchestrate two Docker containers. He needs to configure ECS to allow the two containers to share log data.\r\n\r\nWhich configuration should the developer do?",
    "options": [
      "Use two task definitions for each container and mount an EFS volume between the tasks.",
      "Specify the containers in a single task definition and configure EFS as its volume type.",
      "Use two pod specifications for each container and mount an EFS volume between the pods.",
      "Specify the containers in a single pod specification and configure EFS as its volume type."
    ],
    "question_type": "single",
    "correct_answers": [
      "Specify the containers in a single task definition and configure EFS as its volume type."
    ],
    "correct_answer": "Specify the containers in a single task definition and configure EFS as its volume type.",
    "explanation": "A task definition is required to run Docker containers in Amazon ECS. The following are some of the parameters you can specify in a task definition:\r\n\r\n\u2013 The Docker image to use with each container in your task\r\n\r\n\u2013 How much CPU and memory to use with each task or each container within a task\r\n\r\n\u2013 The launch type to use, which determines the infrastructure on which your tasks are hosted\r\n\r\n\u2013 The Docker networking mode to use for the containers in your task\r\n\r\n\u2013 The logging configuration to use for your tasks\r\n\r\n\u2013 Whether the task should continue to run if the container finishes or fails\r\n\r\n\u2013 The command the container should run when it is started\r\n\r\n\u2013 Any data volumes that should be used with the containers in the task\r\n\r\n\u2013 The IAM role that your tasks should use\r\n\r\nBefore you can run Docker containers on Amazon ECS, you must create a task definition. You can define multiple containers and data volumes in a single task definition.\r\n\r\nHence, the correct answer is: **Specify the containers in a single task definition and configure EFS as its volume type.**\r\n\r\nThe option that says: **Use two task definitions for each container and mount an EFS volume between the tasks** is incorrect because you can define two containers in a task definition. Creating two task definitions is unnecessary.\r\n\r\nA pod is an execution unit specifically used in Kubernetes. Since the scenario mentioned that the containers are to be orchestrated using Amazon ECS, The following options are both incorrect:\r\n\r\n**\u2013 Use two pod specifications for each container and mount an EFS volume between the pods.**\r\n\r\n**\u2013 Specify the containers in a single pod specification and configure EFS as its volume type.**\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/example_task_definitions.html"
  },
  {
    "id": 114,
    "question": "A company is running an e-commerce application on an Amazon EC2 instance. A newly hired developer has been tasked to monitor and handle the necessary updates on the EC2 instance every Saturday. The developer is working from home and needs remote access to the webserver. As the system administrator, you\u2019re looking to use the AWS STS API to give the developer temporary credentials and enforce Multi-factor Authentication (MFA) to protect specific programmatic calls against the instance that could adversely affect the server.\r\n\r\nWhich of the following STS API should you use?",
    "options": [
      "AssumeRoleWithWebIdentity",
      "GetFederationToken",
      "AssumeRoleWithSAML",
      "GetSessionToken"
    ],
    "question_type": "single",
    "correct_answers": [
      "GetSessionToken"
    ],
    "correct_answer": "GetSessionToken",
    "explanation": "**AWS Security Token Service (AWS STS)** is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).\r\n\r\nBelow is the summary of the available STS API:\r\n\r\n**AssumeRole** \u2013 is useful for allowing existing IAM users to access AWS resources that they don\u2019t already have access to. For example, the user might need access to resources in another AWS account. It is also useful as a means to temporarily gain privileged access\u2014for example, to provide multi-factor authentication (MFA). You must call this API using existing IAM user credentials.\r\n\r\n**AssumeRoleWithWebIdentity** \u2013 returns a set of temporary security credentials for federated users who are authenticated through a public identity provider. Examples of public identity providers include Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible identity provider.\r\n\r\n**AssumeRoleWithSAML** \u2013 returns a set of temporary security credentials for federated users who are authenticated by your organization\u2019s existing identity system. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful in organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions.\r\n\r\n**GetFederationToken** \u2013 returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network. You must call the GetFederationToken operation using the long-term security credentials of an IAM user.\r\n\r\n**GetSessionToken** \u2013 returns a set of temporary security credentials to an existing IAM user. This is useful for providing enhanced security, such as allowing AWS requests only when MFA is enabled for the IAM user. Because the credentials are temporary, they provide enhanced security when you have an IAM user who accesses your resources through a less secure environment.\r\n\r\nAll of the options given provide temporary credentials to make API calls against AWS resources, but GetSessionToken is the only API that supports MFA. Hence, the correct answer is **GetSessionToken.**\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#stsapi_comparison\r\n\r\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html"
  },
  {
    "id": 115,
    "question": "A Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The submodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and stored in a MongoDB database.\r\n\r\nWhat should he do to develop the Lambda function in his preferred programming language?\r\n\r\n",
    "options": [
      "Create a Lambda function with a supported runtime version for Ruby.",
      "Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function.",
      "Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.",
      "Create a Lambda function using the AWS SDK for Ruby."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a Lambda function with a supported runtime version for Ruby."
    ],
    "correct_answer": "Create a Lambda function with a supported runtime version for Ruby.",
    "explanation": "AWS Lambda **natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code**, and provides a Runtime API, which allows you to use any additional programming languages to author your functions.\r\n\r\nYou can use the custom runtime to create a Lambda function if your preferred language is not available. You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function\u2019s handler method when the function is invoked. You can include a runtime in your function\u2019s deployment package in the form of an executable file named bootstrap.\r\n\r\nBecause AWS Lambda supports Ruby by default, there\u2019s no additional configuration needed.\r\n\r\nHence, the correct answer is: **Create a Lambda function with a supported runtime version for Ruby.**\r\n\r\nThe option that says: **Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function\u2019s deployment package. Migrate it to a layer that you manage independently from the function** is incorrect because a custom runtime is used to run programming languages that are not readily available to AWS Lambda.\r\n\r\nThe option that says: **Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby** is incorrect because the application will make calls to an external API and not to AWS Resources. AWS SDK is used for making API calls to different AWS services. Additionally, you don\u2019t have to make use of the custom runtime because AWS Lambda natively supports Ruby.\r\n\r\nThe option that says: **Create a Lambda function using the AWS SDK for Ruby** is incorrect because the AWS SDK for Ruby just provides Ruby classes to access AWS services. It is not a valid runtime environment.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/lambda/faqs/\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html?icmpid=docs_lambda_help"
  },
  {
    "id": 116,
    "question": "A developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn\u2019t generating logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups.\r\n\r\nUpon inspection, the following observations were made:\r\n\r\n- The function\u2019s code contains appropriate logging statements.\r\n    \r\n- The Lambda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role has no permissions assigned.\r\n    \r\n- The Lambda function does not have any resource-based policies.\r\n    \r\n\r\nWhich configuration must be done to resolve the issue?",
    "options": [
      "Associate the function with a resource-based policy that contains the logs:PutLogEvents permissions.",
      "Update the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.",
      "Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy.",
      "Associate the function with a resource-based policy that contains the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions."
    ],
    "question_type": "single",
    "correct_answers": [
      "Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy."
    ],
    "correct_answer": "Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy.",
    "explanation": "A Lambda function\u2019s execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. For example, you might create an execution role that has permission to send logs to Amazon CloudWatch and upload trace data to AWS X-Ray.\r\n\r\nThe `AWSLambdaBasicExecutionRole` is a managed policy provided by AWS that includes permissions essential for a Lambda function to create and write logs to Amazon CloudWatch Logs. These permissions include permissions to Log actions such as `logs:CreateLogGroup`, `logs:CreateLogStream`, and `logs:PutLogEvents`.\r\n\r\nWhen you create a Lambda function using the AWS Management Console, the execution role that AWS automatically creates for you often includes this managed policy. However, when defining a Lambda function via a CloudFormation template or other Infrastructure as Code (IAC) methods, you might need to explicitly attach this policy to the function\u2019s execution role to ensure that it has the appropriate logging permissions.\r\n\r\nHence, the correct answer is: **Update the execution role by adding the `AWSLambdaBasicExecutionRole` managed policy.**\r\n\r\nThe option that says: **Associate the function with a resource-based policy that contains the `logs:CreateLogGroup`, `logs:CreateLogStream`, and `logs:PutLogEvents` permissions** is incorrect. While this option contains the required permissions, they are defined as resource-based policies rather than as part of the execution role\u2019s policy. Resource-based policies just define what AWS services or users are allowed to do with the Lambda function (like invoking the function). The permissions required to write to CloudWatch Logs should be associated with the execution role.\r\n\r\nThe option that says: **Update the execution role by adding the `CloudWatchLambdaInsightsExecutionRolePolicy` managed policy** is incorrect. Although this policy might have permissions similar to `AWSLambdaBasicExecutionRole`, its target resource is specific to Lambda Insights and not for Lambda functions.\r\n\r\nThe option that says: **Associate the function with a resource-based policy that contains the `logs:PutLogEvents` permissions** is incorrect. This option suggests associating the Lambda function with a resource-based policy containing only the `logs:PutLogEvents` permission, which is insufficient. This permission alone allows the function to send log data to existing streams but does not permit the creation of new log groups or streams in Amazon CloudWatch Logs.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\r\n\r\nhttps://repost.aws/knowledge-center/lambda-cloudwatch-log-streams-error"
  },
  {
    "id": 117,
    "question": "A developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine handle and recover from State exception errors.\r\n\r\nWhich of the following actions should the developer do?",
    "options": [
      "Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition.",
      "Use Catch and Retry fields in the state machine definition.",
      "Use Catch and Retry fields inside the application code.",
      "Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Catch and Retry fields in the state machine definition."
    ],
    "correct_answer": "Use Catch and Retry fields in the state machine definition.",
    "explanation": "**AWS Step Functions** is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.\r\n\r\n&nbsp;\r\n\r\nTask and Parallel states can have a field named **Retry**, whose value must be an array of objects known as *retriers*. An individual retrier represents a certain number of retries, usually at increasing time intervals.\r\n\r\nA retrier contains the following fields:\r\n\r\n**ErrorEquals**\r\n\r\nA non-empty array of strings that match error names. When a state reports an error, Step Functions scans through the retriers. When the error name appears in this array, it implements the retry policy described in this retrier.\r\n\r\n**IntervalSeconds**\r\n\r\nAn integer that represents the number of seconds before the first retry attempt (1 by default).\r\n\r\n**MaxAttempts**\r\n\r\nA positive integer that represents the maximum number of retry attempts (3 by default). If the error recurs more times than specified, retries cease and normal error handling resumes. A value of 0 specifies that the error or errors are never retried.\r\n\r\n**BackoffRate**\r\n\r\nThe multiplier by which the retry interval increases during each attempt (2.0 by default)\r\n\r\nTask and Parallel states can have a field named **Catch**. This field\u2019s value must be an array of objects, known as *catchers*.\r\n\r\nA catcher contains the following fields.\r\n\r\n**ErrorEquals**\r\n\r\nA non-empty array of strings that match error names, specified exactly as they are with the retrier field of the same name.\r\n\r\n**Next**\r\n\r\nA string that must exactly match one of the state machine\u2019s state names.\r\n\r\n**ResultPath**\r\n\r\nA path that determines what input is sent to the state specified in the Next field.\r\n\r\nWhen a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher\u2019s ErrorEquals field, the state machine transitions to the state named in the Next field.\r\n\r\nIn the given scenario, you can use the Catch and Retry fields inside the state machine definition to capture an exception error and attempt to recover from it by automatically retrying the state.\r\n\r\nHence, the correct answer is: **Use Catch and Retry fields in the state machine definition.**\r\n\r\nThe option that says: **Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition** is incorrect because you don\u2019t have to implement a try and catch logic yourself in the application code as this can already be easily done by declaring a Catch field in your state machine definition.\r\n\r\nThe option that says: **Use Catch and Retry fields inside the application code** is incorrect because these fields are only applicable to the Amazon States Language.\r\n\r\nThe option that says: **Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition** is incorrect because the Catch field is explicitly a function of the Amazon States Language and will not work if implemented in the application code.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\r\n\r\nhttps://aws.amazon.com/step-functions/"
  },
  {
    "id": 118,
    "question": "An application executes GET operations to various AWS services. The development team is using AWS X-Ray to trace all the calls made to AWS. As one of the developers, you are responsible for maintaining a particular block of code on the application. To save time, you only want to record data associated with the code to group the traces in the AWS console.\r\n\r\nWhich of the following X-Ray features should you use?\r\n\r\n",
    "options": [
      "Metadata",
      "Annotations",
      "Subsegment",
      "Sampling"
    ],
    "question_type": "single",
    "correct_answers": [
      "Annotations"
    ],
    "correct_answer": "Annotations",
    "explanation": "**AWS X-Ray** analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can identify performance bottlenecks, edge case errors, and other hard to detect issues.\r\n\r\nWhen you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segmented document as annotations and metadata. Annotations and metadata are aggregated at the trace level and can be added to any segment or subsegment.\r\n\r\n**Annotations** are simple key-value pairs that are indexed for use with filter expressions. **Use annotations to record data that you want to use to group traces** in the console, or when calling the GetTraceSummaries API.\r\n\r\nHence, the correct answer is: **Annotations.**\r\n\r\n**Metadata** is incorrect because you can not group traces with it. Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. You commonly use metadata to record data that you want to store in the trace but don\u2019t need to search for traces.\r\n\r\n**Sampling** is incorrect because it is just used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests.\r\n\r\n**Subsegment** is incorrect because it is only used to provide more granular timing information and details about downstream calls that your application made to fulfill the original request. It cannot group traces from recorded data.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-segment.html"
  },
  {
    "id": 119,
    "question": "A developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script that uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an `InvalidInstanceID.NotFound` error whenever the script is run.\r\n\r\nWhat is the most likely cause of the error?",
    "options": [
      "The Image Id used in running the command for creating a snapshot is incorrect.",
      "The AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance lives.",
      "The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.",
      "The AWS Access Key Id used to configure the AWS CLI is invalid."
    ],
    "question_type": "single",
    "correct_answers": [
      "The AWS Region name used to configure the AWS CLI does not match the region where the instance lives."
    ],
    "correct_answer": "The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.",
    "explanation": "For general use, the `aws configure` command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS CLI prompts you for four pieces of information:\r\n\r\n\u2013 Access Key ID\r\n\r\n\u2013 Secret Access Key\r\n\r\n\u2013 AWS Region\r\n\r\n\u2013 Output format\r\n\r\nAccess keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS.\r\n\r\nAWS Region identifies the AWS Region whose servers you want to send your requests to by default. This is typically the Region closest to you, but it can be any Region. For example, you can type us-west-2 to use US West (Oregon). This is the Region that all later requests are sent to unless you specify otherwise in an individual command.\r\n\r\nOutput format specifies how the results are formatted. The value can be any of the values in the following list. If you don\u2019t specify an output format, JSON is used as the default.\r\n\r\n&nbsp;\r\n\r\nThe `InvalidInstanceID.NotFound` error suggests that an instance does not exist. Ensure that you have indicated the AWS Region in which the instance is located if it\u2019s not in the default Region. This error may occur because the ID of a recently created instance has not propagated through the system.\r\n\r\nSince it was mentioned in the scenario that the EC2 instance already exists, we can conclude that there is a mismatch in the AWS Region configured in the CLI. It means that the EC2 instance is located in another Region which is why the developer got the error message.\r\n\r\nHence, the correct answer is: **The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.**\r\n\r\nThe option that says: **The AWS Region, where the programmatic access for the AWS CLI is created, does not match with the region where the instance lives** is incorrect because the programmatic access is just another way of presenting yourself as an IAM User. IAM users are global entities which means it can not be associated with a particular region.\r\n\r\nThe option that says: **The AWS Access Key Id used to configure the AWS CLI is invalid** is incorrect because you will get an `InvalidAccessKeyId` error as a response if you do not have the correct AWS Access Key Id. The scenario\u2019s issue is about the `InvalidInstanceID.NotFound` error.\r\n\r\nThe option that says: **The Image Id used in running the command for creating a snapshot** is incorrect because the error is about the instance Id and not the Image Id.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\r\n\r\nhttps://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/"
  },
  {
    "id": 120,
    "question": "A team is deploying a serverless app with AWS Cloud Development Kit (CDK), using the Lambda construct library to define Lambda functions. They\u2019ve had successful deployments in a test environment using the cdk deploy command. However, when the team attempts their first deployment to a new AWS account, it fails, returning a NoSuchBucket error.\r\n\r\nThe team is confident that they made no alterations to the code or CDK configurations after the last successful deployment.\r\n\r\nWhat AWS CDK CLI command should be run first to fix the problem?",
    "options": [
      "cdk bootstrap",
      "cdk synth",
      "cdk import",
      "cdk context"
    ],
    "question_type": "single",
    "correct_answers": [
      "cdk bootstrap"
    ],
    "correct_answer": "cdk bootstrap",
    "explanation": "The `NoSuchBucket` error indicates that the AWS CDK is trying to access an S3 bucket that doesn\u2019t exist in the new AWS account. This happens because the CDK uses an S3 bucket to store assets required for deployment, such as Lambda code or Docker images. When you move to a new AWS account or a new region within an account, you need to set up the environment for the CDK. This is done using the `cdk bootstrap` command.\r\n\r\nThis command provisions the necessary resources in the AWS environment to manage and deploy your CDK applications, such as IAM roles and the S3 bucket that the CDK uses for deployments.\r\n\r\nHence, the correct answer is: **`cdk bootstrap`**\r\n\r\nThe option that says: **`cdk synthesize`** is incorrect because this merely generates a CloudFormation template from your CDK code but does not provision resources.\r\n\r\nThe option that says: **`cdk context`** is incorrect. This command is simply used to cache context information about your AWS environment that it retrieves during the execution of CDK commands. This information can include things like availability zones, VPC details, or any other AWS environment details that the CDK needs to know to synthesize your stacks.\r\n\r\nThe option that says: **`cdk import`** is incorrect. This just allows you to bring existing resources into a CDK stack using CloudFormation resource imports.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cdk/v2/guide/troubleshooting.html\r\n\r\nhttps://repost.aws/knowledge-center/cdk-customize-bootstrap-cfntoolkit\r\n\r\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html"
  },
  {
    "id": 121,
    "question": "A developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one of the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before retiring the previous one.\r\n\r\nWhich solution should the developer implement?\r\n\r\n",
    "options": [
      "Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage.",
      "Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.",
      "Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.",
      "Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL."
    ],
    "question_type": "single",
    "correct_answers": [
      "Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage."
    ],
    "correct_answer": "Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.",
    "explanation": "A developer is using the AWS CLI to interact with different AWS services. An `UnauthorizedOperation` error, as shown below, is received after running the `stop-instance` command:\r\n\r\nTo deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, `dev`, `prod`, `beta`, `v2`). API stages are identified by the API ID and stage name. They\u2019re included in the URL that you use to invoke the API. Each stage is a named reference to an API deployment and is made available for client applications to call.\r\n\r\n&nbsp;\r\n\r\nIn the scenario, you can apply the new updates to the backend Lambda function and publish it as a new version. Then, update the integration request of the target API resource by replacing the old Lambda function ARN with the new version\u2019s ARN. Finally, deploy the resource to a new stage and use the new Invoke URL in your application. This way, existing users will be able to access both versions. You can retire the old version eventually after all users have migrated to the new one.\r\n\r\n&nbsp;\r\n\r\nHence, the correct answer is: **Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.**\r\n\r\nThe option that says: **Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL** is incorrect. Although you can invoke a Lambda function by its ARN, you can\u2019t directly invoke a Lambda by its URL because AWS does not provide one. However, you can invoke it through a URL when integrated with API Gateway.\r\n\r\nThe option that says: **Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage** is incorrect because the objective is to publish two live endpoints (one for version 1 and another for version 2) that the users can access. Therefore, we need to create two stages. Deploying the new version to the same stage would overwrite the old version.\r\n\r\nThe option that says: **Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin** is incorrect because a Lambda function is not a valid origin for a CloudFront distribution. Moreover, CloudFront is for content delivery and not used for exposing backend services as API endpoints.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html"
  },
  {
    "id": 122,
    "question": "A developer is building a new feature for an application deployed on an EC2 instance in the N. Virginia region. A co-developer suggests to upload the code on Amazon S3 and use CodeDeploy to deploy the new version of the application. The deployment fails during the DownloadBundle deployment lifecycle event with the UnknownError: not opened for reading error.\r\n\r\nWhat is the possible cause of this?",
    "options": [
      "The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region.",
      "The EC2 instance\u2019s IAM profile does not have the permissions to access the application code in Amazon S3.",
      "Wrong configuration of the DownloadBundle lifecycle event in the AppSec file.",
      "Versioning is not enabled on the Amazon S3 Bucket where the application code resides."
    ],
    "question_type": "single",
    "correct_answers": [
      "The EC2 instance\u2019s IAM profile does not have the permissions to access the application code in Amazon S3."
    ],
    "correct_answer": "The EC2 instance\u2019s IAM profile does not have the permissions to access the application code in Amazon S3.",
    "explanation": "An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.\r\n\r\n`DownloadBundle` deployment lifecycle event will throw an error whenever:\r\n\r\n\u2013 The EC2 instance\u2019s IAM profile does not have permission to access the application code in the Amazon S3.\r\n\r\n\u2013 An Amazon S3 internal error occurs.\r\n\r\n\u2013 The instances you deploy to are associated with one AWS Region (for example, US West Oregon), but the Amazon S3 bucket that contains the application revision is related to another AWS Region (for example, US East N. Virginia).\r\n\r\nHence, the correct answer is: **The EC2 instance\u2019s IAM profile does not have the permissions to access the application code in Amazon S3**.\r\n\r\nThe option that says: **Wrong configuration of the `DownloadBundle` lifecycle event in the AppSec file** is incorrect because you can not manually configure `DownloadBundle` in the Appsec file. The CodeDeploy Agent installed on the EC2 instance manages the `DownloadBundle` lifecycle event.\r\n\r\nThe option that says: **The `DownloadBundle` deployment lifecycle event is not supported in the N. Virginia region** is incorrect because CodeDeploy is supported in N. Virginia.\r\n\r\nThe option that says: **Versioning is not enabled on the Amazon S3 Bucket where the application code resides** is incorrect because versioning on Amazon S3 Bucket is just used to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-downloadbundle\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
  },
  {
    "id": 123,
    "question": "An application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to import module error when invoked.\r\n\r\nAs the developer, how can you fix the error?",
    "options": [
      "Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.",
      "Install the missing modules locally to your application\u2019s folder. Package the folder into a ZIP file and upload it to AWS Lambda.",
      "Run a Linux command inside the Lambda function to install the missing modules.",
      "Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda."
    ],
    "question_type": "single",
    "correct_answers": [
      "Install the missing modules locally to your application\u2019s folder. Package the folder into a ZIP file and upload it to AWS Lambda."
    ],
    "correct_answer": "Install the missing modules locally to your application\u2019s folder. Package the folder into a ZIP file and upload it to AWS Lambda.",
    "explanation": "`ModuleNotFoundError` and `Module cannot be loaded` are common errors for Lambda functions. These errors are usually due to an incorrect folder structure or file permissions with the deployment package .zip file.\r\n\r\nTo fix this error:\r\n\r\n1. Install all dependency modules local to the function project.\r\n\r\n2. Build the deployment package by zipping up the project folder for deployment to Lambda.\r\n\r\n3. Upload the deployment package.\r\n\r\n&nbsp;\r\n\r\nHence, the correct answer is: **Install the missing modules locally to your application\u2019s folder. Package the folder into a ZIP file and upload it to AWS Lambda.**\r\n\r\nThe option that says: **Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked** is incorrect. This will still result in the same error because the Lambda function won\u2019t be able to recognize the modules that are defined in your code.\r\n\r\nThe option that says: **Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda** is incorrect. This won\u2019t work. The modules and your Lambda code must be under the same directory level before packaging them into a ZIP file.\r\n\r\nThe option that says: **Run a Linux command inside the Lambda function to install the missing modules** is incorrect. Although you can run Linux commands via custom runtimes in AWS Lambda, you can\u2019t install dependencies directly on a Lambda function.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/build-python-lambda-deployment-package/\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/python-package.html"
  },
  {
    "id": 124,
    "question": "\r\n\r\nA developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container Docker environment.\r\n\r\nHow should the developer configure the container definitions in the environment?\r\n",
    "options": [
      "Configure the container definitions in the Amazon ECS Console when building the Docker environment.",
      "Use the eb config command to configure the container definitions.",
      "Configure the container definitions in the Dockerrun.aws.json file.",
      "Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure the container definitions in the Dockerrun.aws.json file."
    ],
    "correct_answer": "Configure the container definitions in the Dockerrun.aws.json file.",
    "explanation": "Standard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk environment. In order to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run multiple Docker containers side by side.\r\n\r\nContainer instances\u2014Amazon EC2 instances running Multicontainer Docker in an Elastic Beanstalk environment\u2014require a configuration file named **`Dockerrun.aws.json`.** This file is specific to Elastic Beanstalk and can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform.\r\n\r\nHence, the correct answer is: **Configure the container definitions in the `Dockerrun.aws.json` file.**\r\n\r\nThe option that says: **Configure the container definitions in the Amazon ECS Console when building the Docker environment** is incorrect because the application must be deployed using Elastic Beanstalk. Therefore, you must configure the container definitions in the `Dockerrun.aws.json` file.\r\n\r\nThe option that says: **Configure the container definitions in the `Dockerrun.aws.json.config` and put it inside the `.ebextensions` folder** is incorrect because the `Dockerrun.aws.json` file should be placed on the same level where your application file resides.\r\n\r\nThe option that says: **Use the `eb config` command to configure the container definitions** is incorrect. This is just a command that you can use to change the configuration settings of your environment.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecstutorial.html\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/single-container-docker-configuration.html"
  },
  {
    "id": 125,
    "question": "\r\n\r\nA development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a source code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A RequestError timeout error appears on CloudWatch whenever CodeBuild is accessed.\r\n\r\nWhich is a possible solution to resolve the issue?\r\n",
    "options": [
      "Modify the proxy element of the buildspec.yml file on the source code root directory.",
      "Modify the artifacts element of the buildspec.yml file on the source code root directory.",
      "Modify the proxy element of the AppSpec.yml file on the source code root directory.",
      "Modify the phases element of the AppSpec.yml file on the source code root directory."
    ],
    "question_type": "single",
    "correct_answers": [
      "Modify the proxy element of the buildspec.yml file on the source code root directory."
    ],
    "correct_answer": "Modify the proxy element of the buildspec.yml file on the source code root directory.",
    "explanation": "You can use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy server, you install a proxy server in a public subnet and CodeBuild in a private subnet in a VPC.\r\n\r\nBelow are possible causes of error when running CodeBuild with a proxy server:\r\n\r\n1.  `ssl-bump` is not configured properly.\r\n2.  Your organization\u2019s security policy does not allow you to use `ssl-bump`.\r\n3.  Your `buildspec.yml` file does not have proxy settings specified using a `proxy` element.\r\n\r\nIf you do not use `ssl-bump` for an explicit proxy server, add a proxy configuration to your `buildspec.yml` using a `proxy` element.\r\n\r\n`version: 0.2proxy:upload-artifacts: yeslogs: yes`\r\n\r\nHence, the correct answer is: **Modify the `proxy` element of the `buildspec.yml` file on the source code root directory.**\r\n\r\nThe following options are both incorrect because the AppSpec file is used in AWS CodeDeploy and not in AWS CodeBuild:\r\n\r\n**\u2013 Modify the `phases` element of the `AppSpec.yml` file on the source code root directory.**\r\n\r\n**\u2013 Modify the `proxy` element of the `AppSpec.yml` file on the source code root directory.**\r\n\r\nThe option that says: **Modify the `artifacts` element of the `buildspec.yml` file on the source code root directory** is incorrect because the `artifacts` element represents information about where CodeBuild can find the build output and how CodeBuild prepares it for uploading to the S3 output bucket. You can not configure proxy settings here. Use the `proxy` element if you want to run CodeBuild in an explicit proxy server.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\r\n\r\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.proxy"
  },
  {
    "id": 126,
    "question": "\r\n\r\nA developer is writing an application that will download hundreds of media files. Each file must be encrypted with a unique encryption key within the application before storing it in an S3 bucket. The developer needs a cost-effective solution with low management overhead.\r\n\r\nWhich of the following is the most suitable solution\r\n",
    "options": [
      "Use the CreateKey API command to generate a CMK for each file to encrypt them.",
      "Use an open-source key generator to produce a unique key. Use the key to encrypt the files.",
      "Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.",
      "Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file."
    ],
    "correct_answer": "Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.",
    "explanation": "The `GenerateDataKey` generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.\r\n\r\n`GenerateDataKey` returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.\r\n\r\nHence, the correct answer is: **Use the `GenerateDataKey` API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.**\r\n\r\nThe option that says: **Use the `CreateKey` API command to generate a CMK for each file to encrypt them** is incorrect. A CMK costs $1/month, which means you\u2019d have to spend over $1,000 just to generate unique encryption keys. In addition, a CMK can only encrypt 4 KB of data, which does not make it suitable for encrypting media files.\r\n\r\nThe option that says: **Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket** is incorrect. When you use Server-Side Encryption with SSE-S3, each object is encrypted with a unique key. However, the scenario requires you to generate a unique key for each file prior to uploading. This means that the encryption must be done at the client-side.\r\n\r\nThe option that says: **Use an open-source key generator to produce a unique key. Use the key to encrypt the files** is incorrect. Although this will save costs, it\u2019ll be difficult to manage the keys and you\u2019d have to write your own logic for encrypting and decrypting the files. AWS KMS makes it a lot easier to manage the data keys. Also, you don\u2019t have to write your own code as every KMS function is wrapped in an API.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html"
  },
  {
    "id": 127,
    "question": "\r\n\r\nA media analytics company provides APIs as a service that enables users to retrieve aggregated, daily-updated data on video performance metrics, such as views, likes, and watch times. These APIs are built using Amazon API Gateway and AWS Lambda, with data sourced from a pre-computed file in Amazon S3 that updates every 24 hours. Due to a surge in traffic, users have reported increased latency when accessing the API. The company aims to enhance the API\u2019s responsiveness without changing the backend architecture.\r\n\r\nWhich approach would improve the responsiveness of the APIs?\r\n",
    "options": [
      "Use Amazon ElastiCache to store frequently requested data in memory.",
      "Configure Amazon CloudFront as a caching layer in front of API Gateway.",
      "Enable cross-origin resource sharing (CORS) using the API Gateway.",
      "Enable Amazon API Gateway caching."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable Amazon API Gateway caching."
    ],
    "correct_answer": "Enable Amazon API Gateway caching.",
    "explanation": "**Amazon API Gateway Caching** is a powerful feature that enables users to improve the performance and scalability of its APIs by storing responses in memory. By enabling caching, API Gateway can store responses to API requests for a specified period. With subsequent requests with the same parameters, API Gateway can quickly serve the cached response, reducing the need to invoke the backend Lambda function. This leads to faster response times, less load on backend systems, and a more cost-effective solution by minimizing the executions required for frequently accessed data. API Gateway caching is especially beneficial when the data served is relatively static or changes infrequently, such as aggregated metrics that update once daily.\r\n\r\nAPI Gateway caching is highly configurable, allowing users to specify the time-to-live (TTL) for cached data, meaning how long the data should be stored before being refreshed. This TTL can be adjusted based on the data\u2019s nature and the API\u2019s specific needs. For instance, if the video performance metrics are updated once every 24 hours, a TTL of 24 hours ensures that cached responses remain valid for each update cycle. Additionally, caching can be applied at both the stage and method levels, providing flexibility in managing which API endpoints should be cached and for how long.\r\n\r\nBy using API Gateway\u2019s built-in caching feature, users can significantly reduce the latency experienced by end-users, enhance overall API performance, and improve user experience. This feature also integrates seamlessly with AWS\u2019s broader environment, ensuring reliable and scalable API management without additional infrastructure changes.\r\n\r\nHence, the correct answer is: **Enable Amazon API Gateway caching.**\r\n\r\nThe option that says: **Enable cross-origin resource sharing (CORS) using the API Gateway** is incorrect. Cross-origin resource sharing (CORS) is primarily used to enable secure resource sharing between different domains, typically for web applications running in the browser. It does not directly impact the performance or responsiveness of the API itself. While CORS is crucial for handling cross-origin requests, it doesn\u2019t address the latency issues caused by frequent backend invocations and thus does not serve as a solution to improve the responsiveness of the API.\r\n\r\nThe option that says: **Use Amazon ElastiCache to store frequently requested data in memory** is incorrect. While ElastiCache is an effective tool for caching, it typically requires changes to the backend architecture to integrate the cache layer with the Lambda functions. This approach would involve additional configuration and management overhead, which goes against the goal of improving responsiveness without altering the existing backend architecture.\r\n\r\nThe option that says: **Configure Amazon CloudFront as a caching layer in front of API Gateway** is incorrect. Amazon CloudFront is primarily a content delivery network (CDN) that caches content at edge locations to speed up delivery for users across the globe. While it can help with reducing latency by caching API responses, it typically requires more complex configuration compared to API Gateway\u2019s built-in caching. CloudFront is more suited for static content or large-scale applications where global distribution is essential. Still, for API performance improvements, API Gateway\u2019s own caching is a more direct and simpler solution.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"
  },
  {
    "id": 128,
    "question": "\r\nA Software Engineer is developing a Node.js application that will be deployed using Elastic Beanstalk. The application source code is currently inside a folder called MyApp. He wants to add a configuration file named tutorialsdojo.config to the application.\r\n\r\nWhere should the file be placed?\r\n",
    "options": [
      "Inside the .ebextensions folder",
      "Inside the MyApp folder at the root level",
      "Inside the package.json",
      "Inside the .elasticbeanstalk folder"
    ],
    "question_type": "single",
    "correct_answers": [
      "Inside the .ebextensions folder"
    ],
    "correct_answer": "Inside the .ebextensions folder",
    "explanation": "You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application\u2019s source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.\r\n\r\nHence, the correct answer is: **Inside the .ebextensions folder.**\r\n\r\nThe option that says: **Inside the package.json** is incorrect. This is just the file that contains the libraries needed by the Node.js application to run.\r\n\r\nThe option that says: **Inside the .elasticbeanstalk folder** is incorrect because this is simply the folder that contains the environment configuration settings for the current running environment.\r\n\r\nThe option that says: **Inside the MyApp folder at the root level** is incorrect. Documents with a .config file extension should be placed in the .ebextensions folder.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html"
  },
  {
    "id": 129,
    "question": "\r\n\r\nA developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests, the Lambda function shows a cold start of about 5 seconds.\r\n\r\nWhich of the following could the developer do to reduce the cold start time? (Select TWO.)\r\n",
    "options": [
      "Increase the timeout setting for the Lambda function.",
      "Increase the memory allocation setting for the Lambda function.",
      "Add the Spring Framework to the project and enable dependency injection.",
      "Reduce the deployment package\u2019s size by including only the needed modules from the AWS SDK for Java.",
      "Run the Lambda function in a VPC to gain access to Amazon\u2019s high-end infrastructure."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Increase the memory allocation setting for the Lambda function.",
      "Reduce the deployment package\u2019s size by including only the needed modules from the AWS SDK for Java."
    ],
    "correct_answer": "Increase the memory allocation setting for the Lambda function.",
    "explanation": "A cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There are also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up.\r\n\r\nIn AWS Lambda, whenever you execute a helper function/pre-handler code where you need to do things like pulling data from an S3 bucket, connecting to a database, pulling configuration information and dependencies, or anything of the sorts, it gets executed on the INIT code where the partial cold start occurs.\r\n\r\nIt\u2019s important to note that basically everything that you\u2019re doing outside of the handler function will block its execution. When it comes to thinking about pre handler code dependencies that you want to use, remember that less is more. The more targeted you are at the resource that you include, the better the overall performance your function will have during its execution.\r\n\r\nYou also have the option to tweak the power of the resources that run your function by increasing the memory allocated to your function to optimize its overall performance.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Reduce the deployment package\u2019s size by including only the needed modules from the AWS SDK for Java.**\r\n\r\n**\u2013 Increase the memory allocation setting for the Lambda function.**\r\n\r\nThe option that says: **Increase the timeout setting for the Lambda function** is incorrect as this will not reduce the cold start time. This is usually done in solving the problem of execution timeout.\r\n\r\nThe option that says: **Run the Lambda function in a VPC to gain access to Amazon\u2019s high-end infrastructure** is incorrect because this solution will give no performance gain that will reduce the cold start of the Lambda function. This is usually done to a Lambda function to access private resources in a VPC.\r\n\r\nThe option that says: **Add the Spring Framework to the project and enable dependency injection** is incorrect as doing this can have a significant increase in your function\u2019s cold start time.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/\r\n\r\n[https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start\u2014Spring-Boot](https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start---Spring-Boot)"
  },
  {
    "id": 130,
    "question": "\r\n\r\nA developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the required dependencies in the requirements.txt file, the developer is now ready to test and deploy.\r\n\r\nWhat are the steps to successfully deploy the application?\r\n",
    "options": [
      "Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.",
      "Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.",
      "Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template.",
      "Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodePipeline."
    ],
    "question_type": "single",
    "correct_answers": [
      "Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket."
    ],
    "correct_answer": "Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.",
    "explanation": "Here are the SAM CLI commands needed to deploy serverless applications:\r\n\r\n**`sam init`** \u2013 Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a production-scale application.\r\n\r\n**`sam build` \u2013** The `sam build` command builds any dependencies that your application has, and copies your application source code to folders under `.aws-sam/build` to be zipped and uploaded to Lambda.\r\n\r\n**`sam deploy`** \u2013 performs the functionality of `sam package`. You can use the `sam deploy` command to directly package and deploy your application.\r\n\r\nSince the application\u2019s runtime and dependencies are already defined, the next step is to call the `sam build` command to install and build the dependencies of the application. After running a series of local tests, you can now package and deploy the SAM template into an S3 bucket via the sam deploy command.\r\n\r\nHence, the correct answer is: **Build the SAM template in the local machine and call the `sam deploy` command to package and deploy the SAM template from an S3 bucket.**\r\n\r\nThe option that says: **Build the SAM template in the local machine. Run the `sam deploy` command to package and deploy the SAM template from AWS CodePipeline** is incorrect because it suggests using AWS CodePipeline directly with the `sam deploy` command. While CodePipeline can primarily be used for CI/CD, the `sam deploy` command itself does not interact directly with CodePipeline. Instead, it packages and deploys the application using an S3 bucket and CloudFormation.\r\n\r\nThe option that says: **Run the `sam init` command. Build the SAM template in the local machine and call the `sam deploy` command to package and deploy the SAM template from an S3 bucket** is incorrect. You don\u2019t have to run the `sam init` command because from the conditions given, it is assumed that the runtime and the folder structure of the application have already been established.\r\n\r\nThe option that says: **Upload and build the SAM template in an EC2 instance. Run the `sam deploy` command to package and deploy the SAM template** is incorrect because you can build the SAM template from your local computer by using the SAM CLI. Creating an EC2 instance just adds unnecessary costs. Also, SAM can only deploy from Amazon S3.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html\r\n\r\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-command-reference.html"
  },
  {
    "id": 131,
    "question": "\r\n\r\nA developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT, UPDATE, and DELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table.\r\n\r\nWhich DynamoDB operation should the developer use?\r\n",
    "options": [
      "Scan",
      "BatchWriteItem",
      "Query",
      "TransactWriteItems"
    ],
    "question_type": "single",
    "correct_answers": [
      "TransactWriteItems"
    ],
    "correct_answer": "TransactWriteItems",
    "explanation": "With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing `TransactWriteItems` or `TransactGetItems` operation.\r\n\r\n`TransactWriteItems` is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that **either all of them succeed or none of them succeeds.**\r\n\r\nYou can add the following types of actions to a transaction:\r\n\r\n**`Put`** \u2014 Initiates a `PutItem` operation to create a new item or replace an old item with a new item, conditionally or without specifying any condition.\r\n\r\n**`Update`** \u2014 Initiates an `UpdateItem` operation to edit an existing item\u2019s attributes or add a new item to the table if it does not already exist. Use this action to add, delete, or update attributes on an existing item conditionally or without a condition.\r\n\r\n**`Delete`** \u2014 Initiates a `DeleteItem` operation to delete a single item in a table identified by its primary key.\r\n\r\n**`ConditionCheck`** \u2014 Checks that an item exists or checks the condition of specific attributes of the item.\r\n\r\nHence, the correct answer is: **`TransactWriteItems`**\r\n\r\n**`BatchWriteItem`** is incorrect because this is not a single all-or-nothing operation, which means some requests in a BatchWriteItem operation can succeed or fail. Also, this DynamoDB operation does not support `UpdateItem` , which is one of the requirements in the scenario.\r\n\r\n**`Scan`** is incorrect because this just reads all of the items in a table.\r\n\r\n**`Query`** is incorrect because this simply enables you to get items in a table based on primary key values.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/new-amazon-dynamodb-transactions/"
  },
  {
    "id": 132,
    "question": "A Lambda function is being developed to process a 50MB gzip-compressed file that will be uploaded to an S3 bucket on a daily basis. The function must have access to a storage location where it can load and unzip the file. After processing, the file will be delivered to another S3 bucket.\r\n\r\nWhich solution can the developer implement that requires the LEAST effort and cost?",
    "options": [
      "Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket.",
      "Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs.",
      "Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket.",
      "Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket."
    ],
    "question_type": "single",
    "correct_answers": [
      "Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket."
    ],
    "correct_answer": "Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket.",
    "explanation": "The Lambda execution environment provides ephemeral storage for your code to use at `/tmp`. This space has a size that can be set between **512 MB (free) and 10,240 MB**. For this feature, you are charged for the storage you configure over the 512 MB free limit for the duration of your function invokes.\r\n\r\nIn the scenario, the `/tmp` directory can be used as a staging area for unzipping the file. Also, since the file size is relatively small (50MB), even when unzipped, the default 512 MB should be enough for the job, making the solution the best option in terms of effort and cost.\r\n\r\nHence, the correct answer is: **Download the file to the `/tmp` directory. From there, consume and process the data before sending it to the S3 bucket.**\r\n\r\nThe option that says: **Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket** is incorrect. Although this is a valid solution, using EFS entails additional effort for this type of problem. For example, the Lambda function must be configured first to connect to the VPC where the EFS is located.\r\n\r\nThe option that says: **Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs** is incorrect. This solves nothing as the file will still remain uncompressed. Optionally, you can load the file into the Lambda function\u2019s memory and decompress it from there, but this method eats up a lot of memory and is subjected to the memory limits of the Lambda function.\r\n\r\nThe option that says: **Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket** is incorrect. Using Amazon EBS as storage for unzipping a file is a costly and overkill solution.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/aws-lambda-now-supports-up-to-10-gb-ephemeral-storage/"
  },
  {
    "id": 133,
    "question": "\r\n\r\nA company plans to conduct an online survey to distinguish the users who bought its product from those who didn\u2019t. The survey will be processed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is required to aggregate all the data that passes through the nodes if the process fails.\r\n\r\nWhat should the company do to meet the requirements?\r\n",
    "options": [
      "Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node\u2019s input data with its output.",
      "Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node\u2019s input data with its output.",
      "Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node\u2019s input data with its output.",
      "Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node\u2019s input data with its output."
    ],
    "question_type": "single",
    "correct_answers": [
      "Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node\u2019s input data with its output."
    ],
    "correct_answer": "Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node\u2019s input data with its output.",
    "explanation": "The output of a state can be a copy of its input, the result it produces (for example, the output from a Task state\u2019s Lambda function), or a combination of its input and result. Use ResultPath to control which combination of these is passed to the state output.\r\n\r\nYou can use a Catch field to capture the error in a Task and Parallel State. This field\u2019s value must be an array of objects, known as *catchers*.\r\n\r\nA catcher contains the following fields:\r\n\r\n**ErrorEquals** \u2013 A non-empty array of strings that match error names.\r\n\r\n**Next** \u2013 A string that must exactly match one of the state machine\u2019s state names.\r\n\r\n**ResultPath** \u2013 A path that determines what input is sent to the state specified in the Next field.\r\n\r\nWhen a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher\u2019s ErrorEquals field, the state machine transitions to the state named in the Next field.\r\n\r\n&nbsp;\r\n\r\nThe four states that handle the application logic and error handling are as follows:\r\n\r\nChoice State \u2013 \u201cYes or No\u201d\r\n\r\nTask State \u2013 \u201cYesMessage\u201d and \u201cNoMessage\u201d\r\n\r\nPass State \u2013 \u201cCause Of Error\u201d\r\n\r\nThe workflow is initiated by passing an input of values \u201cyes\u201d or \u201cno\u201d. On the left side, we can see that an error has occurred during the \u201cNoMessage\u201d task state (as labeled by its orange color) when a \u201cno\u201d value was passed as an input.\r\n\r\n&nbsp;\r\n\r\nHence, the correct answer is: **Include a `Catch` field in the state machine definition to capture the errors. Then, use `ResultPath` to include each node\u2019s input data with its output.**\r\n\r\nThe option that says: **Include a `Parameters` field in the state machine definition to capture the errors.** **Then, use `ResultPath` to include each node\u2019s input data with its output** is incorrect because the Parameters field is mainly used for passing a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path. It can\u2019t be used to capture errors in a state.\r\n\r\nThe option that says: **Include a `Catch` field in the state machine definition to capture the errors. Then, use `ItemsPath` to include each node\u2019s input data with its output** is incorrect because the ItemsPath is only applicable in a Map state.\r\n\r\nThe option that says: **Include a `Parameters` field in the state machine definition to capture the errors. Then, use `ItemsPath` to include each node\u2019s input data with its output** is incorrect because the Parameters field cannot be used to capture errors in a state. ItemsPath is also incorrect because this is only applicable in a Map state.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html"
  },
  {
    "id": 134,
    "question": "\r\n\r\nA company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client requests. They have published a version of the AccountService:Prod function with the alias AccountService:Beta. The internal team wants to test these updates before promoting them to production without impacting live users.\r\n\r\nWhich configuration should the company take?\r\n",
    "options": [
      "Update the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter.",
      "Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta.",
      "Modify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta.",
      "Create a \u2018Beta\u2019 stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the AccountService:Prod function that checks for an environment variable and, if set to \u2018Beta\u2019, invokes the AccountService:Beta alias instead"
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta."
    ],
    "correct_answer": "Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta.",
    "explanation": "With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\r\n\r\nFor example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, `http://example.com`). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, `beta.example.com`). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.\r\n\r\nIn the scenario, by creating a new stage named \u2018Beta\u2019, the company can safely test updates by routing internal traffic to this stage, which will reference the `AccountService:Beta` version of the Lambda function. This will allow testers to invoke the new version of the function while end users continue to access the stable, production-ready `AccountService:Prod` version via the \u2018Prod\u2019 stage.\r\n\r\nHence, the correct answer is: **Create a new stage named \u2018Beta\u2019 in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta.**\r\n\r\nThe option that says: **Modify the existing \u2018Prod\u2019 stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta** is incorrect. The \u2018Prod\u2019 stage contains settings critical for the production environment, such as API keys and rate limits. Testing on this stage could disrupt live traffic and affect real users even though you\u2019re hitting a non-prod Lamdba function.\r\n\r\nThe option that says: **Update the integration request settings in the \u2018Prod\u2019 stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter** is incorrect. This is risky since it can expose the beta environment to end users as well.\r\n\r\nThe option that says: **Create a \u2018Beta\u2019 stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the `AccountService:Prod` function that checks for an environment variable and, if set to \u2018Beta\u2019, invokes the `AccountService:Beta` alias instead** is incorrect. This approach would require deploying code changes to the production function, which adds complexity and inadvertently affects the production environment.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\r\n\r\nhttps://aws.amazon.com/blogs/compute/using-api-gateway-stage-variables-to-manage-lambda-functions/"
  },
  {
    "id": 135,
    "question": "\r\n\r\nA developer wants to expose a legacy web service that uses an XML-based Simple Object Access Protocol (SOAP) interface through API Gateway. However, there is a compatibility issue since most modern applications communicate data in JSON format.\r\n\r\nWhich is the most cost-effective method that will overcome this issue?\r\n",
    "options": [
      "Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.",
      "Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.",
      "Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application.",
      "Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway."
    ],
    "correct_answer": "Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.",
    "explanation": "You (or your organization) probably has some existing web services that respond to older protocols such as XML-RPC or SOAP. You can use the API Gateway to modernize these services.\r\n\r\nIn API Gateway, an API\u2019s method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend. API Gateway lets you use `mapping templates` to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.\r\n\r\nHence, the correct answer is: **Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using `mapping templates`. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.**\r\n\r\nThe option that says: **Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using `mapping templates`. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway** is incorrect. The WebSocket protocol is mainly used for applications that require bidirectional persistent connection such as push notifications and chat messaging applications. For this scenario, using a REST-based approach is the more appropriate solution.\r\n\r\nThe option that says: **Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB** is incorrect because ALB is not capable of transforming data.\r\n\r\nThe option that says: **Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application** is incorrect. Although this could work, this means that you\u2019ll have to provision and run an EC2 instance 24/7, which is more expensive than just using a Lambda Function.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\r\n\r\nhttps://github.com/mwittenbols/How-to-use-Lambda-and-API-Gateway-to-consume-XML-instead-of-JSON"
  },
  {
    "id": 136,
    "question": "\r\n\r\nA serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function\u2019s execution time is too slow for the application.\r\n\r\nConsidering that the Lambda function does not return any important data, which method will accelerate data processing the most?\r\n",
    "options": [
      "Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.",
      "Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.",
      "Use synchronous RequestResponse Lambda invocations. Process the files one by one.",
      "Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel."
    ],
    "correct_answer": "Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.",
    "explanation": "Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events. When you invoke a function asynchronously, you don\u2019t wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application.\r\n\r\nFor asynchronous invocation, Lambda places the event in a queue and returns a success response without additional information. A separate process reads events from the queue and sends them to your function. To invoke a function asynchronously, set the invocation type parameter to `Event`.\r\n\r\nSince we are not concerned about waiting for any important data, we can just invoke the Lambda function asynchronously. Because of its non-blocking design, data processing in parallel would be a lot faster than running them in sequence in this kind of scenario. For example, a Lambda function that processes three files for four minutes each would take a total of 12 minutes processing time. If run asynchronously, ideally, the processing time would only take about four minutes.\r\n\r\nHence, the correct answer is: **Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.**\r\n\r\nThe option that says: **Compress the files to reduce their size, then process them with synchronous `RequestResponse` Lambda invocations** is incorrect. Compression may reduce the amount of time it takes to process a file, but the effect won\u2019t be as significant as when running them in parallel.\r\n\r\nThe option that says: **Use synchronous `RequestResponse` Lambda invocations. Process the files one by one** is incorrect because this will not make any improvement in the current processing speed of the Lambda function. Additionally, there is no sense in running the Lambda function synchronously because it is not expected to return any important data.\r\n\r\nThe option that says: **Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation** is incorrect because doing this negates the advantage of using an asynchronous invocation, which is parallelizing workloads. Additionally, there\u2019ll be a higher chance of the Lambda function hitting its maximum execution time.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\r\n\r\nhttps://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/"
  },
  {
    "id": 137,
    "question": "\r\n\r\nA company is running an Artificial Intelligence (AI) software for its automotive clients using the AWS Cloud. The software is used for identifying road obstructions for autonomous driving and predicting failure on vehicle components. The company wants to extend its usage and access based on different levels (students, professionals, and hobbyist developers) by exposing an API through API Gateway. The company should regulate access to the API and monetize it by charging based on usage.\r\n\r\nWhat should the company do?\r\n",
    "options": [
      "Create three Usage Plans. Specify a quota and throttle requests according to the level of access.",
      "Create three stages. Specify a quota and throttle requests according to the level of access.",
      "Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions.",
      "Create three Authorizers to control API access."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create three Usage Plans. Specify a quota and throttle requests according to the level of access."
    ],
    "correct_answer": "Create three Usage Plans. Specify a quota and throttle requests according to the level of access.",
    "explanation": "A *usage plan* specifies who can access one or more deployed API stages and methods\u2014and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys.\r\n\r\nThis feature allows developers to build and monetize APIs and to create ecosystems around them. You can create usage plans for different levels of access (Bronze, Silver, and Gold), different categories of users (Student, Individual, Professional, or Enterprise), and so forth.\r\n\r\nHence, the correct answer is: **Create three Usage Plans. Specify a quota and throttle requests according to the level of access.**\r\n\r\nThe option that says: **Create three stages. Specify a quota and throttle requests according to the level of access** is incorrect because you have to create a usage plan to monetize APIs. Moreover, you can\u2019t specify a quota in a stage.\r\n\r\nThe option that says: **Create three Authorizers to control API access** is incorrect as this will just deny unauthorized users. The solution must regulate API access from authorized users.\r\n\r\nThe option that says: **Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions** is incorrect. You can\u2019t regulate and monetize API access through CloudWatch.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html"
  },
  {
    "id": 138,
    "question": "\r\n\r\nA developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS messages that are larger than 256 KB and up to 1 GB in size.\r\n\r\nHow should the developer manage the SQS messages?\r\n",
    "options": [
      "Use Amazon S3 and the Amazon SQS Extended Client Library for Java",
      "Use Amazon S3 and the Amazon SQS HTTP API",
      "Use Amazon S3 and the Amazon SQS CLI",
      "Use Amazon S3 and the Amazon SQS Console"
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Amazon S3 and the Amazon SQS Extended Client Library for Java"
    ],
    "correct_answer": "Use Amazon S3 and the Amazon SQS Extended Client Library for Java",
    "explanation": "To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.\r\n\r\nYou can use the Amazon SQS Extended Client Library for Java to do the following:\r\n\r\n\u2013 Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB\r\n\r\n\u2013 Send a message that references a single message object stored in an S3 bucket\r\n\r\n\u2013 Retrieve the message object from an S3 bucket\r\n\r\n\u2013 Delete the message object from an S3 bucket\r\n\r\nYou can use the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages using Amazon S3 *only* with the AWS SDK for Java. You can\u2019t do this with the AWS CLI, the Amazon SQS console, the Amazon SQS HTTP API, or any of the other AWS SDKs.\r\n\r\nHence, the correct answer is: **Use Amazon S3 and the Amazon SQS Extended Client Library for Java.**\r\n\r\n**Using Amazon S3 and the Amazon SQS Console** is incorrect because the SQS Console does not support the storing of SQS messages that exceed 256 KB.\r\n\r\n**Using Amazon S3 and the Amazon SQS HTTP API** is incorrect because you can\u2019t store large SQS messages in an S3 bucket using the SQS HTTP API.\r\n\r\n**Using Amazon S3 and the Amazon SQS CLI** is incorrect because the SQS CLI does not support storing of SQS messages to Amazon S3 via the Amazon SQS Extended Client Library for Java.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\r\n\r\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html"
  },
  {
    "id": 139,
    "question": "\r\n\r\nA startup plans to use Amazon Cognito User Pools to easily manage their users\u2019 sign-up and sign-in workflows to an application. To save time from designing the User Interface (UI) for the login page, the development team has decided to use Cognito\u2019s built-in UI. However, the product manager finds the UI bland and instructed the developer to include the product logo on the web page.\r\n\r\nHow should the developer meet the above requirements?\r\n",
    "options": [
      "Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.",
      "Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page.",
      "Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings.",
      "Create a login page with the product logo and upload it to Amazon Cognito."
    ],
    "question_type": "single",
    "correct_answers": [
      "Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page."
    ],
    "correct_answer": "Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.",
    "explanation": "You can use the AWS Management Console, or the AWS CLI or API, to specify customization settings for the built-in app UI experience. You can upload a custom logo image to be displayed in the app. You can also choose many CSS customizations.\r\n\r\nYou can specify app UI customization settings for a single client (with a specific clientId) or for all clients (by setting the clientId to ALL). If you specify ALL, the default configuration will be used for every client that has no UI customization set previously. If you specify UI customization settings for a particular client, it will no longer fall back to the ALL configuration.\r\n\r\nIn the given scenario, you can include the product logo on the webpage by uploading the logo in the Cognito app settings under UI customization.\r\n\r\nHence, the correct answer is: **Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.**\r\n\r\nThe option that says: **Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page** is incorrect as this procedure simply won\u2019t work. The logo should be uploaded on the Cognito app settings and not on the S3 bucket.\r\n\r\nThe option that says: **Create a login page with the product logo and upload it to Amazon Cognito** is incorrect. You don\u2019t have to create a login page as this is already hosted by Cognito.\r\n\r\nThe option that says: **Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings** is incorrect because there is no such option in the Cognito app settings.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-app-ui-customization.html\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"
  },
  {
    "id": 140,
    "question": "\r\n\r\nA developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is open to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html displaying a hello world message.\r\n\r\nWhich of the following should the developer do?\r\n",
    "options": [
      "Connect to the instance via port 22. Run the commands that will install and create the Apache webserver.",
      "Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.",
      "Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.",
      "Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts."
    ],
    "correct_answer": "Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.",
    "explanation": "When you launch an instance in Amazon EC2, you have the option of passing **user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts**. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls).\r\n\r\n&nbsp;\r\n\r\nHence, the correct answer is: **Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.**\r\n\r\nThe option that says: **Connect to the instance via port 22. Run the commands that will install and create the Apache webserver** is incorrect because the scenario mentioned that the setting of the instance security group is only open to port 80. The developer will get a timeout error if he tries to connect to the instance via port 22.\r\n\r\nThe option that says: **Connect to the instance via port 80. Run the commands that will install and create the Apache webserver** is incorrect. Although the security group\u2019s port 80 is open to the public, you can not establish a remote connection to the EC2 instance thru port 80. Port number 80 is used for sending and receiving web pages from an HTTP server.\r\n\r\nThe option that says: **Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts** is incorrect because you can not run scripts on metadata. Metadata is just a list of details about your instance. You should add a User Data instead.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html"
  },
  {
    "id": 141,
    "question": "\r\n\r\nA developer is tasked with enhancing the performance of an online learning platform that uses a serverless architecture. The platform relies on Amazon API Gateway to handle user requests, AWS Lambda to process quiz submissions, Amazon DynamoDB to store course progress data, and Amazon S3 to host video lectures. During peak hours, the platform experiences high latency caused by increased read operations on DynamoDB, leading to a poor user experience.\r\n\r\nWhich AWS service or feature should the developer implement to optimize DynamoDB read performance and reduce latency?\r\n",
    "options": [
      "Amazon CloudFront",
      "Amazon DynamoDB Streams",
      "Amazon DynamoDB Accelerator (DAX)",
      "Amazon Elastic Load Balancing"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon DynamoDB Accelerator (DAX)"
    ],
    "correct_answer": "Amazon DynamoDB Accelerator (DAX)",
    "explanation": "**Amazon DynamoDB Accelerator (DAX)** is a fully managed, in-memory caching service designed to enhance the performance of DynamoDB by delivering microsecond response times for read-intensive applications. By offloading read operations to DAX, applications can achieve significantly reduced latency. It is particularly beneficial for use cases such as real-time bidding, social gaming, and trading platforms that demand rapid data access. DAX seamlessly integrates with existing DynamoDB applications, requiring minimal code changes, and supports API compatibility, simplifying the process of adding caching to DynamoDB tables.\r\n\r\nIn addition to performance improvements, DynamoDB Accelerator reduces the operational complexity of managing cache invalidation, data consistency, and cluster maintenance. It operates as a write-through cache, ensuring that data written to DynamoDB is automatically synchronized with the cache, maintaining consistency between the two. DynamoDB Accelerator also supports encryption at rest and transit, enhancing security for sensitive data. With its ability to handle millions of requests per second and its managed infrastructure, DynamoDB Accelerator (DAX) provides a scalable and reliable solution for high-performance data retrieval applications.\r\n\r\nHence, the correct answer is: **Amazon DynamoDB Accelerator (DAX).**\r\n\r\nThe option that says: **Amazon CloudFront** is incorrect because it is primarily designed as a Content Delivery Network (CDN) to cache and deliver static content like videos, images, or web pages with low latency. While it can simply enhance the performance of S3-hosted video lectures, it does not directly address DynamoDB read operations or latency issues.\r\n\r\nThe option that says:**Amazon Elastic Load Balancing** is incorrect. This option only distributes incoming traffic across multiple targets, such as EC2 instances or containers to ensure high availability and fault tolerance. It is unrelated to DynamoDB performance optimization in a serverless architecture and cannot reduce read latency on DynamoDB.\r\n\r\nThe option that says: **Amazon DynamoDB Streams** is incorrect because it is typically used to capture table data changes for event-driven workflows. It simply provides a mechanism to process data updates but does not optimize or improve the performance of read operations, which is the issue described in this scenario.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"
  },
  {
    "id": 142,
    "question": "\r\n\r\nA development team needs to deploy an application revision into three environments: Test, Staging, and Production. The application should be deployed into the Test environment first, then Staging, and then Production.\r\n\r\nWhich approach will conveniently allow the team to deploy the application into different environments?\r\n",
    "options": [
      "Create multiple deployment groups for each environment using AWS CodeDeploy.",
      "Create separate CloudFormation templates for each environment to deploy the application.",
      "Create separate S3 buckets for each environment to deploy the application.",
      "Create, configure, and deploy multiple application projects for each environment using CodeBuild."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create multiple deployment groups for each environment using AWS CodeDeploy."
    ],
    "correct_answer": "Create multiple deployment groups for each environment using AWS CodeDeploy.",
    "explanation": "In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.\r\n\r\nYou can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times. For example, you might use one deployment group to deploy an application revision to a set of instances tagged Test where you ensure the code\u2019s quality. Next, you deploy the same application revision to a deployment group with instances tagged Staging for additional verification. Finally, when you are ready to release the latest application to customers, you deploy to a deployment group that includes instances tagged Production.\r\n\r\nHence, the correct answer is: **Create multiple deployment groups for each environment using AWS CodeDeploy.**\r\n\r\nThe option that says: **Create separate S3 buckets for each environment to deploy the application** is incorrect. While S3 buckets can be used to store application artifacts, they are not designed for managing deployments. This approach would require additional scripting and manual steps to deploy the application from S3 to the respective environments, making it less convenient and more error-prone compared to using a dedicated deployment service like CodeDeploy.\r\n\r\nThe option that says: **Create, configure, and deploy multiple application projects for each environment using CodeBuild** is incorrect because you can\u2019t use AWS CodeBuild to perform code deployments. CodeBuild is simply a service that allows you to compile and run tests on your code before deployment.\r\n\r\nThe option that says: **Create separate CloudFormation templates for each environment to deploy the application** is incorrect because CloudFormation is primarily used for infrastructure as code, but it\u2019s not designed specifically for managing code deployments in a sequential manner across environments. This approach would be inefficient and overly complex compared to using CodeDeploy.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\r\n\r\nhttps://aws.amazon.com/codedeploy/faqs/"
  },
  {
    "id": 143,
    "question": "\r\n\r\nA developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before sending it to the table and to manage the encryption keys.\r\n\r\nWhich of the following features are supported when using KMS? (Select TWO.)\r\n",
    "options": [
      "Re-enabling disabled keys",
      "Importing a custom key material to an asymmetric KMS key",
      "Automatic key rotation for KMS keys in custom key stores",
      "Creation of symmetric encryption and asymmetric KMS keys",
      "Using AWS Certificate Manager as a custom key store"
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Re-enabling disabled keys",
      "Creation of symmetric encryption and asymmetric KMS keys"
    ],
    "correct_answer": "Re-enabling disabled keys",
    "explanation": "**AWS Key Management Service (AWS KMS)** is a managed service that makes it easy for you to create and control KMS keys, the encryption keys used to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic Module Validation Program except in the China (Beijing) and China (Ningxia) Regions. AWS KMS is integrated with most other AWS services that encrypt your data. AWS KMS is also integrated with AWS CloudTrail to log the use of your KMS keys for auditing, regulatory, and compliance needs.\r\n\r\nYou can perform the following key management functions in AWS KMS:\r\n\r\n\u2013 Create symmetric and asymmetric keys where the key material is only ever used within the service\r\n\r\n\u2013 Create symmetric keys where the key material is generated and used within a custom key store under your control.\r\n\r\n\u2013 Import your own symmetric key for use within the service.\r\n\r\n\u2013 Create both symmetric and asymmetric data key pairs for local use within your applications.\r\n\r\n\u2013 Define which IAM users and roles can manage keys.\r\n\r\n\u2013 Define which IAM users and roles can use keys to encrypt and decrypt data.\r\n\r\n\u2013 Choose to have keys that were generated by the service to be automatically rotated on an annual basis.\r\n\r\n\u2013 Temporarily disable keys so they cannot be used by anyone.\r\n\r\n\u2013 Re-enable disabled keys.\r\n\r\n\u2013 Schedule the deletion of keys that you no longer use.\r\n\r\n\u2013 Audit the use of keys by inspecting logs in AWS CloudTrail.\r\n\r\nBy default, AWS KMS creates the key material for a KMS key. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the KMS key. However, you can import your own key material into a KMS key or create the key material for it in the AWS CloudHSM cluster associated with an AWS KMS custom key store. There are also types of KMS keys that are not eligible for automatic key rotation such as asymmetric keys, keys in custom key stores, and keys with imported key material.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Re-enabling disabled keys.**\r\n\r\n**\u2013 Creation of symmetric encryption and asymmetric KMS keys.**\r\n\r\nThe option that says: **Using AWS Certificate Manager as a custom key store** is incorrect because you can simply use AWS CloudHSM as a custom key store for AWS KMS.\r\n\r\nThe option that says: **Importing a custom key material to an asymmetric KMS key** is incorrect because you can only import your own key material into symmetric keys, not asymmetric keys.\r\n\r\nThe option that says: **Automatic key rotation for KMS keys in custom key stores** is incorrect because automatic key rotation is only supported in symmetric encryption KMS keys. Automatic key rotation is not available for asymmetric keys, keys in custom key stores, and keys with imported key material.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\r\n\r\nhttps://aws.amazon.com/kms/faqs/"
  },
  {
    "id": 144,
    "question": "\r\n\r\nA company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house authentication system for user identification. Each user identifier must be kept consistent across devices and platforms.\r\n\r\nHow can the developer achieve this?\r\n",
    "options": [
      "Generate a unique IAM access key for each user and use the access key ID as the unique identifier.",
      "Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.",
      "Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.",
      "Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users."
    ],
    "correct_answer": "Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.",
    "explanation": "**Amazon Cognito** supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and Amazon Cognito.\r\n\r\nDevelopers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon Cognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the identifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with the Cognito sync service or retrieve temporary, limited-privilege AWS credentials to securely access your AWS resources.\r\n\r\nThe process is simple, you first request a token for your users by using the server-side Cognito API for developer authenticated identities. Cognito then creates a valid token for your users. You can then exchange this token with Amazon Secure Token Service for AWS credentials.\r\n\r\nWith developer authenticated identities, a new API, **`GetOpenIdTokenForDeveloperIdentity`**, was introduced. This API call replaces the use of **`GetId`** and **`GetOpenIdToken`** (APIs needed in the basic authflow) from the device and should be called from your backend as part of your own authentication API. Because this API call is signed by your AWS credentials, Cognito can trust that the user identifier supplied in the API call is valid. This replaces the token validation Cognito performs with public providers.\r\n\r\nHence, the correct answer is: **Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.**\r\n\r\nThe option that says: **Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table** is incorrect because this would produce multiple identifiers for a single user. The scenario requires you to identify users via a unique identifier regardless of the device they use.\r\n\r\nThe option that says: **Generate a unique IAM access key for each user and use the access key ID as the unique identifier** is incorrect because this means that you\u2019ll have to create a unique IAM user (with programmatic access) for each user just for the sake of identification, which is impractical. IAM user is mainly used for accessing services in an AWS account and not for web application authentication.\r\n\r\nThe option that says: **Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers** is incorrect because this requires creating an IAM Role for each user. An IAM Role is not suited for web application authentication, not to mention, that it has a limit per account. Using it to identify game users is infeasible and will not scale well, especially for an application with millions of users.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\r\n\r\nhttps://aws.amazon.com/blogs/mobile/understanding-amazon-cognito-authentication-part-2-developer-authenticated-identities/\r\n\r\nhttps://aws.amazon.com/blogs/mobile/amazon-cognito-announcing-developer-authenticated-identities/"
  },
  {
    "id": 145,
    "question": "\r\n\r\nAn application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local machine. When run, the application\u2019s process takes longer than expected. You are suspecting that the new function sendRequest() you added is the culprit.\r\n\r\nWhich of the following actions should you do to determine the latency of the function?\r\n",
    "options": [
      "Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.",
      "Use CloudTrail to record and store event logs for actions made by your function.",
      "Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.",
      "Using CloudWatch, troubleshoot the issue by checking the logs."
    ],
    "question_type": "single",
    "correct_answers": [
      "Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function."
    ],
    "correct_answer": "Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.",
    "explanation": "AWS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can identify performance bottlenecks, edge case errors, and other hard to detect issues.\r\n\r\nA segment can break down the data about the work done into **subsegments**. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. **You can define arbitrary subsegments to instrument specific functions or lines of code in your application**.\r\n\r\nSubsegments extend a trace\u2019s segment with details about work done in order to serve a request. Each time you make a call with an instrumented client, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to measure the performance of a section of code, or to record annotations and metadata.\r\n\r\nHence, the correct answer is: **Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.**\r\n\r\nThe option that says: **Using AWS X-Ray,** **disable sampling to efficiently trace all requests for calls** is incorrect because sampling is simply used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests.\r\n\r\nThe option that says: **Using CloudWatch,** **troubleshoot the issue by checking the logs** is incorrect because CloudWatch is not suited for debugging applications. CloudWatch is just used to capture performance metrics and log data. But, it can not help you debug the applications\u2019 internal logic flow or determine where the potential bottlenecks are.\r\n\r\nThe option that says: **Use CloudTrail to** **record and store event logs for actions made by your function** is incorrect because CloudTrail just tracks user activity and API usage to enable governance, compliance, operational auditing, and risk auditing of your AWS account.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-subsegments.html"
  },
  {
    "id": 146,
    "question": "\r\n\r\nA company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently, a new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback.\r\n\r\nWhat happens if the deployment of the new version fails?\r\n",
    "options": [
      "CodeDeploy reroutes traffic back to the blue environment and terminates the green environment.",
      "CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.",
      "CodeDeploy redeploys the last known good version of an application with a new deployment ID.",
      "CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a SUCCEEDED status."
    ],
    "question_type": "single",
    "correct_answers": [
      "CodeDeploy redeploys the last known good version of an application with a new deployment ID."
    ],
    "correct_answer": "CodeDeploy redeploys the last known good version of an application with a new deployment ID.",
    "explanation": "In AWS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the latest application revision is installed. After installation, the new version of the application is started and validated.\r\n\r\n&nbsp;\r\n\r\nYou can configure a deployment group or deployment to automatically roll back when a deployment fails or when a monitoring threshold you specify is met. In this case, CodeDeploy redeploys the last known good version of an application revision. These rolled-back deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment.\r\n\r\nHence, the correct answer is: **CodeDeploy redeploys the last known good version of an application with a new deployment ID.**\r\n\r\nThe option that says: **CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a `SUCCEEDED` status** is incorrect. CodeDeploy doesn\u2019t \u201cpause\u201d deployments during a rollback. Instead, it initiates a new deployment with a new unique deployment ID for the last known good revision. It does not reuse the deployment ID of the previous successful deployment.\r\n\r\nThe option that says: **CodeDeploy reroutes traffic back to the blue environment and terminates the green environment** is incorrect because this describes a \u201cblue/green\u201d deployment strategy. While AWS CodeDeploy does support blue/green deployments, the question explicitly mentioned \u201cin-place deployments.\u201d, where the application is directly updated on the existing instances.\r\n\r\nThe option that says: **CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment** is incorrect. AWS CodeDeploy does not take or revert to AMI snapshots as part of its deployment process. Instead, it focuses on deploying application revisions and keeps a record of these revisions. The process of managing AMIs and snapshots is outside the scope of CodeDeploy.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\r\n\r\nhttps://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/in-place-deployments.html"
  },
  {
    "id": 147,
    "question": "\r\n\r\nA developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud Development Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment.\r\n\r\nGiven that AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)\r\n",
    "options": [
      "Execute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.",
      "Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function.",
      "Execute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with function identifiers.",
      "Run the cdk synth command and indicate the stack name of Lambda functions to be tested.",
      "Run the cdk bootstrap command to prepare the staging of CDK assets."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function.",
      "Run the cdk synth command and indicate the stack name of Lambda functions to be tested."
    ],
    "correct_answer": "Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function.",
    "explanation": "The `sam local invoke` command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment. However, to test resources defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where the `cdk synth` command comes into play. It synthesizes or \u201ccompiles\u201d your CDK application into an AWS CloudFormation template.\r\n\r\n&nbsp;\r\n\r\nBy running `cdk synth`, you generate the necessary CloudFormation template that `sam local invoke` can be used to locally execute and test your Lambda functions.\r\n\r\nHence, the correct answers are:\r\n\r\n\u2013 **Run the `cdk synth` command and indicate the stack name of Lambda functions to be tested.**\r\n\r\n****\u2013 Execute the `sam local invoke` command and specify the location of the synthesized CloudFormation template and identifier of each function.****\r\n\r\nThe option that says: **Run the `cdk bootstrap` command to prepare the staging of CDK assets** is incorrect. This command simply sets up the necessary resources in the AWS account to manage the deployments of CDK applications. It\u2019s a prerequisite step for deploying resources, not for local testing.\r\n\r\nThe option that says: **Execute the `sam package` and specify the S3 bucket where the Lambda code will be uploaded for local testing** is incorrect. This just packages a CloudFormation template for deployment to AWS.\r\n\r\nThe option that says: **Execute the `sam local start-lambda` and specify the location of the synthesized CloudFormation template along with function identifiers** is incorrect. While this command is indeed used for local testing, it starts a local endpoint that emulates the AWS Lambda service. This is more suitable for scenarios where you want to test the Lambda function as it would be invoked by other AWS services. Since the developer wants to test only some of the Lambda functions, the `sam local invoke` command is a more direct and appropriate choice for invoking specific Lambda functions one at a time.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cdk/v2/guide/troubleshooting.html\r\n\r\nhttps://repost.aws/knowledge-center/cdk-customize-bootstrap-cfntoolkit\r\n\r\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html"
  },
  {
    "id": 148,
    "question": "\r\n\r\nAn update was made on an AWS Lambda-based application. It is invoked by an API Gateway endpoint with caching enabled to improve latency requests. The developer expected to get the latest data as a response when he tested the application. However, he kept getting stale data upon trying many times.\r\n\r\nWhat should the developer do that will require the LEAST amount of effort to resolve the issue? (Select TWO.)\r\n",
    "options": [
      "Create a new REST API endpoint and disable caching.",
      "Grant permission to the client to invalidate caching when there\u2019s a request using the IAM execution role.",
      "Include Cache-Control: max-age=0 HTTP header on the API request.",
      "Set the new endpoint as a trigger for the lambda function.",
      "Include Cache-Control: no-cache HTTP header on the API request."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Grant permission to the client to invalidate caching when there\u2019s a request using the IAM execution role.",
      "Include Cache-Control: max-age=0 HTTP header on the API request."
    ],
    "correct_answer": "Grant permission to the client to invalidate caching when there\u2019s a request using the IAM execution role.",
    "explanation": "A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the `Cache-Control: max-age=0` header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.\r\n\r\nTo grant permission for a client, attach a policy of the following to an IAM execution role for the user.\r\n\r\nThis policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (or resources).\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Include** **`Cache-Control: max-age=0`** **HTTP header on the API request.**\r\n\r\n**\u2013 Grant permission to the client to invalidate caching when there\u2019s a request using the IAM execution role.**\r\n\r\nThe option that says: **Create a new REST API endpoint and disable caching** is incorrect. You don\u2019t have to create a new REST API endpoint so you can disable caching. Invalidate cache is already a built-in feature on API Gateway.\r\n\r\nThe option that says: **Include** **`Cache-Control: no-cache`** **HTTP header on the API request** is incorrect. Although \u201cno-cache\u201d is a valid value for the Cache-Control HTTP header, it is not the right value when invalidating API Gateway cache. The `Cache-Control: max-age=0` header must be used.\r\n\r\nThe option that says: **Set the new endpoint as a trigger for the lambda function** is incorrect. There is no sense in setting a new endpoint as a trigger for the lambda function as you can use the existing endpoint.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"
  },
  {
    "id": 149,
    "question": "\r\n\r\nA microservices application\u2019s Customer and Payment service components have two separate DynamoDB tables. New items inserted into the Customer service table must be dynamically updated in the Payment service table.\r\n\r\nHow can the Payment service get near real-time update\r\n",
    "options": [
      "Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.",
      "Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table.",
      "Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.",
      "Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table."
    ],
    "correct_answer": "Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.",
    "explanation": "**DynamoDB Streams** captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time.\r\n\r\nAmazon DynamoDB is integrated with AWS Lambda so that you can create triggers\u2014pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\r\n\r\nIf you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table\u2019s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.\r\n\r\nThe Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to persist changes from one DynamoDB database to another.\r\n\r\nHence, the correct answer is: **Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.**\r\n\r\nThe option that says: **Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table** is incorrect because DynamoDB does not support Data Firehose, hence, this solution is not possible.\r\n\r\nThe option that says: **Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table** is incorrect. Kinesis Data Stream is a valid service for streaming changes from a DynamoDB table. However, it cannot directly write stream records to a DynamoDB table. You need a processing layer that will poll records from the stream and update the other DynamoDB table.\r\n\r\nThe option that says: **Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table** is incorrect. With this method, you would have to update new items in batches. Take note that the requirement is to stream changes to the Payment service table in near real-time.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\r\n\r\nhttps://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"
  },
  {
    "id": 150,
    "question": "\r\n\r\nA developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version to 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted.\r\n\r\nWhat change should the developer do?\r\n",
    "options": [
      "Trigger a Lambda function to copy the source code to another S3 bucket.",
      "Configure the Retention setting to retain the source bundle in the S3 bucket.",
      "Modify the value of the Set the application versions limit by age option to zero.",
      "Modify the value of the Set application versions limit by the total count option to zero."
    ],
    "question_type": "single",
    "correct_answers": [
      "Configure the Retention setting to retain the source bundle in the S3 bucket."
    ],
    "correct_answer": "Configure the Retention setting to retain the source bundle in the S3 bucket.",
    "explanation": "Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don\u2019t delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new versions of that application.\r\n\r\nYou can avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified number.\r\n\r\nElastic Beanstalk applies an application\u2019s lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy.\r\n\r\nIn the Retention section of the application version lifecycle settings, you can either choose to delete the source bundle in S3 or retain it. To solve the problem in the scenario, the Retention option must be configured to the Retain source bundle in S3.\r\n\r\nHence, the correct answer is: **Configure the retention setting to retain the source bundle in S3.**\r\n\r\nThe option that says: **Modify the value of the `Set application versions limit by the total count` option to zero** is incorrect because the minimum number that you can set for the application versions limit by total count is 1.\r\n\r\nThe option that says: **Trigger a Lambda function to copy the source code to another S3 bucket** is incorrect. Although this is possible, this solution entails unnecessary configurations as you can just change the Retention of the application version lifecycle settings.\r\n\r\nThe option that says: **Modify the value of the `Set the application versions limit by age` option to zero** is incorrect. You can\u2019t set an application version limit by age that is less than 1.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\r\n\r\nhttps://aws.amazon.com/about-aws/whats-new/2017/05/aws-elastic-beanstalk-supports-version-lifecycle-management/"
  },
  {
    "id": 151,
    "question": "\r\n\r\nA developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda function (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \"No 'Access-Control-Allow-Origin' header is present on the requested resource\" error message.\r\n\r\nWhat should the developer do to resolve this issue?\r\n",
    "options": [
      "In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.",
      "Set the value of the Access-Control-Allow-Credentials header to true.",
      "In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.",
      "Set the value of the Access-Control-Max-Age header to 0."
    ],
    "question_type": "single",
    "correct_answers": [
      "In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource."
    ],
    "correct_answer": "In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.",
    "explanation": "**Cross-origin resource sharing (CORS)** is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts running in the browser. If your REST API\u2019s resources receive non-simple cross-origin HTTP requests, you need to enable CORS support.\r\n\r\nFor a Lambda custom (non-proxy) integration, HTTP custom (non-proxy) integration, or AWS service integration, you can set up the required headers by using API Gateway method response and integration response settings. When you enable CORS by using the AWS Management Console, API Gateway creates an `OPTIONS` method and attempts to add the `Access-Control-Allow-Origin` header to your existing method integration responses.\r\n\r\nHence, the correct answer is: **In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.**\r\n\r\nThe option that says: **In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted** is incorrect. This could be a possible solution if the website is interacting with resources from another website-enabled S3 bucket.\r\n\r\nThe option that says: **Set the value of the `Access-Control-Max-Age` header to 0** is incorrect because this header is simply used to indicate how long the results of a preflight request can be cached.\r\n\r\nThe option that says: **Set the value of the `Access-Control-Allow-Credentials` header to `true`** is incorrect because this is just a response header that tells browsers whether to expose the response to frontend JavaScript code when the request\u2019s credentials mode (`Request.credentials`) is `include`.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers"
  },
  {
    "id": 152,
    "question": "\r\n\r\nA San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your mobile app authenticates with the Identity Provider (IdP) using the provider\u2019s SDK and Amazon Cognito. Once the end-user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.\r\n\r\nWhich of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?\r\n",
    "options": [
      "Cognito SDK",
      "Cognito API",
      "Cognito ID",
      "Cognito Key Pair"
    ],
    "question_type": "single",
    "correct_answers": [
      "Cognito ID"
    ],
    "correct_answer": "Cognito ID",
    "explanation": "You can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources. Amazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier (identity ID) for your end-user immediately if you\u2019re allowing unauthenticated users or after you\u2019ve set the login tokens in the credentials provider if you\u2019re authenticating users.\r\n\r\nWhen your mobile app authenticates with the Identity Provider (IdP) using Amazon Cognito, the token returned from the IdP is passed to Amazon Cognito, which then returns a Cognito ID for the user. This Cognito ID is used to provide a set of temporary, limited-privilege AWS credentials through the Cognito Identity Pool.\r\n\r\nHence, the correct answer is: **Cognito ID**.\r\n\r\n**Cognito SDK** is incorrect because this is not a unique Amazon Cognito identifier but a software development kit that is available in various programming languages.\r\n\r\n**Cognito Key Pair** is incorrect because this is not a unique Amazon Cognito identifier but a cryptography key.\r\n\r\n**Cognito API** is incorrect because this is not a unique Amazon Cognito identifier and is primarily used as an Application Programming Interface.\r\n\r\n**Reference:**\r\n\r\nhttp://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html"
  },
  {
    "id": 153,
    "question": "\r\n\r\nA developer needs to view the percentage of used memory and the number of TCP connections of instances inside an Auto Scaling Group. To achieve this, the developer must send the metrics to Amazon CloudWatch.\r\n\r\nWhich approach provides the MOST secure way of authenticating a CloudWatch PUT request?\r\n",
    "options": [
      "Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and store the access key and secret key in the instance\u2019s configuration file.",
      "Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch template to insert the access key and secret key into the instance user data.",
      "Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances.",
      "Modify the existing Auto Scaling launch template to use an IAM role with the cloudwatch:PutMetricData permission for the instances."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances."
    ],
    "correct_answer": "Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances.",
    "explanation": "A *launch template* is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch template, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you\u2019ve launched an EC2 instance before, you specified the same information in order to launch the instance.\r\n\r\nYou can specify your launch template with multiple Auto Scaling groups. However, you can only specify one launch template for an Auto Scaling group at a time, and you can\u2019t modify a launch template after you\u2019ve created it. To change the launch template for an Auto Scaling group, you must create a launch template and then update your Auto Scaling group with it.\r\n\r\nAccess to AWS resources requires permissions. You can create IAM roles and users that include the permissions that you need for the CloudWatch agent to write metrics to CloudWatch and for the CloudWatch agent to communicate with Amazon EC2 and AWS Systems Manager. You use IAM roles on Amazon EC2 instances, and you use IAM users with on-premises servers.\r\n\r\nIn the given scenario, we can create a launch template and specify an IAM role with `cloudwatch:PutMetricData` permission. Then, use that launch template to create an auto-scaling group.\r\n\r\nHence, the correct answer is: **Create an IAM role with `cloudwatch:PutMetricData` permission for the new Auto Scaling launch template from which you launch instances.**\r\n\r\nThe option that says: **Create an IAM user with programmatic access. Attach a `cloudwatch:PutMetricData` permission and store the access key and secret key in the instance\u2019s configuration file** is incorrect because using IAM roles for applications that run on Amazon EC2 instances to provide credentials to the application is more secure.\r\n\r\nThe option that says: **Create an IAM user with programmatic access. Attach a `cloudwatch:PutMetricData` permission and update the Auto Scaling launch template to insert the access key and secret key into the instance via user data** is incorrect because you can\u2019t update a launch template after you\u2019ve created it.\r\n\r\nThe option that says: **Modify the existing Auto Scaling launch template to use an IAM role with the `cloudwatch:PutMetricData` permission for the instances** is incorrect because modifying an existing launch template is not possible.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.html\r\n\r\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html"
  },
  {
    "id": 154,
    "question": "\r\n\r\nA developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The script often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys.\r\n\r\nWhich approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)\r\n\r\n",
    "options": [
      "Create a Global Secondary Index (GSI) with its own read capacity settings.",
      "Implement a logic that immediately retries the batch request.",
      "Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.",
      "Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.",
      "Use the AWS software development kit (AWS SDK) to send batch requests."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.",
      "Use the AWS software development kit (AWS SDK) to send batch requests."
    ],
    "correct_answer": "Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.",
    "explanation": "A single `BatchGetItem` operation can retrieve up to 16 MB of data, which can contain as many as 100 items.\r\n\r\n`BatchGetItem` returns a partial result if:\r\n\r\n\u2013 The response size limit is exceeded\r\n\r\n\u2013 The table\u2019s provisioned throughput is exceeded\r\n\r\n\u2013 More than 1MB per partition is requested\r\n\r\n\u2013 An internal processing failure occurs.\r\n\r\nFor example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate `UnprocessedKeys` value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset.\r\n\r\nIf none of the items can be processed due to insufficient provisioned throughput on all of the tables in the request, then `BatchGetItem` returns a `ProvisionedThroughputExceededException`. If at least one of the items is successfully processed, then BatchGetItem completes successfully while returning the keys of the unread items in `UnprocessedKeys`.\r\n\r\nIf DynamoDB returns any unprocessed items, you should retry the batch operation on those items. It\u2019s recommended that you use an exponential backoff algorithm. Exponential backoff is a technique where, if a request to a server fails, you wait a bit before retrying. If it keeps failing, you wait longer each time. The main idea is to reduce the frequency of calls over time, which helps avoid overloading the server, giving it a better chance to recover and respond successfully.\r\n\r\nIf you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. Adding progressively longer waits between retries using exponential backoff can make the individual requests in the batch much more likely to succeed. You can implement an exponential backoff yourself in your code or simply use the AWS SDK, which comes with automatic retry logic and exponential backoff.\r\n\r\nHence, the correct answers are:\r\n\r\n\u2013 **Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.**\r\n\r\n****\u2013 Use the AWS software development kit (AWS SDK) to send batch requests.****\r\n\r\nThe option that says: **Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling** is incorrect. Although this solution can help improve the throughput of the table and reduce the likelihood of throttling, it\u2019s not the most reliable method for handling partial results. The batch operation can still return partial results even if the table has sufficient read capacity due to other factors such as large response sizes.\r\n\r\nThe option that says: **Create a Global Secondary Index (GSI) with its own read capacity settings** is incorrect. The presence of a GSI does not change how `BatchGetItem` behaves, and it won\u2019t help with managing `UnprocessedKeys.`\r\n\r\nThe option that says: **Implement a logic that immediately retries the batch request** is incorrect. Retrying the batch operation immediately has more chance of failing than succeeding due to throttling.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#BatchOperations\r\n\r\nhttps://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/"
  },
  {
    "id": 155,
    "question": "\r\n\r\nA startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the account by implementing the best practices in managing access keys in AWS.\r\n\r\nWhich actions follow the security best practices in IAM? (Select TWO.)\r\n\r\n",
    "options": [
      "Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes.",
      "Delete any access keys to your AWS account root user.",
      "Maintain at least one access key for your AWS account root user.",
      "Use IAM roles for applications that need access to AWS services.",
      "Save the access key in your application code for convenience."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Delete any access keys to your AWS account root user.",
      "Use IAM roles for applications that need access to AWS services."
    ],
    "correct_answer": "Delete any access keys to your AWS account root user.",
    "explanation": "**AWS Identity and Access Management (IAM)** is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.\r\n\r\nWhen you first create an AWS account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account *root user* and is accessed by signing in with the email address and password that you used to create the account. AWS strongly recommends that you do not use the root user for your everyday tasks, even the administrative ones.\r\n\r\nAccess keys provide programmatic access to AWS. Do not embed access keys within unencrypted code or share these security credentials between users in your AWS account. For applications that need access to AWS, configure the program to retrieve temporary security credentials using an IAM role.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Delete any access keys to your AWS account root user.**\r\n\r\n**\u2013 Use IAM roles for applications that need access to AWS services.**\r\n\r\nThe option that says: **Save the access key in your application code for convenience** is incorrect. Since access keys are long-term credentials, anyone who might get hold of your application code could easily use the access key inside it to use AWS services on behalf of your account as long as the credentials are valid.\r\n\r\nThe option that says: **Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes** is incorrect. AWS recommends changing your own passwords and access keys regularly. Make sure that all IAM users in your account do as well. That way, if a password or access key is compromised without your knowledge, you limit how long the credentials can be used to access your resources.\r\n\r\nThe option that says: **Maintain at least one access key for your AWS account root user** is incorrect because AWS recommends deleting all access keys to root users. You can create a separate IAM user instead if you want to have full admin access over your AWS resources.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#delegate-using-roles\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html"
  },
  {
    "id": 156,
    "question": "\r\n\r\nA developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without interfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours.\r\n\r\nHow can the developer improve this scan operation?\r\n",
    "options": [
      "Use a parallel scan operation.",
      "Perform a rate-limited parallel scan operation.",
      "Perform a rate-limited sequential scan operation.",
      "Perform a rate-limited sequential scan operation."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use a parallel scan operation."
    ],
    "correct_answer": "Use a parallel scan operation.",
    "explanation": "By default, the `Scan` operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional `Scan` operations to retrieve the next 1 MB of data.\r\n\r\nThe larger the table or index being scanned, the more time the `Scan` takes to complete. In addition, a sequential `Scan` might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a `Scan` operation can only read one partition at a time. For this reason, the throughput of a `Scan` is constrained by the maximum throughput of a single partition.\r\n\r\nTo address these issues, the `Scan` operation can logically divide a table or secondary index into multiple *segments*, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own `Scan` request with the following parameters:\r\n\r\n&nbsp;\r\n\r\nTo make the most of your table\u2019s provisioned throughput, you\u2019ll want to use the Parallel Scan API operation so that your scan is distributed across your table\u2019s partitions. But be careful that your scan doesn\u2019t consume your table\u2019s provisioned throughput and cause the critical parts of your application to be throttled. To avoid throttling, you need to rate-limit your client application.\r\n\r\nHence, the correct answer is: **Perform a rate-limited parallel `scan` operation.**\r\n\r\nThe option that says: **Perform a rate-limited sequential `scan` operation** is incorrect because a DynamoDB `scan` operation is sequential by default, therefore, there will be no improvement in the execution time of the current scan operation.\r\n\r\nThe option that says: **Use a parallel `scan` operation** is incorrect because running a parallel scan alone might consume all of your table\u2019s provisioned throughput which may affect your application\u2019s normal workload. You must use a rate-limiter along with it.\r\n\r\nThe option that says: **Use eventually consistent reads for the `scan` operation instead of strongly consistent reads** is incorrect. You might reduce the cost of your provisioned throughput but the scan operation will still run sequentially, making no improvements at all.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html\r\n\r\nhttps://amazon-dynamodb-labs.com/design-patterns/ex2scan/step2.html"
  },
  {
    "id": 157,
    "question": "\r\n\r\nA Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends to exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using Step Functions, enabling each sub-function to run in separate processes.\r\n\r\nWhich of the following type of states should the developer use to run processes?\r\n",
    "options": [
      "Pass State",
      "Parallel State",
      "Task State",
      "Wait State"
    ],
    "question_type": "single",
    "correct_answers": [
      "Task State"
    ],
    "correct_answer": "Task State",
    "explanation": "**AWS Step Functions** is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.\r\n\r\nStep Functions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that are run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem since Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual Step Functions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout.\r\n\r\nIndividual states can make decisions based on their input, perform actions, and pass output to other states. In AWS Step Functions, you define your workflows in the Amazon States Language. The Step Functions console provides a graphical representation of that state machine to help visualize your application logic.\r\n\r\nStates are elements in your state machine. A state is referred to by its *name*, which can be any string, but must be unique within the scope of the entire state machine.\r\n\r\nStates can perform a variety of functions in your state machine:\r\n\r\n**Task State** \u2013 Do some work in your state machine\r\n\r\n**Choice State** \u2013 Make a choice between branches of execution\r\n\r\n**Fail or Succeed State** \u2013 Stop execution with failure or success\r\n\r\n**Pass State** \u2013 Simply pass its input to its output or inject some fixed data, without performing work.\r\n\r\n**Wait State** \u2013 Provide a delay for a certain amount of time or until a specified time/date.\r\n\r\n**Parallel State** \u2013 Begin parallel branches of execution.\r\n\r\n**Map State** \u2013 Dynamically iterate steps.\r\n\r\nOut of all the types of State, only the Task State and the Parallel State can be used to run processes in the state machine. In the given scenario, the application logic inside the Lambda function process data synchronously. In this case, **Task State** should be used.\r\n\r\n**Pass State** is incorrect because this type of state cannot perform work as it simply passes its input data to its output. Pass State is mainly used for constructing and debugging state machines.\r\n\r\n**Parallel State** is incorrect. Although it can be used to run processes in a state machine, this type of state should only be used when you want to run processes asynchronously. Parallel state executes each branch concurrently and independently. In the given scenario, the Lambda function processes data synchronously. This means that each output of a function is piped as an input to the next function. The Task State is much more applicable in this scenario.\r\n\r\n**Wait State** is incorrect because this type of state just provides a delay mechanism to your state machine.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/step-functions/\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html"
  },
  {
    "id": 158,
    "question": "\r\n\r\nSome static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production account. According to the company policy, the sharing of full credentials between accounts is prohibited.\r\n\r\nWhat steps should be done to delegate access across the two accounts? (Select THREE.)\r\n",
    "options": [
      "Set the policy that will grant access to S3 for the IAM role created in the production account.",
      "On the development account, create an IAM role and specify the production account as a trusted entity.",
      "Set the policy that will grant access to S3 for the IAM role created in the development account.",
      "On the production account, create an IAM role and specify the development account as a trusted entity.",
      "Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to the IAM users.",
      "Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users."
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Set the policy that will grant access to S3 for the IAM role created in the production account.",
      "On the production account, create an IAM role and specify the development account as a trusted entity.",
      "Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to the IAM users."
    ],
    "correct_answer": "Set the policy that will grant access to S3 for the IAM role created in the production account.",
    "explanation": "The problem is about delegating access to the development account to use the S3 Bucket on the production account**.**\r\n\r\n**The steps to execute this are as follows:**\r\n\r\n1.  You use the AWS Management Console to establish trust between the Production account (ID number XXXXXXXXXXXX) and the Development account (ID number YYYYYYYYYYYY) by creating an IAM role.\r\n2.  When you create the role, you define the Development account as a trusted entity and specify a permissions policy that allows trusted users to access the S3 bucket.\r\n3.  On the development account, create an STS policy to assume the role created on the production account. This can be done by referencing the ARN of the role that was created to establish trust between the Production account and the Development account.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 On the production account, create an IAM role and specify the development account as a trusted entity.**\r\n\r\n**\u2013 Set the policy that will grant access to S3 for the IAM role created in the production account**\r\n\r\n**\u2013 Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to corresponding IAM users.**\r\n\r\nThe option that says: **On the development account, create an IAM role and specify the production account as a trusted entity** is incorrect. Since the S3 bucket is in the production account, the role should also be created in the production account.\r\n\r\nThe option that says: **Set the policy that will grant access to S3 for the IAM role created in the development account** is incorrect because the policy associated with the role that will grant access to S3 should be created on the production account.\r\n\r\nThe option that says: **Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users** is incorrect. The policy that will use STS should be created on the account you are delegating access to, which is the development account.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\r\n\r\nhttps://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/"
  },
  {
    "id": 159,
    "question": "A company has launched a new serverless application using AWS Lambda. The app ran smoothly for a few weeks until it was featured on a popular website. As its popularity grew, so did the number of users receiving an error. Upon viewing the Lambda function\u2019s monitoring graph, the developer discovered a lot of throttled invocation requests.\r\n\r\nWhat can the developer do to troubleshoot this issue? (Select THREE.)",
    "options": [
      "Use a compiled language like GoLang to improve the function\u2019s performance",
      "Use exponential backoff in the application.",
      "Increase Lambda function timeout",
      "Request a service quota increase",
      "Deploy the Lambda function in VPC",
      "Configure reserved concurrency"
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Use exponential backoff in the application."
    ],
    "correct_answer": "Use exponential backoff in the application.",
    "explanation": "**AWS Lambda** lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\r\n\r\nWith Lambda, you can run code for virtually any type of application or backend service \u2013 all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.\r\n\r\n**Lambda Throttling** refers to the rejection of the Lambda function to invocation requests. At this event, the Lambda will return a throttling error exception which you need to handle. This happens because your current concurrency execution count is greater than your concurrency limit.\r\n\r\nThrottling is intended to protect your resources and downstream applications. Though Lambda automatically scales to accommodate your incoming traffic, your function can still be throttled for various reasons.\r\n\r\nThe following are the recommended solutions to handle throttling issues:\r\n\r\n**Configure reserved concurrency** \u2013 by default, there are 900 unreserved concurrencies shared across all functions in a region. To prevent other functions from consuming the available concurrent executions, reserve a portion of it to your Lambda function based on the demand of your current workload.\r\n\r\n**Use exponential backoff in your app** \u2013 a technique that uses progressively longer waits between retries for consecutive error responses. This can be used to handle throttling issues by preventing collision between simultaneous requests.\r\n\r\n**Use a dead-letter queue** \u2013 If you\u2019re using Amazon S3 and Amazon EventBridge (Amazon CloudWatch Events), configure your function with a dead letter queue to catch any events that are discarded due to constant throttles. This can protect your data if you\u2019re seeing significant throttling.\r\n\r\n**Request a service quota increase** \u2013 you can reach AWS support to request for a higher service quota for concurrent executions.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013 Use exponential backoff in your application**\r\n\r\n**\u2013 Configure reserved concurrency**\r\n\r\n**\u2013 Request a service quota increase**\r\n\r\nThe option that says: **Deploy the Lambda function in VPC** is incorrect because this has nothing to do with fixing throttling errors. The only time you should do this is if you need to have a connection between a Lambda function and a resource running in your VPC.\r\n\r\nThe option that says: **Use a compiled language like GoLang to improve the function\u2019s performance** is incorrect because no matter what language you use, the Lambda function will still throw a throttling error if the current concurrency execution count is greater than your concurrency limit.\r\n\r\nThe option that says: **Increase Lambda function timeout** is incorrect. If the time a function runs exceeds its current timeout value, it will throw a timeout error. The scenario is a throttling issue and not a timeout.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"
  },
  {
    "id": 160,
    "question": "A development team has a serverless architecture composed of multiple Lambda functions that invoke one another. As the number of Lambda functions increases, the team finds it increasingly difficult to manage the coordination and dependencies between them, leading to errors, duplication of code, and difficulty debugging and troubleshooting issues.\r\n\r\nWhich refactorization should the team implement?",
    "options": [
      "Create an AWS Step Functions state machine and convert each Lambda function into individual Task states.",
      "Create an AWS AppSync GraphQL API endpoint and configure each Lambda function as a resolver.",
      "Use AWS AppConfig\u2019s feature flag to gradually release new code changes to each Lambda function.",
      "Use AWS CodePipeline to define the source, build, and deployment stages for each Lambda function."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create an AWS Step Functions state machine and convert each Lambda function into individual Task states."
    ],
    "correct_answer": "Create an AWS Step Functions state machine and convert each Lambda function into individual Task states.",
    "explanation": "**State Machine** is a technique in modeling systems whose output depends on the entire history of their inputs, not just on the most recent input. In this case, the Lambda functions invoke one another, creating a large state machine.\r\n\r\nAWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications.\r\n\r\nStep Functions automatically triggers and tracks each step, and retries when there are errors so your application executes in order and as expected. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and IT automation.\r\n\r\nYou can manage the coordination of a state machine in Step Functions using the Amazon States Language. The Amazon States Language is a JSON-based, structured language used to define your state machine, a collection of states, that can do work (Task states), determine which states to transition to next (Choice states), stop execution with an error (Fail states), and so on.\r\n\r\nHence, the correct answer is **Create an AWS Step Functions state machine and convert each Lambda function into individual Task states.**\r\n\r\nThe option that says: **Use AWS CodePipeline to define the source, build, and deployment stages for each Lambda function** is incorrect. While this approach can help with the deployment process, it does not directly address the coordination and dependency issues between the Lambda functions. AWS CodePipeline is primarily used for automating the build, test, and deploy phases of your release process every time there is a code change.\r\n\r\nThe option that says: **Create an AWS AppSync GraphQL API endpoint and configure each Lambda function as a resolver** is incorrect. AWS AppSync is a service that enables you to query multiple sources from a single GraphQL API endpoint. While it provides features such as schema generation, resolvers, and real-time subscriptions, it is not specifically designed as a management tool for coordinating and managing the dependencies between multiple Lambda functions.\r\n\r\nThe option that says: **Use AWS AppConfig\u2019s feature flag to gradually release new code changes to each Lambda function** is incorrect. While this approach can help with rolling out new code changes, it does not directly address the coordination and dependency issues between the Lambda functions.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/step-functions/\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html"
  },
  {
    "id": 161,
    "question": "A company is storing highly classified documents on its file server. These documents contain blueprints for electronic devices and are never to be made public due to a legal agreement. To comply with the strict policy, you must explore the capabilities of AWS KMS to improve data security.\r\n\r\nWhich of the following is the MOST suitable procedure for encrypting data?",
    "options": [
      "Generate a data key using a KMS key. Then, encrypt data with the ciphertext version of the data key.",
      "Use a symmetric key for encryption and decryption.",
      "Generate a data key using a KMS key. Then, encrypt data with the plaintext data key.",
      "Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric private key to decrypt the data."
    ],
    "question_type": "single",
    "correct_answers": [
      "Generate a data key using a KMS key. Then, encrypt data with the plaintext data key."
    ],
    "correct_answer": "Generate a data key using a KMS key. Then, encrypt data with the plaintext data key.",
    "explanation": "Your data is protected when you encrypt it, but you have to protect your encryption key. One strategy is to encrypt it. *Envelope encryption* is the **practice of encrypting plaintext data with a data key and then encrypting the data key under another key.**\r\n\r\nYou can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key-encryption key is known as the *master key*.\r\n\r\nEnvelope encryption offers several benefits:\r\n\r\n**Protecting data keys**When you encrypt a data key, you don\u2019t have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data.\r\n\r\n**Encrypting the same data under multiple master keys**Encryption operations can be time-consuming, particularly when the data being encrypted are large objects. Instead of re-encrypting raw data multiple times with different keys, you can re-encrypt only the data keys that protect the raw data.\r\n\r\n**Combining the strengths of multiple algorithms**In general, symmetric key algorithms are faster and produce smaller ciphertexts than public-key algorithms. But public-key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets you combine the strengths of each strategy.\r\n\r\nTo perform envelope encryption using KMS keys, you must first generate a data key using your KMS key and use its plaintext version to encrypt data.\r\n\r\nHence, the correct answer is the option that says: **Generate a data key using a KMS key. Then, encrypt data with the plaintext data key.**\r\n\r\nThe option that says: **Generate a data key using a KMS key. Then, encrypt data with the ciphertext version data key** is incorrect. A ciphertext data key cannot be used for encryption because it is an encrypted version of the data key. You must use the plaintext version of the data key.\r\n\r\nThe option that says: **Use a KMS key for encryption and decryption** is incorrect. KMS keys can only be used to encrypt data up to 4KB in size, making it not suitable for encrypting documents.\r\n\r\nThe option that says: **Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric private key to decrypt the data** is incorrect because the keys on each encryption algorithm are separate entities and are in no way related. You cannot encrypt data with a symmetric key and expect it to be decrypted by an asymmetric private key.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping\r\n\r\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html"
  },
  {
    "id": 162,
    "question": "A transcoding media service is being developed in AWS. Photos uploaded to Amazon S3 will trigger Step Functions to coordinate a series of processes that will perform image analysis tasks. The final output should contain the input plus the result of the final state to conform to the application\u2019s logic flow.\r\n\r\nWhat should the developer do?",
    "options": [
      "Declare a Parameters field filter on the Amazon States Language specification.",
      "Declare a ResultPath field filter on the Amazon States Language specification.",
      "Declare an OutputPath field filter on the Amazon States Language specification.",
      "Declare an InputPath field filter on the Amazon States Language specification."
    ],
    "question_type": "single",
    "correct_answers": [
      "Declare a ResultPath field filter on the Amazon States Language specification."
    ],
    "correct_answer": "Declare a ResultPath field filter on the Amazon States Language specification.",
    "explanation": "A Step Functions execution receives a JSON text as input and passes that input to the first state in the workflow. Individual states receive JSON as input and usually pass JSON as output to the next state. Understanding how this information flows from state to state and learning how to filter and manipulate this data is key to effectively designing and implementing workflows in AWS Step Functions.\r\n\r\nIn the Amazon States Language, these fields filter and control the flow of JSON from state to state:\r\n\r\n\u2013 InputPath\r\n\r\n\u2013 OutputPath\r\n\r\n\u2013 ResultPath\r\n\r\n\u2013 Parameters\r\n\r\nBoth the **InputPath** and **Parameters** fields provide a way to manipulate JSON as it moves through your workflow. InputPath can limit the input that is passed by filtering the JSON notation by using a path. The Parameters field enables you to pass a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path.\r\n\r\nAWS Step Functions applies the InputPath field first, and then the Parameters field. You can first filter your raw input to a selection you want using InputPath, and then apply Parameters to manipulate that input further, or add new values.\r\n\r\nThe output of a state can be a copy of its input, the result it produces (for example, the output from a Task state\u2019s Lambda function), or a combination of its input and result. Use **ResultPath** to control which combination of these is passed to the state output.\r\n\r\n**OutputPath** enables you to select a portion of the state output to pass to the next state. This enables you to filter out unwanted information, and pass only the portion of JSON that you care about.\r\n\r\nOut of these field filters, the ResultPath field filter is the only one that can control input values and its previous results to be passed to the state output. Hence, the correct answer is: **Declare a** **ResultPath** **field filter on the Amazon States Language specification.**\r\n\r\nThe option that says: **Declare an** **InputPath** **field** **filter on the Amazon State Language specification** is incorrect because it just operates on the input level by filtering the JSON notation by using a path. It cannot control both ends of a state (input and output).\r\n\r\nThe option that says: **Declare an** **OutputPath** **field** **filter on the Amazon State Language specification** is incorrect because it just operates on the output level. It is used to filter out unwanted information and pass only the portion of JSON that you care about that will be passed onto the next state.\r\n\r\nThe option that says: **Declare a Parameters field filter on the Amazon State Language specification** is incorrect because this is used in conjunction with the InputPath field filter, which means it can only be used on the input level of a state.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/how-step-functions-works.html"
  },
  {
    "id": 163,
    "question": "\r\n\r\nA serverless application consists of multiple Lambda Functions and a DynamoDB table. The application must be deployed by calling the CloudFormation APIs using AWS CLI. The CloudFormation template and the files containing the code for all the Lambda functions are located on a local computer.\r\n\r\nWhat should the Developer do to deploy the application?\r\n",
    "options": [
      "Use the aws cloudformation deploy command.",
      "Use the aws cloudformation package command and deploy using aws cloudformation deploy.",
      "Use the aws cloudformation update-stack command and deploy using aws cloudformation deploy.",
      "Use the aws cloudformation validate-template command and deploy using aws cloudformation deploy."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use the aws cloudformation package command and deploy using aws cloudformation deploy."
    ],
    "correct_answer": "Use the aws cloudformation package command and deploy using aws cloudformation deploy.",
    "explanation": "The `aws cloudformation package` command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.\r\n\r\nUse this command to quickly upload local artifacts that might be required by your template. After you package your template\u2019s artifacts, run the `aws cloudformation deploy` command to deploy the returned template.\r\n\r\nSince we have local artifacts (source code for the AWS Lambda functions), we should use the package command.\r\n\r\nAfter running the package command, we must deploy the packaged output file by running the deploy command.\r\n\r\nHence, the correct answer is: **Use the `aws cloudformation package` command and deploy using `aws cloudformation deploy`.**\r\n\r\nThe option that says: **Use the `aws cloudformation validate-template` command and deploy using `aws cloudformation deploy`** is incorrect because the **`validate-template`** command will just check the template if it is a valid JSON or YAML file.\r\n\r\nThe option that says: **Use the `aws cloudformation deploy` command** is incorrect because the artifacts are located in the local computer and since the deploy command uses S3 as the location for artifacts that were defined in the Cloudformation template, using the deploy command alone will not work. You must package the template first.\r\n\r\nThe option that says: **Use the `aws cloudformation update-stack` command and deploy using `aws cloudformation deploy`** is incorrect because this just updates an existing stack.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/cloudformation/deploy/index.html"
  },
  {
    "id": 164,
    "question": "\r\n\r\nA code that runs on a Lambda function performs a GetItem call from a DynamoDB table. The function runs three times every week. You noticed that the application kept receiving a ProvisionedThroughputExceededException error for 10 seconds most of the time.\r\n\r\nHow should you handle this error?\r\n",
    "options": [
      "Enable DynamoDB Accelerator (DAX) to reduce response times from milliseconds to microseconds.",
      "Create a Local Secondary Index (LSI) to the existing DynamoDB table to increase the provisioned throughput.",
      "Refactor the code in the Lambda function to optimize its performance.",
      "Reduce the frequency of requests using error retries and exponential backoff."
    ],
    "question_type": "single",
    "correct_answers": [
      "Reduce the frequency of requests using error retries and exponential backoff."
    ],
    "correct_answer": "Reduce the frequency of requests using error retries and exponential backoff.",
    "explanation": "When your program sends a request, DynamoDB attempts to process it. If the request is successful, DynamoDB returns an HTTP success status code (`200 OK`), along with the results from the requested operation. If the request is unsuccessful, DynamoDB returns an error.\r\n\r\nAn HTTP `400` status code indicates a problem with your request, such as authentication failure, missing required parameters, or exceeding a table\u2019s provisioned throughput. You have to fix the issue in your application before submitting the request again.\r\n\r\n**ProvisionedThroughputExceededException** means that your request rate is too high. The AWS SDKs for DynamoDB automatically retries requests that receive this exception. Your request is eventually successful unless your retry queue is too large to finish. To handle this error, you can reduce the frequency of requests using error retries and exponential backoff.\r\n\r\nHence, the correct answer is: **Reduce the frequency of requests using error retries and exponential backoff.**\r\n\r\nThe option that says: **Enable DynamoDB Accelerator (DAX) to** **reduce response times from milliseconds to microseconds** is incorrect because DAX is used to provide a fully managed, in-memory caching solution. This option is not the right way to handle errors due to high request rates.\r\n\r\nThe option that says: **Refactor the code in the Lambda function to optimize its performance** is incorrect because this will just improve the code\u2019s readability and maintainability. This won\u2019t have any impact on reducing the frequency of requests.\r\n\r\nThe option that says: **Create a Local Secondary Index ( LSI ) to the existing DynamoDb table to increase the provisioned throughput** is incorrect. LSI is used to give flexibility to your queries against the DynamoDB table. LSI uses an alternative sort key aside from the original sort key defined at the creation of the table. Additionally, you cannot create an LSI on an existing table. It can only be added during the creation of a DynamoDB table.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff\r\n\r\nhttps://docs.aws.amazon.com/general/latest/gr/api-retries.html"
  },
  {
    "id": 165,
    "question": "\r\n\r\nAn IAM user with programmatic access wants to get information about specific EC2 instances on the us-east-1 region. Due to strict policy, the user was compelled to use the describe-instances operation using AWS Command Line Interface (CLI). He wants to check whether he has the required permission to initiate the command without actually making the request.\r\n\r\nWhich of the following actions should be done to solve the problem?\r\n",
    "options": [
      "Add the --dry-run parameter to the describe-instances command.",
      "Add the --max-items parameter to the describe-instances command.",
      "Add the --generate-cli-skeleton parameter to the describe-instances command.",
      "Add the --filters parameter to the describe-instances command."
    ],
    "question_type": "single",
    "correct_answers": [
      "Add the --dry-run parameter to the describe-instances command."
    ],
    "correct_answer": "Add the --dry-run parameter to the describe-instances command.",
    "explanation": "The **describe-instances** command describes the specified instances or all instances. Optionally, you can add parameters to the describe-instances to modify its function.\r\n\r\nHere is the list of the available parameters for describe-instances:\r\n\r\n*[\u2013dry-run | \u2013no-dry-run]*\r\n\r\n*[\u2013instance-ids ]*\r\n\r\n*[\u2013filters ]*\r\n\r\n*[\u2013cli-input-json ]*\r\n\r\n*[\u2013starting-token ]*\r\n\r\n*[\u2013page-size ]*\r\n\r\n*[\u2013max-items ]*\r\n\r\n*[\u2013generate-cli-skeleton]*\r\n\r\nThe **\u2013dry-run** parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is *DryRun-Operation.* Otherwise, it is *UnauthorizedOperation*.\r\n\r\nHence, the correct answer is: **Add the `--dry-run` parameter to the describe-instances command**\r\n\r\nThe option that says: **Add the `--generate-cli-skeleton` parameter to the describe-instances command** is incorrect because this is a parameter that will generate and display a parameter template that you can customize and use as input on a later command. The generated template includes all of the parameters that the command supports.\r\n\r\nThe option that says: **Add the `--filters` parameter to the describe-instances command** is incorrect. Use this parameter if you want to get only the details that you like from the output of your command. It will not help you check for the permission required to initiate the command.\r\n\r\nThe option that says: **Add the `max-items` parameter to the describe-instances command** is incorrect because it just defines the total number of items to be returned in the command\u2019s output. It has nothing to do with checking the permission required to initiate the command.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-usage-help.html"
  },
  {
    "id": 166,
    "question": "\r\n\r\nA developer is looking for a way to decrease the latency in retrieving data from an Amazon RDS MySQL database. He wants to implement a caching solution that supports Multi-AZ replication with sub-millisecond response times.\r\n\r\nWhat must the developer do that requires the LEAST amount of effort?\r\n",
    "options": [
      "Convert the database schema using the AWS Schema Conversion Tool and move the data to DynamoDB. Enable Amazon DynamoDB Accelerator (DAX).",
      "Set up an Elasticache for Redis cluster between the application and database. Configure it to run with replication to achieve high availability.",
      "Set up AWS Global Accelerator and integrate it with your application to improve overall performance.",
      "Set up an Elasticache for Memcached cluster between the application and database. Configure it to run with replication to achieve high availability."
    ],
    "question_type": "single",
    "correct_answers": [
      "Set up an Elasticache for Redis cluster between the application and database. Configure it to run with replication to achieve high availability."
    ],
    "correct_answer": "Set up an Elasticache for Redis cluster between the application and database. Configure it to run with replication to achieve high availability.",
    "explanation": "**Amazon ElastiCache** allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.\r\n\r\nAmazon ElastiCache offers fully managed Redis and Memcached for most demanding applications that require sub-millisecond response times. However, Redis is the only service in Elasticache that supports replication.\r\n\r\nHence, the correct answer is: **Set up an Elasticache for Redis between the application and database. Configure it to run with replication to achieve high availability**.\r\n\r\nThe option that says: **Convert the database schema using the AWS Schema Conversion Tool and move the data to DynamoDB. Enable Amazon DynamoDB Accelerator (DAX)** is incorrect. DAX is a fully managed in-memory cache for DynamoDB. You don\u2019t have to change the schema of the MySQL database just to achieve a high-performing, high-availability caching solution. You can readily do that with Elasticache. Additionally, changing a schema takes a lot of work, which fails to meet the required solution for the problem.\r\n\r\nThe option that says: **Set up an Elasticache for Memcached cluster between the application and database. Configure it to run with replication to achieve high availability** is incorrect. While Memcached provides sub-millisecond latency, it does not support replication.\r\n\r\nThe option that says: **Set up AWS Global Accelerator and integrate it with your application to improve overall performance** is incorrect because Global Accelerator is not a caching solution. It is a service used to improve the performance of your network traffic by utilizing the AWS global infrastructure instead of the public Internet.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/\r\n\r\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html"
  },
  {
    "id": 167,
    "question": "\r\n\r\nA full-stack developer has developed an application written in Node.js to host an upcoming mobile game tournament. The developer has decided to deploy the application using AWS Elastic Beanstalk because of its ease-of-use. Upon experimenting, he learned that he could configure the webserver environment with several resources.\r\n\r\nWhich of the following services can the developer configure with Elastic Beanstalk? (Select THREE.)\r\n",
    "options": [
      "Amazon EC2 Instance",
      "AWS Lambda",
      "Amazon CloudWatch",
      "Amazon Athena",
      "Amazon CloudFront",
      "Application Load Balancer"
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Amazon EC2 Instance",
      "Amazon CloudWatch",
      "Application Load Balancer"
    ],
    "correct_answer": "Amazon EC2 Instance",
    "explanation": "**AWS Elastic Beanstalk** is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\r\n\r\nYou can upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources.\r\n\r\nWith ElasticBeanstalk, you can:\r\n\r\n\u2013 Select the operating system that matches your application requirements (e.g., Amazon Linux or Windows Server 2016)\r\n\r\n\u2013 Choose from several Amazon EC2 instances, including On-Demand, Reserved Instances, and Spot Instances.\r\n\r\n\u2013 Choose from several available database and storage options.\r\n\r\n\u2013 Enable login access to Amazon EC2 instances for immediate and direct troubleshooting\r\n\r\n\u2013 Quickly improve application reliability by running in more than one Availability Zone.\r\n\r\n\u2013 Enhance application security by enabling HTTPS protocol on the load balancer\r\n\r\n\u2013 Access built-in Amazon CloudWatch monitoring and getting notifications on application health and other important events\r\n\r\n\u2013 Adjust application server settings (e.g., JVM settings) and pass environment variables\r\n\r\n\u2013 Run other application components, such as a memory caching service, side-by-side in Amazon EC2.\r\n\r\n\u2013 Access log files without logging in to the application servers\r\n\r\nHence, the correct answers are: **Amazon EC2 Instance, Amazon CloudWatch,** and **Application Load Balancer.**\r\n\r\nYou cannot configure **Amazon Athena, AWS Lambda**, and **Amazon CloudFront** on ElasticBeanstalk.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/elasticbeanstalk/faqs/\r\n\r\nhttps://aws.amazon.com/elasticbeanstalk/"
  },
  {
    "id": 168,
    "question": "\r\n\r\nA developer is writing a custom script that will run in an Amazon EC2 instance. The script needs to access the local IP address from the instance to manage a connection to an application outside the AWS Cloud. The developer found out that the details about an instance can be viewed by visiting a certain Uniform Resource Identifier (URI).\r\n\r\nWhich of the following is the correct URI?\r\n",
    "options": [
      "http://169.254.169.254/latest/user-data/",
      "http://254.169.254.169/latest/meta-data/",
      "http://169.254.169.254/latest/meta-data/",
      "http://254.169.254.169/latest/user-data/"
    ],
    "question_type": "single",
    "correct_answers": [
      "http://169.254.169.254/latest/meta-data/"
    ],
    "correct_answer": "http://169.254.169.254/latest/meta-data/",
    "explanation": "**Instance metadata** is the data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, hostname, events, and security groups.\r\n\r\nTo view all categories of instance metadata from within a running instance, use the http://169.254.169.254/latest/meta-data/ URI.\r\n\r\nNote that the IP address 169.254.169.254 is a link-local address and is valid only from the instance.\r\n\r\nHence, the correct answer is **http://169.254.169.254/latest/meta-data/**.\r\n\r\nThe option that says: **http://169.254.169.254/latest/user-data/** is incorrect because this URI is used to retrieve user data from within a running instance. The correct path should be \u201c/latest/meta-data/\u201d.\r\n\r\nThe option that says: **http://254.169.254.169/latest/user-data/** is incorrect because that is not the right IP address. The IP address should be: 169.254.169.254.\r\n\r\nThe option that says: **http://254.169.254.169/latest/meta-data/** is incorrect. Although the path is right, the IP address used is invalid.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html#instancedata-meta-data-retrieval-examples\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"
  },
  {
    "id": 169,
    "question": "A company wants to centrally organize login credentials for its internal application. The application prompts users to change passwords every 35 days. Expired login credentials must be removed automatically, and an email notification should be sent to the users when their passwords are about to expire. A developer must create a solution with the least amount of development effort.\r\n\r\nWhich solution meets the requirements?\r\n\r\n\r\n",
    "options": [
      "Use AWS Secret Managers to store user credentials and turn on automatic rotation.",
      "Store the credentials as Standard Parameters in AWS Systems Manager (SSM) Parameter Store and configure Expiration and ExpirationNotification policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications.",
      "Use AWS Secrets Manager to store user credentials. Create a Lambda function that runs periodically to send Amazon SNS email notifications for passwords nearing expiration",
      "Store the credentials as Advanced Parameters in AWS Systems Manager (SSM) Parameter Store and configure Expiration and ExpirationNotification policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications."
    ],
    "question_type": "single",
    "correct_answers": [
      "Store the credentials as Advanced Parameters in AWS Systems Manager (SSM) Parameter Store and configure Expiration and ExpirationNotification policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications."
    ],
    "correct_answer": "Store the credentials as Advanced Parameters in AWS Systems Manager (SSM) Parameter Store and configure Expiration and ExpirationNotification policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications.",
    "explanation": "**Parameter Store**, a capability of AWS Systems Manager, includes *standard parameters* and *advanced parameters*. You individually configure parameters to use either the standard-parameter tier (the default tier) or the advanced-parameter tier.\r\n\r\nYou can change a standard parameter to an advanced parameter at any time, but you can\u2019t revert an advanced parameter to a standard parameter. This is because reverting an advanced parameter to a standard parameter would cause the system to truncate the size of the parameter from 8 KB to 4 KB, resulting in data loss. Reverting would also remove any policies attached to the parameter. Also, advanced parameters use a different form of encryption than standard parameters.\r\n\r\nParameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter, such as an expiration date or *time to live*. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager.\r\n\r\nYou can assign multiple policies to a parameter. For example, you can assign `Expiration` and `ExpirationNotification` policies so that the system initiates an EventBridge event to notify you about the impending deletion of a parameter. The `Expiration` policy lets you delete a parameter at a specified time while the `ExpirationNotification` policy is used to notify when a parameter is about to expire. These features are **only available for Advanced Parameters** in the AWS Systems Manager Parameter Store.\r\n\r\nHence, the correct answer is: **Store the credentials as Advanced Parameters in AWS Systems Manager (SSM) Parameter Store and configure `Expiration` and `ExpirationNotification` policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications.**\r\n\r\nThe option that says: **Use AWS Secrets Manager to store user credentials. Create a Lambda function that runs periodically to send Amazon SNS email notifications for passwords nearing expiration** is incorrect. Although it\u2019s possible to store login credentials in Secrets Manager, creating a cron-based Lambda function for checking password expiration and sending notifications via SNS takes more development overhead than simply using the `ExpirationNotification` policy in SSM Parameter Store.\r\n\r\nThe option that says: **Use AWS Secrets Manager to store user credentials and turn on automatic rotation** is incorrect. Automatic rotation in AWS Secrets Manager is typically used when secrets can be changed programmatically without user intervention (e.g, rotating database credentials). In the scenario\u2019s case, users must manually change their passwords.\r\n\r\nThe option that says: **Store the credentials as Standard Parameters in AWS Systems Manager (SSM) Parameter Store and configure `Expiration` and `ExpirationNotification` policies. Create an Amazon EventBridge rule that sends Amazon SNS email notifications** is incorrect because Standard Parameters do not support `Expiration` and `ExpirationNotification` policies.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-advanced-parameters.html"
  },
  {
    "id": 170,
    "question": "A developer manages a web application hosted on a fleet of Amazon EC2 instances behind a public Application Load Balancer (ALB). Each instance is equipped with an HTTP server that logs incoming requests. Upon reviewing the logs, the developer realizes that they are only capturing the IP address of the ALB instead of the client\u2019s public IP address.\r\n\r\nWhat modification should the developer make to ensure the log files include the client\u2019s public IP address?\r\n\r\n",
    "options": [
      "Configure the HTTP server to include the Host header in its logging configuration.",
      "Deploy the Amazon CloudWatch Logs agent on each instance, customizing it to capture and log the client\u2019s IP address.",
      "Update the HTTP server\u2019s logging configuration to log the X-Forwarded-For header information.",
      "Implement the AWS X-Ray daemon on all instances, adjusting its settings to record the client\u2019s IP address in the logs."
    ],
    "question_type": "single",
    "correct_answers": [
      "Update the HTTP server\u2019s logging configuration to log the X-Forwarded-For header information."
    ],
    "correct_answer": "Update the HTTP server\u2019s logging configuration to log the X-Forwarded-For header information.",
    "explanation": "The `X-Forwarded-For` header helps keep track of a user\u2019s IP address as their internet request moves through things like proxy servers or load balancers on its way to the final server. This way, the final server can still figure out where the request originated, even with these helpers in the middle that might hide user information. This detail is critical for correctly identifying users, safeguarding the network, and analyzing website traffic when the user\u2019s request doesn\u2019t come directly to the server.\r\n\r\n&nbsp;\r\n\r\nWhen a request hits an Application Load Balancer, the balancer automatically includes or updates the `X-Forwarded-For` header with the client\u2019s IP address, ensuring the server knows the request\u2019s source. If this header isn\u2019t part of the request, the balancer adds it to the client\u2019s IP. Should the header exist, the balancer appends the client\u2019s IP. This header can list several IP addresses, separated by commas, indicating the path the request took through various proxies, with the original client\u2019s IP being the first in the list.\r\n\r\nHence, the correct answer is: **Update the HTTP server\u2019s logging configuration to log the `X-Forwarded-For` header information.**\r\n\r\nThe option that says: **Configure the HTTP server to include the Host header in its logging configuration** is incorrect because the Host header is primarily used to specify the server\u2019s domain name and the TCP port number on which the server is listening. While it helps identify the target server in requests, it does not provide information about the client\u2019s IP address, thus not meeting the requirement to log the client\u2019s public IP address.\r\n\r\nThe option that says: **Deploy the Amazon CloudWatch Logs agent on each instance, customizing it to capture and log the client\u2019s IP address** is incorrect. While deploying the CloudWatch Logs agent can help in logging details to CloudWatch, the agent itself cannot capture the client\u2019s IP address unless it\u2019s already being logged by the HTTP server.\r\n\r\nThe option that says: **Implement the AWS X-Ray daemon on all instances, adjusting its settings to record the client\u2019s IP address in the logs** is incorrect. AWS X-Ray is primarily used to analyze and debug distributed applications by tracking and providing traces of requests. Although it is helpful for performance analysis and troubleshooting, X-Ray does not change HTTP server logs to include the client\u2019s IP address.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html\r\n\r\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
  },
  {
    "id": 171,
    "question": "A software development team uses AWS CodePipeline to facilitate continuous integration and delivery (CI/CD) for a Node.js application. The team requires a centralized way of distributing internal npm packages used by the application. It\u2019s crucial that the pipeline automatically starts to build whenever a new version of the package is released.\r\n\r\nWhich solution aligns with these requirements?",
    "options": [
      "Establish an AWS CodeArtifact repository to store the npm packages. Configure an Amazon EventBridge rule to detect changes in the repository and trigger CodePipeline pipeline builds.",
      "Set up an Amazon ECR private repository to host the npm package. Utilize an AWS Lambda function to initiate a CodePipeline pipeline build whenever a new package version is published to the repository.",
      "Create an Amazon ECR private repository and store the npm packages in it. Use Amazon SNS notifications to initiate CodePipeline pipeline builds.",
      "Store the npm packages in an Amazon S3 bucket. Use Amazon SNS to trigger a CodePipeline pipeline build when a new package version is uploaded to the bucket."
    ],
    "question_type": "single",
    "correct_answers": [
      "Establish an AWS CodeArtifact repository to store the npm packages. Configure an Amazon EventBridge rule to detect changes in the repository and trigger CodePipeline pipeline builds."
    ],
    "correct_answer": "Establish an AWS CodeArtifact repository to store the npm packages. Configure an Amazon EventBridge rule to detect changes in the repository and trigger CodePipeline pipeline builds.",
    "explanation": "**AWS CodeArtifact** is a secure and scalable artifact management service offering a centralized repository for storing, managing, and distributing software packages and their dependencies.\r\n\r\nCodeArtifact seamlessly integrates with Amazon EventBridge, a service that automates actions responding to specific events, including any activity within a CodeArtifact repository. This integration allows you to establish rules that dictate the actions to be taken when a particular event occurs.\r\n\r\nWhenever an action is taken in CodeArtifact, such as creating, updating, or deleting a package version, it triggers an event. The event mechanism in CodeArtifact makes it easier to efficiently monitor and automate tasks based on the activities within your CodeArtifact repositories. This helps to streamline workflows and improve overall efficiency.\r\n\r\n**AWS CodePipeline** is a comprehensive service that automates the steps in releasing software, known as continuous integration and continuous delivery (CI/CD). It manages the entire workflow required to take code from version control to deployment, streamlining the process of getting new features and updates to users.\r\n\r\nSetting up an Amazon EventBridge rule to monitor changes within the CodeArtifact repository allows seamless integration with AWS CodePipeline.This configuration is beneficial as it automates processes within the pipeline. More precisely, when a new version of a Node.js file is added to the repository, EventBridge detects the update and triggers the relevant CodePipeline build process, streamlining the workflow without requiring manual intervention.\r\n\r\nHence, the correct answer is: **Establish an AWS CodeArtifact repository to store the npm packages. Configure an Amazon EventBridge rule to detect changes in the repository and trigger CodePipeline pipeline builds.**\r\n\r\nThe option that says: **Store the npm packages in an Amazon S3 bucket. Use Amazon SNS to trigger a CodePipeline pipeline build when a new package version is uploaded to the bucket** is incorrect. While Amazon S3 can store files, it isn\u2019t tailored for managing software package dependencies and lacks the capabilities needed for version control and dependency management.\r\n\r\nThe option that says: **Set up an Amazon ECR private repository to host the npm package. Utilize an AWS Lambda function to initiate a CodePipeline pipeline build whenever a new package version is published to the repository** is incorrect. Amazon ECR is primarily used for Docker container images, not for the management of software packages.\r\n\r\nThe option that says: **Create an Amazon ECR private repository and store the npm packages in it. Use Amazon SNS notifications to initiate CodePipeline pipeline builds** is incorrect. Similar to the other incorrect option, using Amazon ECR as a repository for software packages is technically inappropriate. ECR\u2019s features are specifically designed for Docker containers. Moreover, Amazon SNS is typically used for alerting rather than for directly triggering pipeline builds.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/welcome.html\r\n\r\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/monitoring-events.html\r\n\r\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html\r\n\r\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html"
  },
  {
    "id": 172,
    "question": "A company processes data from IoT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs within the expected performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SLA). The company wants to measure the application\u2019s throughput by tracking the total number of messages the Lambda function processes within a specified timeframe.\r\n\r\nWhich action should a developer take to meet the requirements?",
    "options": [
      "Update the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function, and use these metrics for throughput calculation.",
      "Utilize the Lambda function\u2019s ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.",
      "Enable AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the execution times to infer throughput.",
      "Update the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to periodically trigger a secondary Lambda function for processing these logs."
    ],
    "question_type": "single",
    "correct_answers": [
      "Utilize the Lambda function\u2019s ConcurrentExecutions metric in Amazon CloudWatch to assess throughput."
    ],
    "correct_answer": "Utilize the Lambda function\u2019s ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.",
    "explanation": "The `ConcurrentExecutions` metric in Amazon CloudWatch explicitly measures the number of instances of a Lambda function that are running at the same time. This metric is crucial for understanding how many processes or tasks the Lambda function handles concurrently, which can serve as a proxy for throughput, especially when you want to ensure that your Lambda function is scaling appropriately to meet demands without directly measuring message processing times. While this metric does not directly measure the throughput in terms of messages processed per unit of time, it indirectly indicates the capacity and utilization of the Lambda function, offering insights into whether the function is meeting its required service level agreement (SLA) by handling loads effectively.\r\n\r\nHence, the correct answer is: **Utilize the Lambda function\u2019s `ConcurrentExecutions` metric in Amazon CloudWatch to assess throughput.**\r\n\r\nThe option that says: **Update the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to periodically trigger a secondary Lambda function for processing these logs** is incorrect because it only introduces additional complexity and potential latency. It requires manual calculation and aggregation of throughput metrics, which might fail to deliver real-time monitoring efficiency. Moreover, it doesn\u2019t exclude the time taken for initialization and post-processing without additional logic.\r\n\r\nThe option that says: **Update the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function, and use these metrics for throughput calculation** is incorrect because this option simply involves modifying the Lambda function code to publish these metrics, potentially adding overhead to each execution. In addition, this approach may only automatically exclude initialization and post-processing times from the throughput calculation if explicitly coded, making it less efficient for the specific requirement of excluding these times from the throughput measurement.\r\n\r\nThe option that says: **Enable AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the execution times to infer throughput** is incorrect because this service primarily tracks the execution time of the entire workflow, not the specific time taken to process individual messages. This makes it challenging to measure throughput precisely as defined in the scenario without additional customization. It also does not address the requirement to measure throughput, excluding initialization and post-processing. Therefore, this option does not meet the requirements.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-metrics.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
  },
  {
    "id": 173,
    "question": "A developer must ensure that any new log data published to an existing Amazon CloudWatch Logs group is encrypted. The log group has been actively receiving data for the past few weeks. The log data should be encrypted using an AWS Key Management Service (AWS KMS) key.\r\n\r\nWhich approach requires the least administrative effort to meet the requirements?",
    "options": [
      "Manually encrypt log data before submission using client-side scripts that utilize KMS.",
      "Associate an existing AWS KMS key with the log group by executing the aws logs associate-kms-key command with the key\u2019s Amazon Resource Name.",
      "Create a new log group with encryption settings using the AWS CLI aws logs create-log-group command, specifying the KMS key ARN.",
      "Implement custom encryption using AWS CloudHSM to encrypt logs before sending logs to CloudWatch."
    ],
    "question_type": "single",
    "correct_answers": [
      "Associate an existing AWS KMS key with the log group by executing the aws logs associate-kms-key command with the key\u2019s Amazon Resource Name."
    ],
    "correct_answer": "Associate an existing AWS KMS key with the log group by executing the aws logs associate-kms-key command with the key\u2019s Amazon Resource Name.",
    "explanation": "**AWS Key Management Service (AWS KMS)** is a secure and convenient service that simplifies the management and control of cryptographic keys used for data encryption. It integrates smoothly with numerous AWS services and your applications. With KMS, the keys you generate are robust and reliable, providing assurance that your critical data is encrypted and protected. This service streamlines the creation, management, and application of encryption keys, ensuring the security of your sensitive information.\r\n\r\n**Amazon CloudWatch Logs** is a comprehensive monitoring and observability service designed to aggregate logs from applications, systems, and AWS services. It provides a single platform for collecting and analyzing log data. It allows users to track application performance, respond to system-wide changes, optimize resource utilization, and acquire a comprehensive picture of operational health. This service facilitates the efficient management of log information, aiding in the diagnosis and resolution of issues and enhancing overall system reliability and performance.\r\n\r\nThe `aws logs associate-kms-key` command allows the developer to easily and quickly enable encryption for an existing log group, eliminating the need to recreate the log group or manually encrypt logs before submission. This approach takes advantage of CloudWatch Logs\u2019 built-in features and AWS KMS, guaranteeing that all new log data is automatically encrypted using the given KMS key. This solution involves the least amount of administrative effort because it eliminates the need for further custom development and the complexities of manually managing encryption or creating new log groups.\r\n\r\nHence, the correct answer is: **Associate an existing KMS key with the log group by executing the `aws logs associate-kms-key` command with the key\u2019s Amazon Resource Name.**\r\n\r\nThe option that says: **Implement custom encryption using AWS CloudHSM to encrypt logs before sending logs to CloudWatch** is incorrect because CloudHSM is designed for more complex security requirements, such as custom encryption workflows, and requires significant setup and maintenance. It involves managing your own encryption infrastructure and integrating it into applications, which increases complexity. In contrast, AWS KMS is a managed service that directly integrates with CloudWatch and provides encryption with minimal effort.\r\n\r\nThe option that says: **Create a new log group with encryption settings using the AWS CLI `aws logs create-log-group` command, specifying the KMS key ARN** is incorrect because it primarily suggests creating a new log group, which is unnecessary and adds unnecessary work. The question scenario implies modifying the encryption settings of an existing log group, not making a new one. Moreover, the create-log-group command does not support specifying KMS encryption during log group creation.\r\n\r\nThe option that says: **Manually encrypt log data before submission using client-side scripts that utilize KMS** is incorrect because this approach is just more labor-intensive than necessary. Manually encrypting log data before submission doesn\u2019t leverage CloudWatch\u2019s built-in capabilities to encrypt logs with KMS keys and could be more efficient for a developer aiming for minimal effort.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"
  },
  {
    "id": 174,
    "question": "An engineering team is developing a cloud-based solution for their user registration system for an online service, using AWS for their infrastructure. During the registration process, user form submissions are placed in a queue using Amazon Simple Queue Service (Amazon SQS) and processed by an AWS Lambda function to create new user accounts. However, the team has noticed that the Lambda function occasionally fails or times out, which results in incomplete user registrations. The team is looking for an efficient way to diagnose and fix these processing failures.\r\n\r\nWhich approach minimizes operational overhead while addressing these needs?\r\n\r\n",
    "options": [
      "Implement a dead-letter queue (DLQ) associated with the SQS queue. Configure the Lambda function to divert messages that fail to process correctly to this DLQ for subsequent analysis and action.",
      "Lengthen the message visibility timeout setting of the SQS queue. Utilize Amazon CloudWatch Logs to pinpoint and troubleshoot errors during user registration processing.",
      "Extend the maximum timeout for the Lambda function's execution to 20 minutes. Verify on AWS CloudTrail's event logs to identify and examine errors related to user registrations.",
      "Set up an Amazon DynamoDB table to capture failed user registration attempts. Modify the Lambda function to route messages that cannot be processed to this table for further examination."
    ],
    "question_type": "single",
    "correct_answers": [
      "Implement a dead-letter queue (DLQ) associated with the SQS queue. Configure the Lambda function to divert messages that fail to process correctly to this DLQ for subsequent analysis and action."
    ],
    "correct_answer": "Implement a dead-letter queue (DLQ) associated with the SQS queue. Configure the Lambda function to divert messages that fail to process correctly to this DLQ for subsequent analysis and action.",
    "explanation": "**Amazon Simple Queue Service (Amazon SQS)** is a highly scalable managed message queuing service. It enables microservices, distributed systems, and serverless applications to decouple and scale, facilitating reliable communication and coordination among application components. One of the most important features of Amazon SQS is its support for dead-letter queues (DLQs), which provide a resilient mechanism for managing messages that can\u2019t be processed successfully.\r\n\r\nA dead-letter queue (DLQ) is used to collect messages that fail to be processed by the target Lambda function or consumer after a certain number of attempts. The primary purpose of a DLQ is to isolate these messages from the main queue to prevent them from being retried indefinitely, which could potentially cause further issues. By routing these messages to a DLQ, developers can analyze the reasons for the processing failures, correct any issues, and optionally resubmit the messages for processing. This approach enhances the reliability and resilience of the application by ensuring that message-processing failures are addressed promptly and efficiently.\r\n\r\nImplementing a DLQ with Amazon SQS is straightforward and integrates seamlessly with AWS Lambda and other AWS services. It offers a simple way to add error handling and failure analysis capabilities to your applications without introducing significant complexity or operational overhead. Moreover, using DLQs aligns with best practices for building fault-tolerant, resilient distributed systems on the AWS platform, allowing teams to maintain high service availability and performance levels.\r\n\r\nHence, the correct answer is: **Implement a dead-letter queue (DLQ) associated with the SQS queue. Configure the Lambda function to divert messages that fail to process correctly to this DLQ for subsequent analysis and action.**\r\n\r\nThe option that says: **Extend the maximum timeout for the Lambda function\u2019s execution to 20 minutes. Verify on AWS CloudTrail\u2019s event logs to identify and examine errors related to user registrations** is incorrect. Extending the Lambda function\u2019s timeout might not address the root cause of the issue and could lead to higher costs. While AWS CloudTrail is useful for auditing API calls and events within AWS services, it might not provide the detailed, message-level insight needed for debugging specific registration failures.\r\n\r\nThe option that says: **Lengthen the message visibility timeout setting of the SQS queue. Utilize Amazon CloudWatch Logs to pinpoint and troubleshoot errors during user registration processing** is incorrect. While increasing the message visibility timeout and using Amazon CloudWatch Logs can help troubleshoot, this approach doesn\u2019t directly handle messages that fail repeatedly. This option is primarily for monitoring and logging than efficiently managing and reprocessing failed messages.\r\n\r\nThe option that says: **Set up an Amazon DynamoDB table to capture failed user registration attempts. Modify the Lambda function to route messages that cannot be processed to this table for further examination** is incorrect. Creating a DynamoDB table for failed messages introduces additional operational overhead and cost. It requires custom logic for routing failed messages and does not use built-in features like DLQs to manage message failures. While it could provide a way to store and analyze failures, it\u2019s a more inefficient solution compared to using a DLQ.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\r\n\r\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue.html"
  },
  {
    "id": 175,
    "question": "A web developer must deploy new features to a web application that can be turned on or off based on certain conditions. These new functionalities should not be accessible until fully developed and tested. The company wants to seamlessly control the visibility and access of these new features without revealing them until it\u2019s officially released.\r\n\r\nWhich of the following solutions would fulfill these criteria?",
    "options": [
      "Employ AWS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization capabilities to modify feature visibility.",
      "Develop a feature flag feature using AWS Lambda functions. Store the feature flag values in AWS Systems Manager Parameter Store. Use the Lambda function to fetch and check the feature flag status to manage the availability of new functionalities.",
      "Employ AWS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization capabilities to modify feature visibility.",
      "Utilize AWS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature flags according to development progress."
    ],
    "question_type": "single",
    "correct_answers": [
      "Utilize AWS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature flags according to development progress."
    ],
    "correct_answer": "Utilize AWS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature flags according to development progress.",
    "explanation": "**AWS AppConfig** is a specialized service that enables developers to create, manage, and deploy application configurations quickly and efficiently. It also provides the functionality to manage feature flags, a powerful technique that allows developers to test and control new features in live environments without affecting all users. With AWS AppConfig, developers can gradually introduce new features, control who can access them based on specific criteria such as user role or location, and easily update or revert features without deploying new code. This capability helps developers seamlessly control the visibility and access of new features while keeping them hidden until they\u2019re ready for release.\r\n\r\nAWS AppConfig is a tool that enables you to define different environments, such as production, development, and testing. You can also create configuration profiles and deployment strategies using this tool. It provides a feature flag management system that helps you test changes in isolation and gradually roll them out to your user base. This ensures a robust and error-free deployment of new features.\r\n\r\nHence, the correct answer is: **Utilize AWS AppConfig to set the feature flag by creating a feature flag** **configuration profile. Enable and disable the feature flags according to development progress.**\r\n\r\nThe option that says: **Develop a feature flag feature using AWS Lambda functions. Store the feature flag values in AWS Systems Manager Parameter Store. Use the Lambda function to fetch and check the feature flag status to manage the availability of new functionalities** is incorrect. This approach introduces unnecessary complexity and operational overhead. Managing feature flags via Lambda and Parameter Store requires additional development and maintenance of Lambda functions. This setup can also introduce latency issues, as each feature flag check involves a Lambda function execution, which may not be efficient for high-frequency checks required in a dynamic feature management scenario.\r\n\r\nThe option that says: **Utilize Amazon DynamoDB to store information about upcoming features. Leverage Amazon DynamoDB Streams to switch features\u2019 status from hidden to visible upon updates** is incorrect. Although DynamoDB can be used to store data and DynamoDB Streams can react to changes, using them for feature flag management would require additional custom implementation. It is not as straightforward or well-suited for feature flag management as AWS AppConfig.\r\n\r\nThe option that says: **Employ AWS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization capabilities to modify feature visibility** is incorrect. AWS Amplify DataStore offers a programming model for leveraging shared and distributed data without writing additional code for offline and online synchronization. However, its primary use is to allow developers to work seamlessly with data in a web or mobile application across platforms, focusing on data synchronization and local data storage. Thus, this service lacks direct support for feature flagging explicitly designed for conditional feature visibility and management.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/appconfig/latest/userguide/what-is-appconfig.html\r\n\r\nhttps://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-creating-configuration-and-profile-feature-flags.html"
  },
  {
    "id": 176,
    "question": "A startup wants an application that triggers processes in response to customer orders and inventory updates using AWS Lambda and Amazon EventBridge. The application requires a Lambda function to send the specific events to an Amazon EventBridge event bus. The developer uses the AWS SDK to call the PutEvents action on EventBridge. Upon deployment, the developer notices AccessDeniedException errors in the event logs, which is the reason why the function is not functioning as intended.\r\n\r\nWhich solution will help the developer in resolving the issue?",
    "options": [
      "Adjust the AWS Lambda function\u2019s execution role to grant it permissions for the PutEvents action on EventBridge.",
      "Establish a Virtual Private Cloud (VPC) peering connection to facilitate communication between AWS Lambda and EventBridge.",
      "Update the developer\u2019s IAM role by including permission to explicitly allow the PutEvents operation on EventBridge.",
      "Implement a resource-based policy on the Lambda function that grants necessary permissions to perform the PutEvents action on EventBridge."
    ],
    "question_type": "single",
    "correct_answers": [
      "Adjust the AWS Lambda function\u2019s execution role to grant it permissions for the PutEvents action on EventBridge."
    ],
    "correct_answer": "Adjust the AWS Lambda function\u2019s execution role to grant it permissions for the PutEvents action on EventBridge.",
    "explanation": "**AWS Lambda** is a service that allows you to run your code without managing any servers. It only runs your code when it is needed and can scale from a few requests per day to thousands per second automatically. Moreover, AWS Lambda functions can be triggered by various AWS services, such as Amazon EventBridge. Amazon EventBridge is a serverless event bus that can connect application data from your apps, SaaS, and AWS services. Using EventBridge, you can build event-driven architectures that are highly scalable and decoupled from each other. This approach can make your applications more resilient and flexible.\r\n\r\nLambda functions often need to interact with other AWS services, such as sending events to an EventBridge bus. However, to access and perform actions on these services, the function requires the necessary permissions. This is where the concept of an execution role comes into play. An execution role is an IAM role that grants the Lambda function permissions to access AWS services and resources. By adjusting the execution role of a Lambda function to include permissions for the `PutEvents` action on EventBridge, the function can safely interact with EventBridge. This method follows the AWS best practice of granting the least privilege, ensuring that the function only has the permissions needed to perform its designated tasks. As a result, the security posture of the application is enhanced.\r\n\r\nIn the provided scenario, where a developer\u2019s AWS Lambda function encounter `AccessDeniedException` errors when attempting to publish events to an EventBridge event bus, adjusting the Lambda function\u2019s execution role is the correct approach to resolving the issue. By granting the execution role permission for the `PutEvents` action on EventBridge, the Lambda function is authorized to execute the `PutEvents` call successfully. This solution emphasizes the importance of IAM roles in managing permissions within AWS environments, ensuring secure and efficient access management for serverless applications.\r\n\r\nHence, the correct answer is: **Adjust the AWS Lambda function\u2019s execution role to grant it permissions for the `PutEvents` action on EventBridge.**\r\n\r\nThe option that says: **Establish a Virtual Private Cloud (VPC) peering connection to facilitate communication between AWS Lambda and EventBridge** is incorrect. VPC peering is a networking feature that allows traffic to be routed between two VPCs using private IP addresses. However, this option does not address the underlying issue, which is related to IAM permissions and not network connectivity. Amazon EventBridge is a serverless event bus service that doesn\u2019t require VPC peering with AWS Lambda for connectivity. The `AccessDeniedException` error indicates a permissions issue that needs IAM configuration adjustments rather than network infrastructure changes.\r\n\r\nThe option that says: **Update the developer\u2019s IAM role by including permission to explicitly allow the `PutEvents` operation on EventBridge** is incorrect because the permissions required to execute AWS service operations from within an AWS Lambda function are determined by the execution role attached to the Lambda function itself, not the IAM role of the developer. The execution role provides the Lambda function with the necessary AWS credentials to interact with other AWS services under the permissions defined in that role. Modifying the programmer\u2019s IAM role would not grant the Lambda function the permissions it needs to interact with EventBridge.\r\n\r\nThe option that says: **Implement a resource-based policy on the Lambda function that grants necessary permissions to perform the `PutEvents` action on EventBridge** is incorrect. Lambda functions do not use resource-based policies to grant permissions to access other AWS services. Instead, permissions are primarily managed through the execution role.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\r\n\r\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html"
  },
  {
    "id": 177,
    "question": "An enterprise deployed a new serverless application on AWS designed for real-time data processing. This application uses AWS Lambda functions to fetch data from Amazon S3, process it, and then store the results in a DynamoDB database. After deployment, users began experiencing intermittent slow responses. The development team suspects the issue might be related to the interaction between the Lambda function and Amazon S3, as this is where the data fetching occurs.\r\n\r\nWhich approach should a developer take to identify the root cause of the latency issue while following operational excellence principles?",
    "options": [
      "Analyze the detailed metrics and logs available in Amazon CloudWatch for the Lambda function. Focus on execution duration and error rates. Apply anomaly detection models to identify unusual patterns or spikes in latency.",
      "Explore the AWS CloudWatch Synthetics to generate canary that mimics user interactions with the portal. Ensure AWS X-Ray tracing is enabled for this canary to gain insights into the service interactions and performance metrics. After gathering enough data, review the outcomes on the CloudWatch Synthetics dashboard to locate the latency issues.",
      "Enhance the Lambda function with AWS X-Ray SDK integration, including the setup for HTTP/HTTPS requests and SDK client handlers. Redeploy the function with the modifications. Activate X-Ray tracing and, upon collecting sufficient operational data, utilize the X-Ray service maps to analyze average response timings and pinpoint probable sources of latency.",
      "Implement additional log statements within the Lambda function to record execution times with precise timestamps surrounding each call to external services. Then, redeploy the enhanced Lambda function. After sufficient operational data has been gathered, analyze the Amazon CloudWatch logs for the function to identify potential culprits for the delayed responses."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enhance the Lambda function with AWS X-Ray SDK integration, including the setup for HTTP/HTTPS requests and SDK client handlers. Redeploy the function with the modifications. Activate X-Ray tracing and, upon collecting sufficient operational data, utilize the X-Ray service maps to analyze average response timings and pinpoint probable sources of latency."
    ],
    "correct_answer": "Enhance the Lambda function with AWS X-Ray SDK integration, including the setup for HTTP/HTTPS requests and SDK client handlers. Redeploy the function with the modifications. Activate X-Ray tracing and, upon collecting sufficient operational data, utilize the X-Ray service maps to analyze average response timings and pinpoint probable sources of latency.",
    "explanation": "**AWS X-Ray** is a powerful service that provides developers with essential insights into the performance and behavior of their applications. With X-Ray, developers can trace the requests as they move through an application and collect data about the interactions, which it uses to generate detailed service maps and performance metrics. This tracing capability extends across multiple AWS services such as AWS Lambda, Amazon EC2, Amazon ECS, and API Gateway. The service can also be integrated into applications on-premises or other environments. This visibility helps developers analyze and understand how different application components interact, enabling them to optimize and improve the application\u2019s performance.\r\n\r\nThe integration of AWS X-Ray with AWS Lambda allows developers to see detailed information about their serverless components\u2019 function invocation, response times, and operational impacts. This visibility is crucial for optimizing serverless applications and meeting performance expectations. X-Ray provides visual analysis through service maps detailing the interactions between services in an application, helping developers quickly pinpoint where failures occur, and performance bottlenecks are forming.\r\n\r\nHence, the correct answer is: **Enhance the Lambda function with AWS X-Ray SDK integration, including the setup for HTTP/HTTPS requests and SDK client handlers. Redeploy the function with the modifications. Activate X-Ray tracing and, upon collecting sufficient operational data, utilize the X-Ray service maps to analyze average response timings and pinpoint probable sources of latency.**\r\n\r\nThe option that says: **Implement additional log statements within the Lambda function to record execution times with precise timestamps surrounding each call to external services. Then, redeploy the enhanced Lambda function. After sufficient operational data has been gathered, analyze the Amazon CloudWatch logs for the function to identify potential culprits for the delayed responses** is incorrect. Although adding more detailed logging can provide insights into the Lambda function\u2019s performance, this approach does not offer a comprehensive view of the interactions between services and external APIs. Thus making it less effective for identifying the root cause of latency than AWS X-Ray.\r\n\r\nThe option that says: **Explore the AWS CloudWatch Synthetics to generate canary that mimics user interactions with the portal. Ensure AWS X-Ray tracing is enabled for this canary to gain insights into the service interactions and performance metrics. After gathering enough data, review the outcomes on the CloudWatch Synthetics dashboard to locate the latency issues** is incorrect. AWS CloudWatch Synthetics primarily helps in monitoring your endpoints and URLs from the outside in to confirm they are reachable and performing as expected. However, for the purpose of identifying latency issues within the application\u2019s architecture, this approach may not provide the detail required for diagnosing the specific issue mentioned.\r\n\r\nThe option that says: **Analyze the detailed metrics and logs available in Amazon CloudWatch for the Lambda function. Focus on execution duration and error rates. Apply anomaly detection models to identify unusual patterns or spikes in latency** is incorrect. While CloudWatch metrics and logs offer important insights into the function\u2019s performance, without the contextual trace data provided by AWS X-Ray, identifying the precise location and cause of latency within the distributed components of the application can be challenging.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-services-lambda.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/trace-requests.html"
  },
  {
    "id": 178,
    "question": "An engineer is developing a cloud-native application on AWS designed to handle a large volume of data. Within this application, a state machine in AWS Step Functions calls upon multiple AWS Lambda functions for processing. Occasionally, one of these Lambda functions does not complete its task within the allocated time, particularly during spikes in data volume, leading to timeout errors. The engineer must set up the system so that any invocation of the Lambda function that fails due to these timeouts is automatically attempted again.\r\n\r\nWhich approach would ensure the system meets this requirement?\r\n\r\n",
    "options": [
      "Set the TimeoutSeconds value within the AWS Step Functions state machine setup. Assign a limit for retry attempts in response to operation timeouts.",
      "Modify the AWS Step Functions state machine to forward the function call to an Amazon SNS topic and link the topic to the Lambda function. Set a retry limit for the Lambda to handle timeout error scenarios.",
      "Integrate a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries and the target timeout error type as the retry trigger.",
      "Establish a designated Fail state within the AWS Step Functions state machine architecture. Determine a retry limit for handling execution failures."
    ],
    "question_type": "single",
    "correct_answers": [
      "Integrate a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries and the target timeout error type as the retry trigger."
    ],
    "correct_answer": "Integrate a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries and the target timeout error type as the retry trigger.",
    "explanation": "**AWS Step Functions** is a service that helps developers create and run workflows that integrate different AWS services, like AWS Lambda, into applications without worrying about servers. At its core, Step Functions uses Amazon States Language, a JSON-based language that defines state machines. These state machines represent workflows as a series of steps, where each step corresponds to a state that can perform work (Task states), make decisions (Choice states), wait for a certain time or event (Wait states), succeed or fail (Succeed/Fail states), and more. This provides a robust framework for managing complex application logic, handling error scenarios, and ensuring application components interact seamlessly.\r\n\r\nAWS Step Functions has a useful feature with built-in error handling and retry capabilities. When creating a state machine, developers can set error-handling options for each state. This includes defining retries on errors with customizable parameters such as the number of retry attempts, the delay between attempts, and the backoff rate. Developers can target specific errors, such as timeouts or exceptions thrown by AWS Lambda functions, by defining a Retry field in the state\u2019s configuration. This ensures that transient issues or expected problems do not immediately cause the workflow to fail. Instead, the system can attempt to resolve the issue through retries based on the parameters provided. This mechanism is handy for handling spikes in data volume or temporary resource constraints, ensuring that workflows are designed to be robust and resilient under varying conditions.\r\n\r\nHence, the correct answer is: **Integrate a Retry field into the AWS Step Functions state machine\u2019s configuration. Specify the maximum attempts for retries and the target timeout error type as the retry trigger.**\r\n\r\nThe option that says: **Set the `TimeoutSeconds` value within the AWS Step Functions state machine setup. Assign a limit for retry attempts in response to operation timeouts** is incorrect. Although Step Functions provide a way to set timeouts for states, the `TimeoutSeconds` only specify the maximum time a state can run before it fails. It does not have a built-in mechanism to retry failed state executions based on timeouts. To enable retries, you must define them using the Retry field and target specific errors. If you only set a timeout without specifying retries for certain errors, it won\u2019t automatically retry failed Lambda function invocations.\r\n\r\nThe option that says: **Establish a designated Fail state within the AWS Step Functions state machine architecture. Determine a retry limit for handling execution failures** is incorrect. AWS Step Functions provide a feature called Fail state, which is helpful for handling errors and terminating the execution if certain conditions are met. However, this feature does not include a built-in mechanism for automatically retrying a task. The Fail state is a final state that indicates the execution has failed and cannot be retried. Therefore, while the Fail state is useful for error handling, it cannot automatically retry a failed Lambda function invocation due to timeouts.\r\n\r\nThe option that says: **Modify the AWS Step Functions state machine to forward the function call to an Amazon SNS topic and link the topic to the Lambda function. Set a retry limit for the Lambda to handle timeout error scenarios** is incorrect. This option adds unnecessary complexity by involving Amazon SNS that are not required to retry a Lambda function on failure within AWS Step Functions. Although Amazon SNS can be used for decoupling components and retrying message deliveries, it does not inherently benefit the use case described. AWS Step Functions natively supports retry policies for tasks, including Lambda functions. By configuring retries within the state machine, you can manage retries without the overhead of integrating and managing additional services. Moreover, this option does not directly address the requirement to handle retries for timeout errors within the context of a Step Functions state machine.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\r\n\r\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
  },
  {
    "id": 179,
    "question": "A technology firm manages an internal portal that contains proprietary information. The firm intends to make this portal available to the general public. However, access must be restricted solely to employees authenticated through the firm\u2019s OpenID Connect (OIDC) identity provider (IdP). This authentication mechanism must be implemented without modifying the existing website code.\r\n\r\nWhich action will accomplish this requirement?",
    "options": [
      "Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuration.",
      "Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to invoke an AWS Lambda function for OIDC authentication.",
      "Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 80. Add a default authenticating operation that returns the OIDC IdP configuration.",
      "Set up an internet-facing Network Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuratio"
    ],
    "question_type": "single",
    "correct_answers": [
      "Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuration."
    ],
    "correct_answer": "Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuration.",
    "explanation": "**Application Load Balancer (ALB)** is created to handle both HTTP and HTTPS traffic at the application layer (Layer 7). It provides advanced routing capabilities and supports features such as user authentication and SSL termination, which are essential for securing web applications exposed to the internet. By configuring a public ALB, the company ensures that all incoming traffic to the internal website is managed through a central point that can enforce security policies, including OIDC authentication. The ALB can be easily integrated with the company\u2019s OIDC identity provider to authenticate users, making it an ideal solution for this requirement.\r\n\r\nSetting up a listener on HTTPS port 443 ensures that all data transferred between users and the website is encrypted, protecting sensitive information from being intercepted. The listener can be configured with rules to authenticate users using the company\u2019s OIDC IdP, which verifies the identity of the users before granting access. This setup leverages the ALB\u2019s native support for OIDC authentication, streamlining the authentication process without requiring changes to the website itself. This approach aligns with best practices for securing web applications, as outlined in the latest AWS documentation, ensuring robust protection for the company\u2019s internal resources.\r\n\r\nHence, the correct answer is: **Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuration.**\r\n\r\nThe option that says: **Set up an internet-facing Network Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to authenticate users using the OIDC IdP configuration** is incorrect because Network Load Balancers only operate at the transport layer (Layer 4) and does not support OIDC authentication, which is required for this scenario.\r\n\r\nThe option that says: **Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 80. Add a default authenticating operation that returns the OIDC IdP configuration** is incorrect This is incorrect because port 80 is typically used for HTTP, not HTTPS. OIDC authentication only works for HTTPS listeners.\r\n\r\nThe option that says: **Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443. Configure the rule\u2019s default action to invoke an AWS Lambda function for OIDC authentication** is incorrect. While this approach is technically feasible, it adds unnecessary complexity and requires custom coding, which might not align with the requirement to avoid modifying the existing code infrastructure.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\r\n\r\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html"
  },
  {
    "id": 180,
    "question": "A software engineer utilizes AWS IAM Identity Center (AWS Single Sign-On) to communicate with the AWS CLI and SDKs on their on-premises server. The engineer initially enabled SSO access correctly and was able to make API calls to multiple AWS services without difficulty. However, after a few weeks, the engineer began to receive Access Denied messages when trying to make the same API calls. The engineer has confirmed that there have been no modifications to any configuration files, scripts, or rights on their local machine. Furthermore, the developer ensured that their network connection and VPN access were operational.\r\n\r\nWhat is the MOST probable reason for the engineer\u2019s access issue",
    "options": [
      "The credentials issued by the IAM Identity Center federated role are no longer valid.",
      "The AWS CLI executable file of the engineer has had its access permissions modified.",
      "The engineer's VPN connection is incorrectly configured.",
      "The permission set assigned by the IAM Identity Center lacks the required permissions to perform the API call."
    ],
    "question_type": "single",
    "correct_answers": [
      "The credentials issued by the IAM Identity Center federated role are no longer valid."
    ],
    "correct_answer": "The credentials issued by the IAM Identity Center federated role are no longer valid.",
    "explanation": "**AWS IAM Identity Center (AWS Single Sign-On)** is a service that centralizes access management across AWS accounts and applications. It issues temporary security credentials for users to access AWS resources, ensuring enhanced security by limiting the duration these credentials remain valid. Temporary credentials typically expire after a predefined period, requiring users to re-authenticate to obtain new ones.\r\n\r\nIn this scenario, the most probable cause of the Access Denied errors is that the credentials issued by the IAM Identity Center federated role are no longer valid. When credentials expire, any subsequent API calls will fail until the user re-authenticates and receives fresh credentials. This expiration mechanism is a security feature designed to minimize risk by ensuring that access is time-bound. Furthermore, temporary credentials must be periodically refreshed to maintain continuous access to AWS services. Given that no changes were made to configuration files or scripts and the access was previously working, it is most likely that the credentials have simply expired. The engineer can resolve the issue by re-authenticating with AWS IAM Identity Center by obtaining new valid credentials.\r\n\r\nHence, the correct answer is: **The credentials issued by the IAM Identity Center federated role are no longer valid.**\r\n\r\nThe option that says: **The AWS CLI executable file of the engineer has had its access permissions modified** is incorrect because changes to the AWS CLI executable file\u2019s access rights would not specifically result in Access Denied errors for API calls. Such changes would more likely prevent the CLI from running altogether not cause access issues with specific API calls.\r\n\r\nThe option that says: **The permission set assigned by the IAM Identity Center lacks the required permissions to perform the API call** is incorrect. While it\u2019s possible that permission sets could cause access issues, the scenario described indicates that the setup was previously working and no changes were made. This makes it less likely compared to the expiration of temporary credentials, which is primarily a time-based issue rather than a configuration one.\r\n\r\nThe option that says: **The engineer\u2019s VPN connection is incorrectly configured** is incorrect. According to the scenario, the engineer has confirmed that the network connection and VPN access are operational. If the VPN connection were incorrectly configured, it would likely result in broader connectivity issues rather than specific \u201cAccess Denied\u201d messages from AWS API calls. Thus, this option is not likely to cause the scenario described.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\r\n\r\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html"
  },
  {
    "id": 181,
    "question": "A financial services company is developing a real-time fraud detection system for its payment processing application. Payment transaction events are sent via an API, and the system must analyze each transaction in real-time to identify and flag potentially fraudulent activities. The solution should support concurrent processing by multiple fraud detection models and monitoring systems while ensuring scalability, low-latency performance, and cost optimization.\r\n\r\nWhich AWS service is best suited to meet these requirements?",
    "options": [
      "Amazon Kinesis Data Streams",
      "Amazon Data Firehose",
      "Amazon Kinesis Agent",
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK)"
    ],
    "question_type": "single",
    "correct_answers": [
      "Amazon Kinesis Data Streams"
    ],
    "correct_answer": "Amazon Kinesis Data Streams",
    "explanation": "**Amazon Kinesis Data Streams** is a fully managed, scalable service designed to handle large amounts of real-time data streaming. It enables the collection, processing, and analysis of real-time data at a massive scale, allowing applications to react to new information almost immediately. Data streams can be ingested from various sources, such as IoT devices, application logs, or payment transactions, making it ideal for use cases like fraud detection, real-time analytics, and monitoring. Kinesis Data Streams allows users to process data with multiple consumers simultaneously, supporting the parallel analysis by various fraud detection models and enabling real-time decision-making. Its low-latency capabilities ensure that the data is quickly available for processing, making it a great fit for applications that require instant responses, such as fraud prevention in payment systems.\r\n\r\nWith its ability to scale dynamically based on the volume of incoming data, Kinesis Data Streams ensures cost optimization by allowing users to adjust their resources according to demand. The service automatically handles partitioning and replication of data, ensuring high availability and fault tolerance. Additionally, it integrates seamlessly with other AWS services like AWS Lambda for real-time processing, Amazon Redshift for data storage and analytics, and Amazon S3 for long-term data storage.\r\n\r\nHence, the correct answer is: **Amazon Kinesis Data Streams.**\r\n\r\nThe option that says: **Amazon Data Firehose** is incorrect because it is primarily used for loading streaming data into other AWS services (e.g., Amazon S3, Amazon Redshift, Amazon Elasticsearch) for storage or analysis. While it can stream data, it does not provide the real-time processing and concurrent model support that Kinesis Data Streams offers. Data Firehose is optimized for delivering data to storage destinations. It lacks fine-grained control over stream processing, which is necessary for fraud detection, where real-time analysis and immediate action are crucial.\r\n\r\nThe option that says: **Amazon Managed Streaming for Apache Kafka (Amazon MSK)** is incorrect. While Apache Kafka is a powerful distributed streaming platform, it typically requires more operational overhead compared to Kinesis Data Streams. It\u2019s more complex to set up, manage, and scale. For a real-time fraud detection system, simplicity and low-latency processing are essential, and Kinesis Data Streams provides a more streamlined solution with easier integration into the AWS environment.\r\n\r\nThe option that says: **Amazon Kinesis Agent** is incorrect because it is a lightweight software that helps send log and event data from on-premises servers to Amazon Kinesis Data Streams or Data Firehose. However, it\u2019s not a service for data streaming itself. It is only used for pushing data to a stream or Firehose from local sources but does not provide the real-time stream processing capabilities that Kinesis Data Streams does.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html\r\n\r\nhttps://aws.amazon.com/kinesis/data-streams/faqs/?nc=sn&loc=6"
  },
  {
    "id": 182,
    "question": "A reporting application is hosted in AWS Elastic Beanstalk and uses Amazon DynamoDB as its database. If a user requests data, the application scans the entire table and returns the requested data. The table is expected to grow due to the surge of new users and the increase in requests for reports in the coming weeks.\r\n\r\nWhich of the following should be done as a preparation to improve the application\u2019s performance with minimal cost? (Select TWO.)",
    "options": [
      "Set the ScanIndexForward parameter to control the order of query results.",
      "Use Query operations instead",
      "Increase the Write Compute Unit (WCU) of the table",
      "Use DynamoDB Accelerator (DAX)",
      "Reduce page size"
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Use Query operations instead",
      "Reduce page size"
    ],
    "correct_answer": "Use Query operations instead",
    "explanation": "In general, `Scan` operations are less efficient than other operations in DynamoDB. A `Scan` operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.\r\n\r\nIf possible, you should avoid using a `Scan` operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the `Scan` operation slows. The `Scan` operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use `Query` instead of `Scan`. For tables, you can also consider using the `GetItem` and `BatchGetItem` APIs.\r\n\r\nAlternatively, you can refactor your application to use `Scan` operations in a way that minimizes the impact on your request rate. Instead of using a large `Scan` operation, you can use the following techniques to minimize the impact of a scan on a table\u2019s provisioned throughput.\r\n\r\n**Reduce page size** \u2013 Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The `Scan` operation provides a *Limit* parameter that you can use to set the page size for your request. Each `Query` or `Scan` request that has a smaller page size uses fewer read operations and creates a \u201cpause\u201d between each request. For example, suppose that each item is 4 KB and you set the page size to 40 items. A `Query` request would then consume only 20 eventually consistent read operations or 40 strongly consistent read operations. A larger number of smaller `Query` or `Scan` operations would allow your other critical requests to succeed without throttling.\r\n\r\n**Isolate scan operations** \u2013 DynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking \u201cmission-critical\u201d traffic. Some applications handle this load by rotating traffic hourly between two tables\u2014one for critical traffic, and one for bookkeeping. Other applications can do this by performing every write on two tables: a \u201cmission-critical\u201d table, and a \u201cshadow\u201d table.\r\n\r\nHence, the correct answers are:\r\n\r\n**\u2013** **Use Query operations instead**\r\n\r\n**\u2013** **Reduce page size**\r\n\r\nThe option that says: **Using DynamoDB Accelerator (DAX)** is incorrect. Although this will improve the scalability and read performance of the application, it simply adds a significant cost in maintaining your application. Using Query operations and reducing the page size of your query are the more cost-effective solutions in this scenario.\r\n\r\nThe option that says: **Set the `ScanIndexForward` parameter to control the order of query results** is incorrect. While useful for ordering results, it does not improve the efficiency or performance of the underlying operation. Changing the order of scan results does not address the fundamental issue of scanning the entire table.\r\n\r\nThe option that says: **Increasing the Write Compute Unit (WCU) of the table** is incorrect because the reporting application is primarily used for reading data and not for writing. In addition, increasing the WCU will increase the cost.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#bp-query-scan-spikes\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html"
  },
  {
    "id": 183,
    "question": "A developer is instructed to configure a worker daemon to queue messages based on a specific schedule using a worker environment hosted in Elastic Beanstalk. Periodic tasks should be defined to automatically add jobs to your worker environment\u2019s queue at regular intervals.\r\n\r\nWhich configuration file should the developer add to the source bundle to meet the above requirement?\r\n\r\n\r\n",
    "options": [
      "cron.yaml",
      "Dockerrun.aws.json",
      "env.yaml",
      "appspec.yml"
    ],
    "question_type": "single",
    "correct_answers": [
      "cron.yaml"
    ],
    "correct_answer": "cron.yaml",
    "explanation": "AWS resources created for a worker environment tier include an Auto Scaling group, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Elastic Beanstalk also creates and provisions an Amazon SQS queue if you don\u2019t already have one. When you launch a worker environment tier, Elastic Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the Auto Scaling group.\r\n\r\nThe daemon is responsible for pulling requests from an Amazon SQS queue and then sending the data to the web application running in the worker environment tier that will process those messages. If you have multiple instances in your worker environment tier, each instance has its own daemon, but they all read from the same Amazon SQS queue.\r\n\r\nYou can define periodic tasks in a file named **cron.yaml** in your source bundle to add jobs to your worker environment\u2019s queue automatically at a regular intervals. For example, you can configure and upload a cron.yaml file, which creates two periodic tasks: one that runs every 12 hours and a second that runs at 11 pm UTC every day.\r\n\r\nHence, using the **cron.yaml** is the correct configuration file to be used in this scenario.\r\n\r\n**`Dockerrun.aws.json`** is incorrect because this configuration file is primarily used in multi-container Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform.\r\n\r\n**`env.yaml`** is incorrect because this is primarily used to configure the environment name, solution stack, and environment links to use when creating your environment in Elastic Beanstalk.\r\n\r\n**`appspec.yml`** is incorrect because this is used to manage each application deployment as a series of lifecycle event hooks in CodeDeploy and not in Elastic Beanstalk.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html#worker-periodictasks\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html"
  },
  {
    "id": 184,
    "question": "You are deploying a serverless application composed of Lambda, API Gateway, CloudFront, and DynamoDB using CloudFormation. The AWS SAM syntax should be used to declare resources in your template which requires you to specify the version of the AWS Serverless Application Model (AWS SAM).\r\n\r\nWhich of the following sections is required, aside from the Resources section, that should be in your CloudFormation template?",
    "options": [
      "Format Version",
      "Transform",
      "Mappings",
      "Parameters"
    ],
    "question_type": "single",
    "correct_answers": [
      "Transform"
    ],
    "correct_answer": "Transform",
    "explanation": "For serverless applications (also referred to as Lambda-based applications), the optional **Transform** section specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it is processed.\r\n\r\nThis section specifies one or more macros that AWS CloudFormation uses to process your template. The Transform section builds on the simple, declarative language of AWS CloudFormation with a powerful macro system.\r\n\r\nYou can declare one or more macros within a template. AWS CloudFormation executes macros in the order that they are specified. When you create a change set, AWS CloudFormation generates a change set that includes the processed template content. You can then review the changes and execute the change set.\r\n\r\nAWS CloudFormation also supports the AWS::Serverless and AWS::Include transforms, which are macros hosted by AWS CloudFormation. AWS CloudFormation treats these transforms the same as any macros you create in terms of execution order and scope.\r\n\r\nTherefore, the **Transform** section should be the correct one to be added to your template.\r\n\r\n**Mappings** section is incorrect because this is just a literal mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table.\r\n\r\n**Parameters** section is incorrect because this only contains the values that will be passed to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template, but this is not used to specify the AWS SAM version.\r\n\r\n**Format Version** section is incorrect because this just refers to the AWS CloudFormation template version that the template conforms to, and not the version of the AWS Serverless Application Model (AWS SAM)\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\r\n\r\nhttps://aws.amazon.com/blogs/aws/cloudformation-macros/"
  },
  {
    "id": 185,
    "question": "A serverless application composed of Lambda, API Gateway, and DynamoDB has been running without issues for quite some time. As part of the IT compliance of the company, a developer was instructed to ensure that all of the new changes made to the items in DynamoDB are recorded and stored in another DynamoDB table in another region.\r\n\r\nIn this scenario, which of the following is the MOST ideal way to comply with the requirements?",
    "options": [
      "Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table.",
      "Enable DynamoDB Point-in-Time Recovery to automatically sync the two tables.",
      "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that tracks table-level events in DynamoDB. Set a Lambda function as a rule target to process and save new changes to the other table.",
      "Set up DynamoDB Accelerator"
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table."
    ],
    "correct_answer": "Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table.",
    "explanation": "**DynamoDB Streams** enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time.\r\n\r\nA *DynamoDB stream* is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\r\n\r\nWhenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A *stream record* contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \u201cbefore\u201d and \u201cafter\u201d images of modified items.\r\n\r\nHence, the correct answer is: **Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table.**\r\n\r\nThe option that says: **Set up DynamoDB Accelerator** is incorrect because the DynamoDB Accelerator (DAX) feature simply takes the performance of the DynamoDB table to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. You have to use DynamoDB Streams instead.\r\n\r\nThe option that says: **Enable DynamoDB Point-in-Time Recovery to automatically sync the two tables** is incorrect because this feature just helps protect your DynamoDB tables from accidental write or delete operations.\r\n\r\nThe option that says: **Create an Amazon EventBridge (Amazon CloudWatch Events) rule that tracks table-level events in DynamoDB. Set a Lambda function as a rule target to process and save new changes to the other table** is incorrect. An Amazon EventBridge (Amazon CloudWatch Events) rule is not capable of detecting table-level events from Amazon DynamoDB.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\r\n\r\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html"
  },
  {
    "id": 186,
    "question": "A developer is managing a distributed system that consists of an Application Load Balancer, an SQS queue, and an Auto Scaling group of EC2 instances. The system has been integrated with CloudFront to better serve clients worldwide. To enhance the security of the in-flight data, the developer was instructed to establish an end-to-end SSL connection between the origin and the end-users.\r\n\r\nWhich TWO options will allow the developer to meet this requirement using CloudFront? (Select TWO.)\r\n\r\n",
    "options": [
      "Configure the Origin Protocol Policy to use HTTPS only",
      "Set up an Origin Access Control (OAC) setting",
      "Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution.",
      "Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config.",
      "Configure the Viewer Protocol Policy to use HTTPS only"
    ],
    "question_type": "multiple",
    "correct_answers": [
      "Configure the Origin Protocol Policy to use HTTPS only",
      "Configure the Viewer Protocol Policy to use HTTPS only"
    ],
    "correct_answer": "Configure the Origin Protocol Policy to use HTTPS only",
    "explanation": "For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers. You can also configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.\r\n\r\nIf you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here\u2019s what happens when CloudFront receives a request for an object. The process works basically the same way whether your origin is an Amazon S3 bucket or a custom origin such as an HTTP/S server:\r\n\r\n1. A viewer submits an HTTPS request to CloudFront. There\u2019s some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format.\r\n\r\n2. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it.\r\n\r\n3. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format.\r\n\r\n4. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront.\r\n\r\n5. CloudFront decrypts the response, re-encrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it\u2019s requested.\r\n\r\n6. The viewer decrypts the response.\r\n\r\nYou can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS, so that CloudFront requires HTTPS for some objects but not for others.\r\n\r\nTo implement this setup, you have to change the **Origin Protocol Policy** setting for the applicable origins in your distribution. If you\u2019re using the domain name that CloudFront assigned to your distribution, such as dtut0rial5d0j0.cloudfront.net, you change the **Viewer Protocol Policy** setting for one or more cache behaviors to require HTTPS communication. With this configuration, CloudFront provides the SSL/TLS certificate.\r\n\r\nHence, the correct answers are: **Configure the Origin Protocol Policy to use HTTPS only** and **Configure the Viewer Protocol Policy to use HTTPS only** are correct answers in this scenario.\r\n\r\nThe option that says: **Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config** is incorrect because you can\u2019t store a certificate in AWS Config.\r\n\r\nThe option that says: **Set up an Origin Access Control (OAC) setting** is incorrect because this CloudFront feature only allows you to secure S3 origins by granting access to S3 buckets for designated CloudFront distributions. This method is applicable only to S3 origins and cannot be used to establish end-to-end SSL connections for other origins.\r\n\r\nThe option that says: **Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution** is incorrect because AWS WAF is primarily used to protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. This will not allow you to establish an SSL connection between your origin and your clients.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html"
  },
  {
    "id": 187,
    "question": "You want to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a quick way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually increase the traffic going to the new version.\r\n\r\nWhich of the following implementation is the BEST option to use?\r\n\r\n",
    "options": [
      "Use ELB to route traffic to both Lambda functions.",
      "Use Traffic Shifting with Lambda Aliases.",
      "Use Route 53 weighted routing to two Lambda functions.",
      "Use stage variables in your Lambda function."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Traffic Shifting with Lambda Aliases."
    ],
    "correct_answer": "Use Traffic Shifting with Lambda Aliases.",
    "explanation": "By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To minimize this impact, you can implement the `routing-config` parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version.\r\n\r\nFor example, you can specify that only 2 percent of incoming traffic is routed to the new version while you analyze its readiness for a production environment, while the remaining 98 percent is routed to the original version. As the new version matures, you can gradually update the ratio as necessary until you have determined that the new version is stable. You can then update the alias to route all traffic to the new version.\r\n\r\nYou can point an alias to a maximum of two Lambda function versions. In addition:\r\n\r\n\u2013 Both versions must have the same IAM execution role.\r\n\r\n\u2013 Both versions must have the same [AWS Lambda Function Dead Letter Queues](https://docs.aws.amazon.com/lambda/latest/dg/dlq.html) configuration, or no DLQ configuration.\r\n\r\n\u2013 When pointing an alias to more than one version, the alias cannot point to `$LATEST`.\r\n\r\nHence, **using Traffic Shifting for Lambda Aliases** is the correct answer.\r\n\r\n**Using Route 53 weighted routing to two Lambda functions** is incorrect. Although you may configure 2 different endpoints for your Lambda versions and use Route 53 Weighted Routing, this is still not a manageable and convenient way of handling the failover of your serverless function. The best way is to use Lambda Aliases for the different versions of your function and do traffic shifting on these two versions.\r\n\r\n**Using ELB to route traffic to both Lambda functions** is incorrect because this is not the recommended way to gradually deploy the new version of your Lambda function. It is still best to use Lambda Aliases instead of an Application Load Balancer.\r\n\r\n**Using stage variables in your Lambda function** is incorrect because stage variables are primarily used in API Gateway and not in Lambda. Although this solution may work, you are still required to create an API Gateway and create a stage variable that will point to the new and old versions of the Lambda function. This entails extra configuration, compared with just doing traffic shifting in Lambda.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html"
  },
  {
    "id": 188,
    "question": "To improve their information security management system (ISMS), a company recently released a new policy which requires all database credentials to be encrypted and be automatically rotated to avoid unauthorized access.\r\n\r\nWhich of the following is the MOST appropriate solution to secure the credentials?\r\n\r\n",
    "options": [
      "Create a secret in AWS Secrets Manager and enable automatic rotation of the database credentials.",
      "Create an IAM Role which has full access to the database. Attach the role to the services which require access.",
      "Create a parameter to the Systems Manager Parameter Store using the PutParameter API with a type of SecureString.",
      "Enable IAM DB authentication which rotates the credentials by default."
    ],
    "question_type": "single",
    "correct_answers": [
      "Create a secret in AWS Secrets Manager and enable automatic rotation of the database credentials."
    ],
    "correct_answer": "Create a secret in AWS Secrets Manager and enable automatic rotation of the database credentials.",
    "explanation": "**AWS Secrets Manager** is an AWS service that makes it easier for you to manage secrets. *Secrets* can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs.\r\n\r\nIn the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another.\r\n\r\nSecrets Manager enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can\u2019t be compromised by someone examining your code, because the secret simply isn\u2019t there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.\r\n\r\nHence, **creating a secret in AWS Secrets Manager and enabling automatic rotation of the database credentials** is the most appropriate solution for this scenario.\r\n\r\nThe option that says: **Create a parameter to the Systems Manager Parameter Store using the PutParameter API with a type of `SecureString`** is incorrect because, by default, Systems Manager Parameter Store doesn\u2019t rotate its parameters.\r\n\r\nThe option that says: **Enable IAM DB authentication which rotates the credentials by default** is incorrect because this solution only enables the service to connect to Amazon RDS with IAM credentials. It doesn\u2019t have the capability to rotate the credentials like what AWS Secrets Manager does to its secrets.\r\n\r\nThe option that says: **Create an IAM Role which has full access to the database. Attach the role to the services which requires access** is incorrect because although IAM Role is a preferred way to grant access to certain services, this solution doesn\u2019t rotate the keys/credentials.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\r\n\r\nhttps://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/"
  },
  {
    "id": 189,
    "question": "A company has a microservices application that must be integrated with API Gateway. The developer must configure custom data mapping between the API Gateway and the microservices.\r\n\r\nIn addition, the developer must specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.\r\n\r\nWhich of the following integration types is the MOST suitable one to use in API Gateway to meet this requirement?",
    "options": [
      "AWS_PROXY",
      "HTTP",
      "AWS",
      "HTTP_PROXY"
    ],
    "question_type": "single",
    "correct_answers": [
      "HTTP"
    ],
    "correct_answer": "HTTP",
    "explanation": "You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways:\r\n\r\n\u2013 HTTP proxy integration\r\n\r\n\u2013 HTTP custom integration\r\n\r\nIn your API Gateway console, you can define the type of HTTP integration of your resource by toggling the \u201c**Proxy resource**\u201d switch.\r\n\r\nWith proxy integration, the setup is simple. You only need to set the HTTP method and the HTTP endpoint URI, according to the backend requirements, if you are not concerned with content encoding or caching.\r\n\r\nWith custom integration, setup is more involved. In addition to the proxy integration setup steps, you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. API Gateway supports the following endpoint ports: 80, 443 and 1024-65535.\r\n\r\nProgrammatically, you choose an integration type by setting the [`type`](https://docs.aws.amazon.com/apigateway/api-reference/resource/integration/#type) property on the [`Integration`](https://docs.aws.amazon.com/apigateway/api-reference/resource/integration/) resource. For the Lambda proxy integration, the value is **`AWS_PROXY`**. For the Lambda custom integration and all other AWS integrations, it is **`AWS`**. For the HTTP proxy integration and HTTP integration, the value is **`HTTP_PROXY`** and **`HTTP`**, respectively. For the mock integration, the `type` value is **`MOCK`**.\r\n\r\nSince the integration type that is being described in the scenario fits the definition of an *HTTP custom integration*, the correct answer in this scenario is to use the **`HTTP`** integration type.\r\n\r\nHence, the correct answer is: `HTTP.`\r\n\r\n**`AWS`** is incorrect because this type is primarily used for Lambda custom integration. Since the scenario does not specify that the microservices are Lambda functions, the HTTP integration type is the most flexible and suitable for such a scenario.\r\n\r\n**`AWS_PROXY`** is incorrect because this type is primarily used for Lambda proxy integration. The scenario didn\u2019t mention that it uses a serverless application or Lambda.\r\n\r\n**`HTTP_PROXY`** is incorrect because this type is only used for HTTP proxy integration where you don\u2019t need to do data mapping for your request and response data.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/setup-http-integrations.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html"
  },
  {
    "id": 190,
    "question": "A recruitment agency has a large collection of resumes stored in an Amazon S3 bucket. The agency wants to perform an analysis on these files, but for privacy compliance reasons, they need to ensure that certain personally identifiable information (PII) is redacted before being processed by their internal service.\r\n\r\nWhich solution can meet the requirements in the most cost-effective way?",
    "options": [
      "Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files.",
      "Use Amazon S3 Object Lambda to redact PII before it is returned to the application.",
      "Implement a solution with AWS Glue to transform the data and redact PII before storing it in an S3 bucket.",
      "Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda@Edge function to redact the PII as data is fetched from the S3 bucket."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Amazon S3 Object Lambda to redact PII before it is returned to the application."
    ],
    "correct_answer": "Use Amazon S3 Object Lambda to redact PII before it is returned to the application.",
    "explanation": "S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it\u2019s being returned to an application. This feature is designed for use cases where data needs to be transformed on-the-fly without the need to store a transformed copy of the data. It\u2019s useful in scenarios like filtering rows, redacting confidential data, dynamically resizing images and other similar situations where data transformation or processing is required during data retrieval.\r\n\r\nIn the scenario, when the internal service fetches a file from the S3 bucket, S3 Object Lambda will run a Lambda function to redact the PII from the data as it is being retrieved before it is returned to the application. This eliminates the need to create and store a separate, redacted copy of each resume, thereby saving on storage costs. Plus, since the redaction happens during data retrieval, there\u2019s no need to create a proxy for the internal service. This makes the solution efficient and cost-effective.\r\n\r\nHence, the correct answer is: **Use Amazon S3 Object Lambda to redact PII before it is returned to the application.**\r\n\r\nThe option that says: **Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) function to redact the PII as data is fetched from the S3 bucket** is incorrect. Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) functions operate at the CDN edge locations, making them ideal for use cases that need low-latency responses to end users. However, in the scenario, the requirement is to redact PII before processing by an internal service, not necessarily to serve end users quickly. Moreover, Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) is generally more expensive to run than a regular Lambda function.\r\n\r\nThe option that says: **Implement a solution with AWS Glue to transform the data and redact PII before storing it in the S3 bucket** is incorrect. AWS Glue is an ETL service designed for complex transformations and analytics. With this approach, you\u2019d create redacted copies of resumes, leading to increased storage costs. Also, using AWS Glue to merely redact PII might be overkill and less cost-effective compared to simpler solutions like S3 Object Lambda.\r\n\r\nThe option that says: **Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files** is incorrect. In this approach, storing redacted copies of data in S3 would increase storage costs. However, with S3 Object Lambda, on-the-fly redaction of PII becomes possible, eliminating the need for storing separate redacted copies of the data.\r\n\r\n**References:**\r\n\r\nhttps://aws.amazon.com/s3/features/object-lambda/\r\n\r\nhttps://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/"
  },
  {
    "id": 191,
    "question": "A developer is creating a script using AWS CLI to retrieve a list of objects in an S3 bucket. However, the script is timing out if the bucket has tens of thousands of objects.\r\n\r\nWhich solution would most likely rectify the issue?",
    "options": [
      "Enable Amazon S3 Transfer Acceleration",
      "Increase the AWS CLI timeout value",
      "Apply the pagination parameters in the AWS CLI command",
      "Enable CORS"
    ],
    "question_type": "single",
    "correct_answers": [
      "Apply the pagination parameters in the AWS CLI command"
    ],
    "correct_answer": "Apply the pagination parameters in the AWS CLI command",
    "explanation": "For commands that possibly return a long list of items, the AWS CLI provides parameters allowing you to limit the number of items included in the output when the AWS CLI queries a service\u2019s API.\r\n\r\nBy default, the AWS CLI retrieves all accessible items with a page size of 1,000. If you need help running list commands on a large number of resources, the default page size of 1000 may be too large. This can cause calls to AWS services to exceed the maximum allowed time, resulting in a \u201ctimed out\u201d error.\r\n\r\nOne of the pagination options you can use is the `--page-size` option. This option tells the AWS CLI to request a smaller number of items from each call to the AWS service.\r\n\r\n### `aws s3api list-objects --bucket tdbucket --page-size 100`\r\n\r\nThe CLI still retrieves the entire list, but it makes a greater number of service API calls in the background and retrieves fewer items with each request. This increases the probability that individual calls will succeed in without the use of a timeout.\r\n\r\nHence, the correct answer is: **Apply the pagination parameters in the AWS CLI command.**\r\n\r\nThe option that says: **Increase the AWS CLI timeout value** is incorrect. Increasing CLI parameters like `--cli-connect-timeout` or `--cli-read-timeout` would only prolong the process and increase susceptibility to timeouts due to network latency. On the other hand, pagination would handle large data sets by retrieving objects in manageable chunks, aligning with S3\u2019s response limits and preventing timeouts.\r\n\r\nThe option that says: **Enabling Amazon S3 Transfer Acceleration** is incorrect because this is only a bucket-level feature that enables faster data transfers to and from Amazon S3. Although this will improve the retrieval times of your objects, this feature will still not paginate the result, which may still cause time-out errors.\r\n\r\nThe option that says: **Enabling CORS** is incorrect because the Cross-origin resource sharing (CORS) is simply thats allow client web applications that are loaded in one domain to communicate with resources in a different domain. This is not useful in paginating the results from an AWS CLI call.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/list-objects.html"
  },
  {
    "id": 192,
    "question": "A new IT policy requires you to trace all calls that your Node.js application sends to external HTTP web APIs as well as SQL database queries. You have to instrument your application, which is hosted in Elastic Beanstalk, in order to properly trace the calls via the X-Ray console .\r\n\r\nWhat should you do to comply with the given requirement?",
    "options": [
      "Create a Docker image that runs the X-Ray daemon.",
      "Use a user data script to run the daemon automatically.",
      "Enable active tracing in the Elastic Beanstalk by including the healthcheckurl.config configuration file in the .ebextensions directory of your source code.",
      "Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code."
    ],
    "question_type": "single",
    "correct_answers": [
      "Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code."
    ],
    "correct_answer": "Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code.",
    "explanation": "You can use the **AWS Elastic Beanstalk** console or a configuration file to run the AWS X-Ray daemon on the instances in your environment. X-Ray is an AWS service that gathers data about the requests that your application serves, and uses it to construct a service map that you can use to identify issues with your application and opportunities for optimization.\r\n\r\nTo relay trace data from your application to AWS X-Ray, you can run the X-Ray daemon on your Elastic Beanstalk environment\u2019s Amazon EC2 instances. Elastic Beanstalk platforms provide a configuration option that you can set to run the daemon automatically. You can enable the daemon in a configuration file in your source code or by choosing an option in the Elastic Beanstalk console. When you enable the configuration option, the daemon is installed on the instance and runs as a service.\r\n\r\nHence, the correct answer is to: **enable the X-Ray daemon by including the `xray-daemon.config` configuration file in the `.ebextensions` directory of your source code**, just as shown above.\r\n\r\n**Using a user data script to run the daemon automatically** is incorrect because this is only applicable if you want to enable X-Ray to your EC2 instances.\r\n\r\n**Creating a Docker image that runs the X-Ray daemon** is incorrect because this is what you need to do if you want to enable X-Ray on ECS Cluster and not on Elastic Beanstalk.\r\n\r\n**Enabling active tracing in the Elastic Beanstalk by including the `healthcheckurl.config` configuration file in the `.ebextensions` directory of your source code** is incorrect because this configuration file only sets the application health check URL and not X-Ray Tracing.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-beanstalk.html\r\n\r\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html"
  },
  {
    "id": 193,
    "question": "Your application is processing one Kinesis data stream which has four shards, and each instance has one KCL worker. To scale up processing in your application, you reshard your stream to increase the number of open shards to six.\r\n\r\nWhat is the MAXIMUM number of EC2 instances that you should launch to achieve optimum performance?",
    "options": [
      "6",
      "5",
      "12",
      "3"
    ],
    "question_type": "single",
    "correct_answers": [
      "6"
    ],
    "correct_answer": "6",
    "explanation": "Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nTo scale up processing in your application, you should test a combination of these approaches:\r\n\r\n\u2013 Increasing the instance size (because all record processors run in parallel within a process)\r\n\r\n\u2013 Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently)\r\n\r\n\u2013 Increasing the number of shards (which increases the level of parallelism)\r\n\r\nThus, the maximum number of instances you can launch is **6**, to match the number of open shards in a ratio of 1:1.\r\n\r\nAlthough you can launch **3** instances in which each instance handles 2 shards, this is not the maximum number of instances you can deploy for your application. Hence, this option is incorrect. Take note that the maximum number of your instances is not half the number of open shards.\r\n\r\nJust like the above option, you can also launch **5** instances in which each instance handles 3 shards. However, this is not the maximum number of instances you can launch. Keep in mind that the maximum number of your instances can be equal to the number of open shards of the Kinesis stream. Therefore, this option is also incorrect.\r\n\r\nLaunching **12** instances is incorrect because you should ensure that the number of instances does not exceed the number of open shards. The maximum number of instances that you should deploy is 6.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\r\n\r\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html"
  },
  {
    "id": 194,
    "question": "A company selling smart security cameras uses an S3 bucket behind a CloudFront web distribution to store its static content, which it shares with customers worldwide. The company has recently released a new firmware update intended only for its premium customers, and unauthorized access should be denied with a user authentication process that has minimal latency.\r\n\r\nHow can a developer refactor the current setup to achieve this requirement with the MOST efficient solution?",
    "options": [
      "Restrict access to the S3 bucket only to premium customers using an Origin Access Control (OAC).",
      "Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update.",
      "Use Signed URLs and Signed Cookies in CloudFront to distribute the firmware update file.",
      "Use the AWS Serverless Application Model (AWS SAM) and Amazon Cognito to authenticate the premium customers."
    ],
    "question_type": "single",
    "correct_answers": [
      "Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update."
    ],
    "correct_answer": "Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update.",
    "explanation": "Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/), you don\u2019t have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume \u2013 there is no charge when your code is not running.\r\n\r\nWith Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/), you can enrich your web applications by making them globally distributed and improving their performance \u2014 all with zero server administration. Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). Just upload your code to AWS Lambda, which takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user.\r\n\r\nYou can use Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) to help authenticate and authorize users for the premium pay-wall content on your website, filtering out unauthorized requests before they reach your origin infrastructure. For example, you can trigger a Lambda function to authorize each viewer request by calling authentication and user management service such as Amazon Cognito.\r\n\r\nHence, the correct answer is: **Use Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) and Amazon Cognito to authenticate and authorize premium customers to download the firmware update.**\r\n\r\nThe option that says: **Use the AWS Serverless Application Model (AWS SAM) and Amazon Cognito to authenticate the premium customers** is incorrect because AWS SAM is just an open-source framework that you can use to build serverless applications on AWS. In this scenario, you have to integrate your CloudFront web distribution with Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/), and you can do this without using AWS SAM.\r\n\r\nThe option that says: **Restrict access to the S3 bucket only to premium customers by using an Origin Access Control (OAC)** is incorrect because OAC is primarily used to prevent your users from viewing your S3 files by simply using the direct S3 URL.\r\n\r\nThe option that says: **Use Signed URLs and Signed Cookies in CloudFront to distribute the firmware update file** is incorrect. Although this solution provides a way to authenticate the premium users for the private content, the process of authentication has a significant latency in comparison to the Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/) solution. In this option, you have to refactor your application (which is deployed to a specific AWS region) to either create and distribute signed URLs to authenticated users or to send `Set-Cookie` headers that set signed cookies on the viewers for authenticated users. This will cause the latency, which could have been improved if the authentication logic resides on CloudFront edge locations using Lambda[@Edge](https://portal.tutorialsdojo.com/members/edge/).\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html\r\n\r\nhttps://aws.amazon.com/lambda/edge/"
  },
  {
    "id": 195,
    "question": "You developed a shell script which uses AWS CLI to create a new Lambda function. However, you received an InvalidParameterValueException after running the script.\r\n\r\nWhat is the MOST likely cause of this issue?\r\n\r\n\r\n",
    "options": [
      "The resource already exists.",
      "The AWS Lambda service encountered an internal error.",
      "You provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume.",
      "You have exceeded your maximum total code size per account."
    ],
    "question_type": "single",
    "correct_answers": [
      "You provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume."
    ],
    "correct_answer": "You provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume.",
    "explanation": "To create a Lambda function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing. You can use the CreateFunction API via the AWS CLI or the AWS SDK of your choice.\r\n\r\nA function has an unpublished version, and can have published versions and aliases. The unpublished version changes when you update your function\u2019s code and configuration. A published version is a snapshot of your function code and configuration that can\u2019t be changed. An alias is a named resource that maps to a version, and can be changed to map to a different version.\r\n\r\nThe `InvalidParameterValueException` will be returned if one of the parameters in the request is invalid. For example, if **you provided an IAM role in the `CreateFunction` API which AWS Lambda is unable to assume**. Hence, this option is the most likely cause of the issue in this scenario.\r\n\r\nIf *y***ou have exceeded your maximum total code size per account**, the `CodeStorageExceededException` will be returned, which is why this option is incorrect.\r\n\r\nIf **the resource already exists**, the `ResourceConflictException` will be returned and not `InvalidParameterValueException`. Therefore, this option is also incorrect.\r\n\r\nIf **the AWS Lambda service encountered an internal error**, the `ServiceException` will be returned hence, this option is incorrect.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/lambda/create-function.html"
  },
  {
    "id": 196,
    "question": "A company offers a Generative Artificial Intelligence (AI) service exposed through a REST API managed by Amazon API Gateway. They recently rolled out a subscription tier where users receive API keys to access premium features. The company uses the CreateApiKey API for generating these keys.\r\n\r\nDuring testing, developers noticed that while existing users can access the service without issues, new premium subscribers get a 403 Forbidden error when using their API keys.\r\n\r\nWhat must be done to give new users access to the service?",
    "options": [
      "Use the ImportApiKeys operation to import the premium users\u2019 keys, then apply the UpdateUsagePlan operation to set the new tier access.",
      "Associate the API keys for the premium users with the intended usage plan using the CreateUsagePlanKey operation.",
      "Use the UpdateAuthorizer operation to modify the authorization settings. Promote the changes to the production stage by calling the CreateDeployment operation.",
      "Instruct users to send their API key in a custom header. In the integration request, adjust the mapping template to extract and evaluate this header to distinguish between free-tier and premium subscribers."
    ],
    "question_type": "single",
    "correct_answers": [
      "Associate the API keys for the premium users with the intended usage plan using the CreateUsagePlanKey operation."
    ],
    "correct_answer": "Associate the API keys for the premium users with the intended usage plan using the CreateUsagePlanKey operation.",
    "explanation": "In Amazon API Gateway, API keys by themselves do not grant access to execute an API. They need to be associated with a usage plan, and that usage plan then determines which API stages and methods the API key can access.\r\n\r\nIf the API key is not associated with a usage plan, it will not have permission to access any of the resources, which will result in a \u201c403 Forbidden\u201d error.\r\n\r\nIn the given scenario, existing users can access the service, but new premium subscribers cannot. This indicates that while the API keys were created for new users, they might not have been associated with the appropriate usage plan. Hence, after generating an API key, it must be added to a usage plan by calling the **`CreateUsagePlanKey`** method**.**\r\n\r\nHence, the correct answer is: **Associate the API keys for the premium users with the intended usage plan using the `CreateUsagePlanKey` operation.**\r\n\r\nThe option that says: **Use the `ImportApiKeys` operation to import the premium users\u2019 keys, then apply the `UpdateUsagePlan` operation to set the new tier access** is incorrect. The `importApiKeys` API is primarily used for bulk importing API keys, not for associating them with a usage plan. Although the `updateUsagePlan` API modifies properties of a usage plan; it doesn\u2019t handle direct association of API keys.\r\n\r\nThe option that says: **Use the `UpdateAuthorizer` operation to modify the authorization settings. Promote the changes to the production stage by calling the `CreateDeployment` operation** is incorrect. The `updateAuthorizer` operation is only used to modify the settings of an existing custom authorizer, which handles custom authorization logic for APIs. In the scenario, the issue is not related to custom authorization but rather to the association of API keys with a usage plan.\r\n\r\nThe option that says: **Instruct users to send their API key in a custom header. In the integration request, adjust the mapping template to extract and evaluate this header to distinguish between free-tier and premium subscribers** is incorrect. Changing the way users provide their API key adds unnecessary complexity and won\u2019t solve the issue at hand. The problem isn\u2019t with how the API key is being sent but with the API key not having appropriate permissions because it\u2019s not associated with a usage plan.\r\n\r\n**References:**\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/api/API_UpdateUsagePlan.html\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/api/API_CreateApiKey.html"
  }
]
