[{"id": 1, "question": "Which AWS service is primarily used for storing static files?", "options": ["EC2", "S3", "DynamoDB", "RDS"], "correct_answer": "S3", "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance for storing static files."}, {"id": 2, "question": "Which AWS service would you use to run containers?", "options": ["EC2", "S3", "ECS/EKS", "Lambda"], "correct_answer": "ECS/EKS", "explanation": "Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service) are services designed specifically for running containers in AWS."}, {"id": 3, "question": "A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.\r\n\r\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?", "options": ["4 shards : 2 instances", "1 shard : 6 instances", "6 shards : 1 instance", "4 shards : 8 instances"], "correct_answer": "4 shards : 2 instances", "explanation": "A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.\r\n\r\n\r\n\r\nTypically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\u2019s fine if the number of shards exceeds the number of instances.\r\n\r\nSince the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer.\r\n\r\nThe 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.\r\n\r\nThe 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn\u2019t have a backup instance to process the shards in the event of an outage.\r\n\r\nThe 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well.\r\n\r\n"}, {"id": 4, "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\r\n\r\nHow should the developer retrieve the variables with the FEWEST application changes? ", "options": ["Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", " Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment. ", " Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment. ", "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process. "], "correct_answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.", "explanation": "A is correct:\r\n\r\n- It uses the appropriate services for the right types of data (Parameter Store for configuration, Secrets Manager for sensitive credentials)\r\n\r\n- It provides a centralized approach that requires minimal application changes\r\n- It supports hierarchical organization for different environments\r\n- It provides robust security controls through IAM\r\n- It enables changes to parameters without application redeployment\r\n\r\nThe application would only need to be updated once to retrieve variables from these services, and then all future changes to the variables would be managed through the services without additional application changes."}, {"id": 5, "question": "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.\r\n\r\nWhich section of a CloudFormation template cannot be associated with Condition?", "options": ["Conditions", "Resources", "Outputs", "Parameters"], "correct_answer": "Parameters", "explanation": "Parameters\r\n\r\nParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\r\n\r\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\r\n\r\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.\r\n\r\nConditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.\r\n\r\nPlease review this note for more details:  via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\r\n\r\nPlease visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.\r\n\r\nIncorrect options:\r\n\r\nResources - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.\r\n\r\nConditions - You actually define conditions in this section of the CloudFormation template\r\n\r\nOutputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create."}, {"id": 6, "question": "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.\r\n\r\nAs a Developer Associate, which of the following options would you recommend for the given use-case?\r\n\r\n\r\n\r\n\r\n\r\n", "options": ["Cognito User Pools", "Lambda Authorizer", "API Gateway User Pools", "IAM permissions with sigv4"], "correct_answer": "Lambda Authorizer", "explanation": "Correct option:\r\n\r\n\"Lambda Authorizer\"\r\n\r\nAn Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.\r\n\r\n via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\r\n\r\nIncorrect options:\r\n\r\n\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.\r\n\r\n\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.\r\n\r\n\"API Gateway User Pools\" - This is a made-up option, added as a distractor.\r\n\r\nReference:\r\n\r\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"}, {"id": 7, "question": "You're designing an application that processes data from an Amazon Kinesis Data Stream. Your current architecture includes a Kinesis stream with 8 shards. Each EC2 instance in your application runs a single Kinesis Client Library (KCL) consumer application.\r\n\r\nWhat is the optimal number of EC2 instances you should deploy to efficiently process this Kinesis stream?", "options": ["4 instances", " 8 instances", "16 instances", "32 instances"], "correct_answer": " 8 instances", "explanation": "When designing applications that process Amazon Kinesis Data Streams, it's important to understand the relationship between shards and KCL workers for optimal performance.\r\n\r\nThe best practice is to match the number of instances (each running one KCL worker) to the number of shards in your Kinesis stream. This is because:\r\n\r\n- Each shard in a Kinesis Data Stream can be processed by exactly one KCL worker at any given time\r\n\r\n- A single KCL worker can process multiple shards, but this may not be optimal for performance\r\n\r\n- Having more KCL workers than shards is inefficient as some workers would be idle\r\n\r\n- Having fewer KCL workers than shards means some workers would be processing multiple shards, which might create a bottleneck\r\n\r\nIn this scenario, with 8 shards in your Kinesis stream, the optimal configuration would be 8 EC2 instances, each running one KCL worker application. This provides a 1:1 mapping between shards and KCL workers, ensuring maximum throughput and parallel processing capability.\r\n\r\nIf your data processing needs change, you would typically adjust both the number of shards and the number of instances accordingly to maintain this optimal ratio."}, {"id": 8, "question": "You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.\r\n\r\n\r\nWhich of the following does **NOT** help with debugging the issue?", "options": ["X-Ray sampling", "EC2 Instance Role", "EC2 X-Ray Daemon", "CloudTrail"], "correct_answer": "X-Ray sampling", "explanation": "Correct option:\r\n\r\n**X-Ray sampling**\r\n\r\nWhy X-Ray sampling WON'T help:\r\nX-Ray sampling only controls which requests get traced, not whether those traces successfully reach X-Ray. Adjusting sampling rules is like deciding how many photos to take, but won't help if the camera can't transmit pictures to your cloud storage.\r\n\r\n\r\n**EC2 X-Ray Daemon**\r\n\r\n- The X-Ray daemon is the component that actually sends trace data to AWS\r\n\r\n- Checking daemon logs would show connection errors, timeouts, or permission issues\r\n\r\n- You could verify if the daemon is running correctly with ps aux | grep xray\r\n\r\n- The daemon log file at /var/log/xray/xray.log might contain error messages\r\n\r\n\r\n#### 2. EC2 Instance Role\r\n* The X-Ray daemon needs proper IAM permissions to send data to X-Ray\r\n* The EC2 instance role provides these permissions automatically\r\n* Checking the attached role and its policies would reveal missing permissions\r\n* You could verify the policy includes `xray:PutTraceSegments` and `xray:PutTelemetryRecords`\r\n\r\n\r\n#### 3. CloudTrail\r\n* CloudTrail logs all API calls made to AWS services\r\n* It would show denied API calls due to permission issues\r\n* You could search for X-Ray related API calls from your EC2 instance\r\n* Failed API calls would include detailed error messages explaining why they failed\r\n\r\n## Key Troubleshooting Steps\r\n\r\nIn this scenario, you should:\r\n1. Check if the X-Ray daemon is running on the EC2 instance\r\n2. Verify the EC2 instance role has appropriate X-Ray permissions\r\n3. Look at CloudTrail logs for denied X-Ray API calls\r\n4. Check security groups and network ACLs to ensure outbound traffic to X-Ray endpoints is allowed\r\n\r\nAdjusting sampling rules would not provide any useful diagnostic information since no data is being transmitted at all."}, {"id": 9, "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket, the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).\r\n\r\nWhich solution will guarantee that any upload request without the mandated encryption is not processed?", "options": ["Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `aws:kms`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `sse:s3`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header."], "correct_answer": "Invoke the PutObject API operation and set the `x-amz-server-side-encryption` header as `AES256`. Use an S3 bucket policy to deny permission to upload an object unless the request has this header.", "explanation": "### Why Option C is Correct:\r\nWhen using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), you need to:\r\n\r\n1. Set the `x-amz-server-side-encryption` header to `AES256` in your PutObject requests\r\n2. Implement a bucket policy that enforces this requirement\r\n\r\nThis approach works because:\r\n* `AES256` is the correct value that specifies SSE-S3 encryption (Amazon S3-managed keys)\r\n* The bucket policy can deny any requests that either:\r\n  * Don't include the encryption header at all\r\n  * Include the header but with an incorrect value\r\n\r\nA typical bucket policy would look like this:\r\n```json\r\n{\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"StringNotEquals\": {\r\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"Effect\": \"Deny\",\r\n      \"Principal\": \"*\",\r\n      \"Action\": \"s3:PutObject\",\r\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\",\r\n      \"Condition\": {\r\n        \"Null\": {\r\n          \"s3:x-amz-server-side-encryption\": \"true\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis policy uses two statements:\r\n* The first statement denies requests where the header exists but has an incorrect value\r\n* The second statement denies requests where the header is missing entirely\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option A is incorrect** because:\r\n* `aws:kms` is used for SSE-KMS (Server-Side Encryption with AWS KMS keys)\r\n* The question specifically requires SSE-S3 (Server-Side Encryption with Amazon S3-managed keys)\r\n* While this would enforce encryption, it would use the wrong type of encryption\r\n\r\n**Option B is incorrect** because:\r\n* `sse:s3` is not a valid value for the `x-amz-server-side-encryption` header\r\n* The valid values are `AES256` (for SSE-S3) or `aws:kms` (for SSE-KMS)\r\n* Using an invalid header value would cause all requests to fail\r\n\r\n**Option D is incorrect** because:\r\n* With SSE-S3, you don't specify or manage the encryption keys\r\n* Amazon S3 automatically handles key management, generating a unique key for each object\r\n* There is no way to \"set the encryption key for SSE-S3\" in an HTTP header\r\n\r\n### Key Concept:\r\nWhen using SSE-S3, remember that:\r\n1. Amazon S3 handles all key management automatically\r\n2. Each object is encrypted with a unique key\r\n3. These keys are themselves encrypted with a master key that AWS rotates regularly\r\n4. The encryption standard used is AES-256\r\n5. You only need to specify the encryption method (`AES256`), not any keys\r\n\r\n## AWS Documentation Reference\r\nFor more information, refer to the [Amazon S3 Developer Guide on Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)."}, {"id": 10, "question": "A development team has created a serverless application that uses Amazon API Gateway and AWS Lambda. They want to use a single Lambda function across multiple API Gateway stages (development, testing, and production), but they need the function to read from a different DynamoDB table depending on which stage is being called. \r\n\r\nWhat is the MOST appropriate way for the developer to pass these configuration parameters to the Lambda function?", "options": ["Use Stage Variables in API Gateway and reference them in mapping templates", "Set up an API Gateway Private Integration to the Lambda function", "Create environment variables in the Lambda function for each table name", "Configure traffic shifting with Lambda Aliases for each stage"], "correct_answer": "Use Stage Variables in API Gateway and reference them in mapping templates", "explanation": "### Why Option A is Correct:\r\nStage variables are name-value pairs that function as configuration attributes for different deployment stages of your API Gateway REST API. They effectively work like environment variables that can be accessed from various parts of your API configuration, including mapping templates.\r\n\r\nThis solution works perfectly for the scenario because:\r\n\r\n1. **Dynamic Configuration Per Stage**: Stage variables allow you to set different values for each deployment stage (development, testing, production)\r\n\r\n2. **Accessible in Mapping Templates**: You can reference stage variables in the mapping templates that generate the request for your Lambda function\r\n\r\n3. **No Code Changes Needed**: The Lambda function code remains the same across all environments, making maintenance easier\r\n\r\n4. **Implementation Example**:\r\n   ```json\r\n   // API Gateway mapping template example\r\n   {\r\n     \"tableName\": \"$stageVariables.dynamoDBTableName\",\r\n     \"operation\": \"read\",\r\n     \"key\": {\r\n       \"id\": \"$input.params('id')\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In this example, `$stageVariables.dynamoDBTableName` would contain different values in different stages:\r\n   - In development: \"dev-customer-table\"\r\n   - In testing: \"test-customer-table\"\r\n   - In production: \"prod-customer-table\"\r\n\r\n### Why Other Options Are Incorrect:\r\n\r\n**Option B is incorrect** because:\r\n* API Gateway Private Integration is used to connect API Gateway to private resources in your VPC\r\n* It doesn't provide a way to dynamically configure which DynamoDB table the Lambda function should use\r\n* It's designed for network connectivity, not for configuration parameter passing\r\n\r\n**Option C is incorrect** because:\r\n* Lambda environment variables are static for a given function and don't change based on which API stage called the function\r\n* While you could check the stage name in your Lambda code and use different tables based on that, this would require code changes and additional logic\r\n* This approach would be less maintainable as it mixes configuration with application code\r\n\r\n**Option D is incorrect** because:\r\n* Lambda Aliases are used to point to specific versions of Lambda functions\r\n* Traffic shifting with aliases is for gradually moving traffic between different versions of a function\r\n* This doesn't solve the problem of using different DynamoDB tables without modifying the function code\r\n* This approach would require maintaining multiple versions of essentially the same code with different table names hardcoded\r\n\r\n### Key Concept:\r\nAPI Gateway Stage Variables provide a clean separation of configuration from code, allowing you to deploy the same Lambda function code to multiple environments while dynamically changing its behavior based on which stage is calling it.\r\n\r\n## AWS Documentation Reference\r\nFor more information on using stage variables with Lambda functions, refer to the [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html)."}]